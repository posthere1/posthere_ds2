{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c5vGdURrUei1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "colab_type": "code",
    "id": "HPkBTAyQUtkn",
    "outputId": "6b52d4f0-064c-48ea-eaec-9bd9b7e51c80"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ac0f6801b25e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreddit_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"reddit_posts.pkl.xz\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcompression\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"infer\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mcompression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# 1) try standard library Pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;31m# XZ Compression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mcompression\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"xz\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_lzma_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlzma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0;31m# Unrecognized Compression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/lzma.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, mode, format, check, preset, filters)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_closefp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode_code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'reddit_posts.pkl.xz'"
     ]
    }
   ],
   "source": [
    "reddit_df = pd.read_pickle(\"reddit_posts.pkl.xz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wgxCqzYk8oa2"
   },
   "source": [
    "# Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FigoXm_JU0aV"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LeakyReLU, GRU\n",
    "from tensorflow.keras.models import Sequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YqE1brLYVD1R"
   },
   "outputs": [],
   "source": [
    "num_words = 10000  # Size of vocabulary obtained when preprocessing text data\n",
    "embeddings_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kExeJX3UUgRX"
   },
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QKw6WKnQpITA"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "colab_type": "code",
    "id": "XfMaM4qdYgZ4",
    "outputId": "5159e558-22a9-431b-fe37-bf5e24cd5e8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting category_encoders\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/44/57/fcef41c248701ee62e8325026b90c432adea35555cbc870aff9cfba23727/category_encoders-2.2.2-py2.py3-none-any.whl (80kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 2.2MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.5.1)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.10.2)\n",
      "Requirement already satisfied: pandas>=0.21.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (1.0.5)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (1.18.5)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.22.2.post1)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (1.4.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from patsy>=0.5.1->category_encoders) (1.15.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21.1->category_encoders) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21.1->category_encoders) (2.8.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20.0->category_encoders) (0.16.0)\n",
      "Installing collected packages: category-encoders\n",
      "Successfully installed category-encoders-2.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "HnVGPqUmYlmq",
    "outputId": "676bc50f-f8c7-4fa2-92d7-d610f00d3358"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "from category_encoders import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r_jz7Y0fYrHK"
   },
   "outputs": [],
   "source": [
    "ce = OneHotEncoder(use_cat_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q8KdbJOdYx9R"
   },
   "outputs": [],
   "source": [
    "subreddits = ce.fit_transform(reddit_df['subreddit'])\n",
    "reddit_df = pd.concat([reddit_df, subreddits], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lcBcH9XpyN_M"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    reddit_df.drop([\"up_votes\", 'subreddit'], axis=1), reddit_df[\"up_votes\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Ug81j8-LZMoy",
    "outputId": "02a306d0-feb2-4b5c-b091-00afecfe58cf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subreddit_r/AskReddit</th>\n",
       "      <th>subreddit_r/gaming</th>\n",
       "      <th>subreddit_r/aww</th>\n",
       "      <th>subreddit_r/Music</th>\n",
       "      <th>subreddit_r/science</th>\n",
       "      <th>subreddit_r/worldnews</th>\n",
       "      <th>subreddit_r/videos</th>\n",
       "      <th>subreddit_r/movies</th>\n",
       "      <th>subreddit_r/Showerthoughts</th>\n",
       "      <th>subreddit_r/IAmA</th>\n",
       "      <th>subreddit_r/EarthPorn</th>\n",
       "      <th>subreddit_r/askscience</th>\n",
       "      <th>subreddit_r/Jokes</th>\n",
       "      <th>subreddit_r/explainlikeimfive</th>\n",
       "      <th>subreddit_r/books</th>\n",
       "      <th>subreddit_r/LifeProTips</th>\n",
       "      <th>subreddit_r/blog</th>\n",
       "      <th>subreddit_r/DIY</th>\n",
       "      <th>subreddit_r/sports</th>\n",
       "      <th>subreddit_r/nottheonion</th>\n",
       "      <th>subreddit_r/space</th>\n",
       "      <th>subreddit_r/gadgets</th>\n",
       "      <th>subreddit_r/television</th>\n",
       "      <th>subreddit_r/GetMotivated</th>\n",
       "      <th>subreddit_r/photoshopbattles</th>\n",
       "      <th>subreddit_r/listentothis</th>\n",
       "      <th>subreddit_r/UpliftingNews</th>\n",
       "      <th>subreddit_r/tifu</th>\n",
       "      <th>subreddit_r/InternetIsBeautiful</th>\n",
       "      <th>subreddit_r/history</th>\n",
       "      <th>subreddit_r/philosophy</th>\n",
       "      <th>subreddit_r/Futurology</th>\n",
       "      <th>subreddit_r/OldSchoolCool</th>\n",
       "      <th>subreddit_r/WritingPrompts</th>\n",
       "      <th>subreddit_r/nosleep</th>\n",
       "      <th>subreddit_r/personalfinance</th>\n",
       "      <th>subreddit_r/creepy</th>\n",
       "      <th>subreddit_r/TwoXChromosomes</th>\n",
       "      <th>subreddit_r/technology</th>\n",
       "      <th>subreddit_r/Fitness</th>\n",
       "      <th>subreddit_r/wholesomememes</th>\n",
       "      <th>subreddit_r/politics</th>\n",
       "      <th>subreddit_r/interestingasfuck</th>\n",
       "      <th>subreddit_r/WTF</th>\n",
       "      <th>subreddit_r/bestof</th>\n",
       "      <th>subreddit_r/travel</th>\n",
       "      <th>subreddit_r/oddlysatisfying</th>\n",
       "      <th>subreddit_r/leagueoflegends</th>\n",
       "      <th>subreddit_r/me_irl</th>\n",
       "      <th>subreddit_r/lifehacks</th>\n",
       "      <th>subreddit_r/NatureIsFuckingLit</th>\n",
       "      <th>subreddit_r/pcmasterrace</th>\n",
       "      <th>subreddit_r/dankmemes</th>\n",
       "      <th>subreddit_r/Whatcouldgowrong</th>\n",
       "      <th>subreddit_r/Tinder</th>\n",
       "      <th>subreddit_r/relationship_advice</th>\n",
       "      <th>subreddit_r/Minecraft</th>\n",
       "      <th>subreddit_r/PS4</th>\n",
       "      <th>subreddit_r/nba</th>\n",
       "      <th>subreddit_r/woahdude</th>\n",
       "      <th>subreddit_r/FoodPorn</th>\n",
       "      <th>subreddit_r/photography</th>\n",
       "      <th>subreddit_r/Overwatch</th>\n",
       "      <th>subreddit_r/Unexpected</th>\n",
       "      <th>subreddit_r/dadjokes</th>\n",
       "      <th>subreddit_r/relationships</th>\n",
       "      <th>subreddit_r/boardgames</th>\n",
       "      <th>subreddit_r/instant_regret</th>\n",
       "      <th>subreddit_r/programming</th>\n",
       "      <th>subreddit_r/PublicFreakout</th>\n",
       "      <th>subreddit_r/pokemon</th>\n",
       "      <th>subreddit_r/pokemongo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6255</th>\n",
       "      <td>I'm dyslexic so reading has always been hard f...</td>\n",
       "      <td>I'm 35 years old and have struggled with dysle...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18012</th>\n",
       "      <td>Through the Ages nearly giving me an aneurysm.</td>\n",
       "      <td>I was playing the new Through the Ages IOS app...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8379</th>\n",
       "      <td>TIFU by peeing in a girls mouth thinking i was...</td>\n",
       "      <td>Me and this girl met up after ages of speaking...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16092</th>\n",
       "      <td>So today my five-year-old daughter made me pro...</td>\n",
       "      <td>She was eating watermelon, and she wanted to k...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7374</th>\n",
       "      <td>LPT: When writing an email, leave the To-field...</td>\n",
       "      <td>This avoids sending an unfinished email by acc...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6314</th>\n",
       "      <td>So my date took me to a bookstore yesterday...</td>\n",
       "      <td>I hope this isn't off topic for this subreddit...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6905</th>\n",
       "      <td>LPT — Quickly find out whether a power outage ...</td>\n",
       "      <td>Generally, no power = no router = no Wi-Fi = n...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7097</th>\n",
       "      <td>LPT: If you live far away from a young family ...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>What song did you fall in love with not becaus...</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1082</th>\n",
       "      <td>Seven years ago today the world lost the God o...</td>\n",
       "      <td>First off, let me plug Dio's charity The Ronni...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13932 rows × 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  ... subreddit_r/pokemongo\n",
       "6255   I'm dyslexic so reading has always been hard f...  ...                     0\n",
       "18012     Through the Ages nearly giving me an aneurysm.  ...                     0\n",
       "8379   TIFU by peeing in a girls mouth thinking i was...  ...                     0\n",
       "16092  So today my five-year-old daughter made me pro...  ...                     0\n",
       "7374   LPT: When writing an email, leave the To-field...  ...                     0\n",
       "...                                                  ...  ...                   ...\n",
       "6314      So my date took me to a bookstore yesterday...  ...                     0\n",
       "6905   LPT — Quickly find out whether a power outage ...  ...                     0\n",
       "7097   LPT: If you live far away from a young family ...  ...                     0\n",
       "376    What song did you fall in love with not becaus...  ...                     0\n",
       "1082   Seven years ago today the world lost the God o...  ...                     0\n",
       "\n",
       "[13932 rows x 74 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zuEqj7gfFA0W"
   },
   "outputs": [],
   "source": [
    "def tokenize_and_pad(train, test, col):\n",
    "    tokenizer = Tokenizer(num_words=num_words)\n",
    "    tokenizer.fit_on_texts(train[col])\n",
    "    train_tokens = tokenizer.texts_to_sequences(train[col])\n",
    "    train_len = max([len(x) for x in train_tokens])\n",
    "    train_padded = pad_sequences(train_tokens, train_len, padding=\"post\")\n",
    "    test_tokens = tokenizer.texts_to_sequences(test[col])\n",
    "    test_padded = pad_sequences(test_tokens, train_len, padding=\"post\")\n",
    "    return (train_padded, test_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9uEhBlB_F6xM"
   },
   "outputs": [],
   "source": [
    "X_train_title_padded, X_test_title_padded = tokenize_and_pad(X_train, X_test, 'title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PVpxlSHTG7Vq"
   },
   "outputs": [],
   "source": [
    "X_train_text_padded, X_test_text_padded = tokenize_and_pad(X_train, X_test, 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t7OVYic4minb"
   },
   "outputs": [],
   "source": [
    "title_input = tf.keras.Input(\n",
    "    shape=(None,), name=\"title\"\n",
    ")  # Variable-length sequence of ints\n",
    "body_input = tf.keras.Input(shape=(None,), name=\"body\")  # Variable-length sequence of ints\n",
    "sub_reddit_input = tf.keras.Input(shape=(len(set(subreddits))), name=\"subreddit\")\n",
    "\n",
    "# Embed each word in the title into a 64-dimensional vector\n",
    "title_features = Embedding(num_words, embeddings_size)(title_input)\n",
    "# Embed each word in the text into a 64-dimensional vector\n",
    "body_features = Embedding(num_words, embeddings_size)(body_input)\n",
    "\n",
    "# Reduce sequence of embedded words in the title into a single 128-dimensional vector\n",
    "title_features = GRU(128, return_sequences=True)(title_features)\n",
    "title_features = GRU(128, return_sequences=True)(title_features)\n",
    "title_features = GRU(64)(title_features)\n",
    "# Reduce sequence of embedded words in the body into a single 32-dimensional vector\n",
    "body_features = GRU(125, return_sequences=True)(body_features)\n",
    "body_features = GRU(64)(body_features)\n",
    "\n",
    "# Merge all available features into a single large vector via concatenation\n",
    "x = tf.keras.layers.concatenate([title_features, body_features, sub_reddit_input])\n",
    "\n",
    "# Stick a logistic regression for priority prediction on top of the features\n",
    "# x = Dense(1024)(x)\n",
    "# x = LeakyReLU()(x)\n",
    "# # x = Dropout(0.5)(x)\n",
    "# x = Dense(1024)(x)\n",
    "# x = LeakyReLU()(x)\n",
    "# # x = Dropout(0.5)(x)\n",
    "# x = Dense(1024)(x)\n",
    "# x = LeakyReLU()(x)\n",
    "# # x = Dropout(0.5)(x)\n",
    "x = Dense(1028)(x)\n",
    "x = LeakyReLU()(x)\n",
    "# # x = Dropout(0.5)(x)\n",
    "x = Dense(1028)(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(1024)(x)\n",
    "x = LeakyReLU()(x)\n",
    "upvote_predictor = Dense(1, name=\"upvotes\")(x)\n",
    "\n",
    "model = tf.keras.Model([title_input, body_input, sub_reddit_input],upvote_predictor )\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(epsilon=0.1)\n",
    "# loss = tf.keras.losses.MeanSquaredError()\n",
    "# loss = tf.keras.losses.MeanSquaredLogarithmicError()\n",
    "loss = tf.keras.losses.Huber(10000)\n",
    "\n",
    "model.compile (\n",
    "    optimizer, loss, [\"mae\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VfsMHnUIjvt0"
   },
   "outputs": [],
   "source": [
    "stopping = tf.keras.callbacks.EarlyStopping('val_mae', 1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 318
    },
    "colab_type": "code",
    "id": "9jznJzA30g3n",
    "outputId": "498c01ca-24b8-4f8a-e796-bd227e9a2026"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "436/436 [==============================] - 351s 806ms/step - loss: 81224608.0000 - mae: 11704.1045 - val_loss: 33662172.0000 - val_mae: 6026.3867\n",
      "Epoch 2/1000\n",
      "436/436 [==============================] - 350s 804ms/step - loss: 36300516.0000 - mae: 6452.1855 - val_loss: 32536794.0000 - val_mae: 5881.9609\n",
      "Epoch 3/1000\n",
      "436/436 [==============================] - 351s 805ms/step - loss: 35383592.0000 - mae: 6301.9146 - val_loss: 33973260.0000 - val_mae: 6403.3735\n",
      "Epoch 4/1000\n",
      "436/436 [==============================] - 350s 803ms/step - loss: 35049700.0000 - mae: 6274.6548 - val_loss: 34242232.0000 - val_mae: 6430.6387\n",
      "Epoch 5/1000\n",
      "436/436 [==============================] - 352s 807ms/step - loss: 34070032.0000 - mae: 6127.7695 - val_loss: 32378198.0000 - val_mae: 5968.7798\n",
      "Epoch 6/1000\n",
      "436/436 [==============================] - 351s 806ms/step - loss: 32319522.0000 - mae: 5891.8291 - val_loss: 35019108.0000 - val_mae: 6105.5854\n",
      "Epoch 7/1000\n",
      "436/436 [==============================] - 350s 803ms/step - loss: 31717448.0000 - mae: 5835.3994 - val_loss: 33908488.0000 - val_mae: 6064.9355\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff479eec320>"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    {\n",
    "        \"title\": X_train_title_padded, \n",
    "     \"body\": X_train_text_padded,\n",
    "     \"subreddit\": X_train.drop([\"text\",\"title\"], axis = 1)}, y_train,\n",
    "    epochs=1000, validation_data=(\n",
    "        {\n",
    "            \"title\": X_test_title_padded, \n",
    "         \"body\": X_test_text_padded,\n",
    "         \"subreddit\": X_test.drop([\"text\",\"title\"], axis = 1)}\n",
    "         , y_test),\n",
    "    callbacks=[stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EFVQJlealv29"
   },
   "outputs": [],
   "source": [
    "model.evaluate({\"title\": X_test_title_padded, \"body\": X_test_text_padded}, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 148
    },
    "colab_type": "code",
    "id": "jTjN2sdGlx6W",
    "outputId": "dd58aaf6-49b5-4523-f3a0-9319d0d3458b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,    0,    0,    0],\n",
       "       [  26,   79,    1, ...,    0,    0,    0],\n",
       "       [1658, 1210, 2420, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [  26,  737,   19, ...,    0,    0,    0],\n",
       "       [   2,  105,  818, ...,    0,    0,    0],\n",
       "       [  26,   71, 4019, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 71,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_text_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 148
    },
    "colab_type": "code",
    "id": "838svqsel2FZ",
    "outputId": "259e1db4-ab05-4688-96d8-dfc7990d30fe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  21,   54,  170, ...,    0,    0,    0],\n",
       "       [   8,  737,  141, ...,    0,    0,    0],\n",
       "       [ 937, 4264, 1437, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [  84,    5,    6, ...,    0,    0,    0],\n",
       "       [  24,  231,    1, ...,    0,    0,    0],\n",
       "       [  42,   22, 3116, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 72,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_title_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C8GNaTWh2OGf"
   },
   "outputs": [],
   "source": [
    "model.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DtvjE0v_8v2z"
   },
   "source": [
    "# Sklearn with Basilica embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "colab_type": "code",
    "id": "44uuww4w78zU",
    "outputId": "5c20776b-bef5-498d-c2d8-f16f1fc1c10d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting basilica\n",
      "  Downloading https://files.pythonhosted.org/packages/68/19/6216f1c0ad6d0f738bd1061cb5c65097021b41f3891046fac87bc4c4e1ae/basilica-0.2.8.tar.gz\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from basilica) (2.23.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from basilica) (1.15.0)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from basilica) (7.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->basilica) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->basilica) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->basilica) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->basilica) (1.24.3)\n",
      "Building wheels for collected packages: basilica\n",
      "  Building wheel for basilica (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for basilica: filename=basilica-0.2.8-cp36-none-any.whl size=4711 sha256=ff6baafd57ca885f13cf73d4dbc33e5370c7d1941c2171ba38a814394db1024b\n",
      "  Stored in directory: /root/.cache/pip/wheels/31/18/9f/46f6face8baf98e31b52bf91a0d76930ec76860f9e9211104d\n",
      "Successfully built basilica\n",
      "Installing collected packages: basilica\n",
      "Successfully installed basilica-0.2.8\n"
     ]
    }
   ],
   "source": [
    "!pip install basilica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ov_Ja5_h3LpH"
   },
   "outputs": [],
   "source": [
    "import basilica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FTOPZ0c42NRi"
   },
   "outputs": [],
   "source": [
    "def series_iterator(s:pd.Series):\n",
    "    for i, v in s.iteritems():\n",
    "        yield v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PPazCw27z3im"
   },
   "outputs": [],
   "source": [
    "def embed_sentences(sentences:pd.Series):\n",
    "    with basilica.Connection('370a60d1-2938-b1bf-d813-0cb6954f5a0e') as c:\n",
    "        embeddings = c.embed_sentences(\n",
    "            series_iterator(sentences),\n",
    "             model='reddit',\n",
    "              batch_size = 200,\n",
    "              timeout=120\n",
    "              )\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TKIJNOAs2r9G"
   },
   "outputs": [],
   "source": [
    "titles = reddit_df['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 242
    },
    "colab_type": "code",
    "id": "xafO6iOW2xo2",
    "outputId": "db303123-b57b-4a9e-d342-ab13fafe8a34"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        People who haven't pooped in 2019 yet, why are...\n",
       "1        Would you watch a show where a billionaire CEO...\n",
       "2        How would you feel about a feature where if so...\n",
       "3                 Stan Lee has passed away at 95 years old\n",
       "4        Reddit, how would you feel about a law that ba...\n",
       "                               ...                        \n",
       "18572                       Pokemon Go Update [2016-08-08]\n",
       "18573    It boggles my mind that there is no way to sig...\n",
       "18574    Is it really a coincidence that both Niantic a...\n",
       "18575    The three footstep glitch has turned the game ...\n",
       "18576    I feel like transfering second and third stage...\n",
       "Name: title, Length: 18577, dtype: object"
      ]
     },
     "execution_count": 77,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TzMSVo_e3EiI"
   },
   "outputs": [],
   "source": [
    "title_embeddings = embed_sentences(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "2NKhKZwg3Jeu",
    "outputId": "626166eb-2557-4cd5-c898-693c907eff45"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Connection.embed at 0x7f587c70c9e8>"
      ]
     },
     "execution_count": 79,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 761
    },
    "colab_type": "code",
    "id": "CRErbCsv3P1s",
    "outputId": "0a300858-8988-42d6-ab04-e364aa7e9346"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2645\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2646\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'title_embeddings'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-318-993abc9fde64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreddit_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title_embeddings'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreddit_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title_embeddings'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2798\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2799\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2800\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2801\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2802\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2646\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2648\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2649\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'title_embeddings'"
     ]
    }
   ],
   "source": [
    "reddit_df['title_embeddings'] = pd.Series(reddit_df['title_embeddings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "colab_type": "code",
    "id": "Iho12a9x3e4c",
    "outputId": "cc3651fc-4a9a-4de3-c45a-ca996b708a69"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [0.182144, -0.0526975, 0.296982, -0.0468543, 0...\n",
       "1    [0.0903963, -0.0630344, 0.183588, -0.132425, 0...\n",
       "2    [0.20443, -0.299815, 0.278582, -0.330959, 0.28...\n",
       "3    [0.0740549, -0.99451, 0.0833148, 0.17379, 0.43...\n",
       "4    [0.254428, -0.0114105, 0.334044, -0.152447, 0....\n",
       "Name: title_embeddings, dtype: object"
      ]
     },
     "execution_count": 81,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_df['title_embeddings'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "colab_type": "code",
    "id": "QQMNlZMl3j5z",
    "outputId": "c7fa23e6-ca69-4662-9c7b-ff0cbd09c4a9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [-0.239577, 0.182857, 0.284122, -0.0861422, 0....\n",
       "1    [-0.239577, 0.182857, 0.284122, -0.0861422, 0....\n",
       "2    [-0.239577, 0.182857, 0.284122, -0.0861422, 0....\n",
       "3    [0.0186579, -1.24461, 0.382696, 0.328033, 0.07...\n",
       "4    [-0.239577, 0.182857, 0.284122, -0.0861422, 0....\n",
       "Name: text_embeddings, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = reddit_df['text']\n",
    "text_embeddings = embed_sentences(text)\n",
    "reddit_df['text_embeddings'] = pd.Series(text_embeddings)\n",
    "reddit_df['text_embeddings'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "avkF1AanIv2E",
    "outputId": "29854380-82d4-446d-80e0-35dac3792eff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'up_votes', 'subreddit', 'title_embeddings', 'text_embeddings'], dtype='object')"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zWLxLcUY7_vr"
   },
   "outputs": [],
   "source": [
    "df.to_parquet(\"embeddings.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v2lw2i2e8HO7"
   },
   "outputs": [],
   "source": [
    "df['title_embeddings'] = pd.Series(df['title_embeddings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_XqdaH34pgsL"
   },
   "outputs": [],
   "source": [
    "df['text_embeddings'] = pd.Series(df['text_embeddings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "id": "sp7bFKdlpmmV",
    "outputId": "98e20a41-2b85-4036-d06e-c21c5b9f4e2c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>up_votes</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title_embeddings</th>\n",
       "      <th>text_embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>221854</td>\n",
       "      <td>r/AskReddit</td>\n",
       "      <td>[0.182144, -0.0526975, 0.296982, -0.0468543, 0...</td>\n",
       "      <td>[-0.239577, 0.182857, 0.284122, -0.0861422, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>197524</td>\n",
       "      <td>r/AskReddit</td>\n",
       "      <td>[0.0903963, -0.0630344, 0.183588, -0.132425, 0...</td>\n",
       "      <td>[-0.239577, 0.182857, 0.284122, -0.0861422, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>186368</td>\n",
       "      <td>r/AskReddit</td>\n",
       "      <td>[0.20443, -0.299815, 0.278582, -0.330959, 0.28...</td>\n",
       "      <td>[-0.239577, 0.182857, 0.284122, -0.0861422, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>175339</td>\n",
       "      <td>r/AskReddit</td>\n",
       "      <td>[0.0740549, -0.99451, 0.0833148, 0.17379, 0.43...</td>\n",
       "      <td>[0.0186579, -1.24461, 0.382696, 0.328033, 0.07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>160311</td>\n",
       "      <td>r/AskReddit</td>\n",
       "      <td>[0.254428, -0.0114105, 0.334044, -0.152447, 0....</td>\n",
       "      <td>[-0.239577, 0.182857, 0.284122, -0.0861422, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   up_votes  ...                                    text_embeddings\n",
       "0    221854  ...  [-0.239577, 0.182857, 0.284122, -0.0861422, 0....\n",
       "1    197524  ...  [-0.239577, 0.182857, 0.284122, -0.0861422, 0....\n",
       "2    186368  ...  [-0.239577, 0.182857, 0.284122, -0.0861422, 0....\n",
       "3    175339  ...  [0.0186579, -1.24461, 0.382696, 0.328033, 0.07...\n",
       "4    160311  ...  [-0.239577, 0.182857, 0.284122, -0.0861422, 0....\n",
       "\n",
       "[5 rows x 4 columns]"
      ]
     },
     "execution_count": 328,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4jMmnX6u3_c2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_pickle(\"embeddings2.pkl.xz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cyZ1T3wL54jL"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8kCCTwtJr16B"
   },
   "outputs": [],
   "source": [
    "df = expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CV96OskO64SA"
   },
   "outputs": [],
   "source": [
    "X = df.drop([\"up_votes\", \"subreddit\"], axis=1)\n",
    "y = df['up_votes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kU_8jeJJ8zq0"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CkMEW1_d9YGw"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor as linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pfilzU4kHr0r"
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor as linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9CsCM6dXJa9Y"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor as linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yRICioJt9cGs"
   },
   "outputs": [],
   "source": [
    "model = linear_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 167
    },
    "colab_type": "code",
    "id": "yHcIBZqv9hPs",
    "outputId": "7c9a049d-9733-4348-f3b5-e4dc467f92ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:42:14] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
       "             max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "             silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 365,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "X4awphQxFWsW",
    "outputId": "1f3545f0-9501-480e-f7c6-41a3bf6daa87"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4518445083917112"
      ]
     },
     "execution_count": 366,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i2GsCIqMGzD-"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "qxBoNILkHEj7",
    "outputId": "1a71d1d9-12ee-4222-91a8-7c890ee97ac0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "289856989.2280188"
      ]
     },
     "execution_count": 368,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "Y3d_-rF3slSW",
    "outputId": "73d3a47f-11c1-4135-b0e9-eea66e976d08"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11289.522104992164"
      ]
     },
     "execution_count": 369,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lNfUlGE59lFg"
   },
   "outputs": [],
   "source": [
    "zero = df['text_embeddings'].apply(lambda x: list(x))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "cmGvRHtH-4B1",
    "outputId": "0666b48b-2f1a-405a-fbfd-2b80f2e50d76"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 53,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "tofXwSlk-MfE",
    "outputId": "f2fb5c13-ae90-4c0c-91ac-97c82bca18ae"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>...</th>\n",
       "      <th>728</th>\n",
       "      <th>729</th>\n",
       "      <th>730</th>\n",
       "      <th>731</th>\n",
       "      <th>732</th>\n",
       "      <th>733</th>\n",
       "      <th>734</th>\n",
       "      <th>735</th>\n",
       "      <th>736</th>\n",
       "      <th>737</th>\n",
       "      <th>738</th>\n",
       "      <th>739</th>\n",
       "      <th>740</th>\n",
       "      <th>741</th>\n",
       "      <th>742</th>\n",
       "      <th>743</th>\n",
       "      <th>744</th>\n",
       "      <th>745</th>\n",
       "      <th>746</th>\n",
       "      <th>747</th>\n",
       "      <th>748</th>\n",
       "      <th>749</th>\n",
       "      <th>750</th>\n",
       "      <th>751</th>\n",
       "      <th>752</th>\n",
       "      <th>753</th>\n",
       "      <th>754</th>\n",
       "      <th>755</th>\n",
       "      <th>756</th>\n",
       "      <th>757</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.182144</td>\n",
       "      <td>-0.052698</td>\n",
       "      <td>0.296982</td>\n",
       "      <td>-0.046854</td>\n",
       "      <td>0.066862</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.109145</td>\n",
       "      <td>0.115977</td>\n",
       "      <td>-0.300374</td>\n",
       "      <td>0.012484</td>\n",
       "      <td>0.142182</td>\n",
       "      <td>-0.068124</td>\n",
       "      <td>-0.208033</td>\n",
       "      <td>0.149216</td>\n",
       "      <td>0.046058</td>\n",
       "      <td>0.014590</td>\n",
       "      <td>0.069055</td>\n",
       "      <td>0.108952</td>\n",
       "      <td>0.146693</td>\n",
       "      <td>-0.115352</td>\n",
       "      <td>0.096257</td>\n",
       "      <td>-0.078912</td>\n",
       "      <td>-0.141155</td>\n",
       "      <td>-0.117024</td>\n",
       "      <td>0.098890</td>\n",
       "      <td>-0.202187</td>\n",
       "      <td>0.057147</td>\n",
       "      <td>0.059443</td>\n",
       "      <td>0.018391</td>\n",
       "      <td>0.045539</td>\n",
       "      <td>0.011138</td>\n",
       "      <td>-0.157200</td>\n",
       "      <td>-0.041015</td>\n",
       "      <td>0.011421</td>\n",
       "      <td>-0.031243</td>\n",
       "      <td>0.112957</td>\n",
       "      <td>-0.087513</td>\n",
       "      <td>-0.060702</td>\n",
       "      <td>0.018499</td>\n",
       "      <td>-0.012571</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.079505</td>\n",
       "      <td>0.104720</td>\n",
       "      <td>-0.026521</td>\n",
       "      <td>-0.118880</td>\n",
       "      <td>-0.133549</td>\n",
       "      <td>0.286074</td>\n",
       "      <td>0.133212</td>\n",
       "      <td>0.056164</td>\n",
       "      <td>0.017515</td>\n",
       "      <td>0.037554</td>\n",
       "      <td>-0.028962</td>\n",
       "      <td>0.015284</td>\n",
       "      <td>-0.267593</td>\n",
       "      <td>0.120774</td>\n",
       "      <td>-0.010770</td>\n",
       "      <td>0.151480</td>\n",
       "      <td>0.093855</td>\n",
       "      <td>0.009536</td>\n",
       "      <td>-0.033602</td>\n",
       "      <td>-0.096230</td>\n",
       "      <td>0.050461</td>\n",
       "      <td>0.054975</td>\n",
       "      <td>-0.051313</td>\n",
       "      <td>0.039797</td>\n",
       "      <td>-9.00595</td>\n",
       "      <td>0.057208</td>\n",
       "      <td>-0.009514</td>\n",
       "      <td>0.064110</td>\n",
       "      <td>-0.130201</td>\n",
       "      <td>0.078111</td>\n",
       "      <td>-0.004505</td>\n",
       "      <td>-0.211739</td>\n",
       "      <td>-0.026336</td>\n",
       "      <td>0.000743</td>\n",
       "      <td>0.333825</td>\n",
       "      <td>-0.151763</td>\n",
       "      <td>-0.064953</td>\n",
       "      <td>0.050184</td>\n",
       "      <td>-0.023745</td>\n",
       "      <td>-0.108041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.090396</td>\n",
       "      <td>-0.063034</td>\n",
       "      <td>0.183588</td>\n",
       "      <td>-0.132425</td>\n",
       "      <td>0.218097</td>\n",
       "      <td>-0.061284</td>\n",
       "      <td>0.194140</td>\n",
       "      <td>0.099648</td>\n",
       "      <td>-0.110126</td>\n",
       "      <td>-0.059502</td>\n",
       "      <td>0.033862</td>\n",
       "      <td>0.062735</td>\n",
       "      <td>-0.222219</td>\n",
       "      <td>0.129082</td>\n",
       "      <td>-0.075026</td>\n",
       "      <td>0.102772</td>\n",
       "      <td>0.047288</td>\n",
       "      <td>0.282739</td>\n",
       "      <td>0.116910</td>\n",
       "      <td>-0.196864</td>\n",
       "      <td>-0.068466</td>\n",
       "      <td>-0.230079</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-0.075219</td>\n",
       "      <td>0.086746</td>\n",
       "      <td>-0.081459</td>\n",
       "      <td>0.198271</td>\n",
       "      <td>0.131429</td>\n",
       "      <td>0.164833</td>\n",
       "      <td>0.104517</td>\n",
       "      <td>0.061296</td>\n",
       "      <td>-0.225312</td>\n",
       "      <td>0.071521</td>\n",
       "      <td>-0.182611</td>\n",
       "      <td>-0.167435</td>\n",
       "      <td>0.170808</td>\n",
       "      <td>-0.015743</td>\n",
       "      <td>-0.016329</td>\n",
       "      <td>0.002535</td>\n",
       "      <td>0.039329</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027212</td>\n",
       "      <td>0.175481</td>\n",
       "      <td>-0.031569</td>\n",
       "      <td>-0.048201</td>\n",
       "      <td>-0.122848</td>\n",
       "      <td>0.264615</td>\n",
       "      <td>0.055259</td>\n",
       "      <td>-0.122896</td>\n",
       "      <td>-0.073748</td>\n",
       "      <td>0.025588</td>\n",
       "      <td>-0.026207</td>\n",
       "      <td>0.110434</td>\n",
       "      <td>-0.153051</td>\n",
       "      <td>0.239316</td>\n",
       "      <td>0.075326</td>\n",
       "      <td>0.244874</td>\n",
       "      <td>0.320699</td>\n",
       "      <td>0.046222</td>\n",
       "      <td>-0.022302</td>\n",
       "      <td>-0.087042</td>\n",
       "      <td>-0.110124</td>\n",
       "      <td>0.065037</td>\n",
       "      <td>-0.062415</td>\n",
       "      <td>0.060719</td>\n",
       "      <td>-8.97873</td>\n",
       "      <td>0.044415</td>\n",
       "      <td>0.048184</td>\n",
       "      <td>-0.005165</td>\n",
       "      <td>-0.039495</td>\n",
       "      <td>0.020332</td>\n",
       "      <td>-0.104020</td>\n",
       "      <td>-0.159283</td>\n",
       "      <td>0.081489</td>\n",
       "      <td>-0.094288</td>\n",
       "      <td>0.081652</td>\n",
       "      <td>-0.091267</td>\n",
       "      <td>0.003904</td>\n",
       "      <td>-0.030987</td>\n",
       "      <td>0.065567</td>\n",
       "      <td>0.015198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.204430</td>\n",
       "      <td>-0.299815</td>\n",
       "      <td>0.278582</td>\n",
       "      <td>-0.330959</td>\n",
       "      <td>0.283490</td>\n",
       "      <td>0.167636</td>\n",
       "      <td>-0.000396</td>\n",
       "      <td>0.186616</td>\n",
       "      <td>-0.080640</td>\n",
       "      <td>-0.002074</td>\n",
       "      <td>0.083603</td>\n",
       "      <td>0.029153</td>\n",
       "      <td>-0.071468</td>\n",
       "      <td>-0.101955</td>\n",
       "      <td>-0.088183</td>\n",
       "      <td>0.027302</td>\n",
       "      <td>-0.042051</td>\n",
       "      <td>0.134516</td>\n",
       "      <td>0.101671</td>\n",
       "      <td>-0.028426</td>\n",
       "      <td>0.077124</td>\n",
       "      <td>-0.186997</td>\n",
       "      <td>-0.313275</td>\n",
       "      <td>-0.250894</td>\n",
       "      <td>0.080097</td>\n",
       "      <td>-0.078265</td>\n",
       "      <td>0.297395</td>\n",
       "      <td>0.140284</td>\n",
       "      <td>-0.008929</td>\n",
       "      <td>0.023978</td>\n",
       "      <td>0.057998</td>\n",
       "      <td>-0.050854</td>\n",
       "      <td>0.088528</td>\n",
       "      <td>0.067004</td>\n",
       "      <td>-0.133049</td>\n",
       "      <td>0.062508</td>\n",
       "      <td>-0.102993</td>\n",
       "      <td>-0.093009</td>\n",
       "      <td>-0.085430</td>\n",
       "      <td>0.076165</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013532</td>\n",
       "      <td>0.145643</td>\n",
       "      <td>0.047052</td>\n",
       "      <td>-0.193214</td>\n",
       "      <td>-0.099032</td>\n",
       "      <td>0.441819</td>\n",
       "      <td>-0.026864</td>\n",
       "      <td>0.064393</td>\n",
       "      <td>-0.085460</td>\n",
       "      <td>-0.063976</td>\n",
       "      <td>0.001207</td>\n",
       "      <td>0.048360</td>\n",
       "      <td>-0.295799</td>\n",
       "      <td>0.250361</td>\n",
       "      <td>0.007193</td>\n",
       "      <td>0.002607</td>\n",
       "      <td>0.367675</td>\n",
       "      <td>0.064131</td>\n",
       "      <td>-0.044522</td>\n",
       "      <td>-0.257064</td>\n",
       "      <td>-0.175361</td>\n",
       "      <td>0.061791</td>\n",
       "      <td>-0.198281</td>\n",
       "      <td>-0.037277</td>\n",
       "      <td>-9.16263</td>\n",
       "      <td>0.137411</td>\n",
       "      <td>0.065680</td>\n",
       "      <td>0.157470</td>\n",
       "      <td>-0.289391</td>\n",
       "      <td>0.087019</td>\n",
       "      <td>-0.099552</td>\n",
       "      <td>-0.125894</td>\n",
       "      <td>0.119627</td>\n",
       "      <td>-0.012324</td>\n",
       "      <td>0.406567</td>\n",
       "      <td>-0.109556</td>\n",
       "      <td>-0.028680</td>\n",
       "      <td>0.201417</td>\n",
       "      <td>-0.064270</td>\n",
       "      <td>0.095350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.074055</td>\n",
       "      <td>-0.994510</td>\n",
       "      <td>0.083315</td>\n",
       "      <td>0.173790</td>\n",
       "      <td>0.435514</td>\n",
       "      <td>0.489149</td>\n",
       "      <td>-0.442962</td>\n",
       "      <td>-0.515874</td>\n",
       "      <td>-0.247308</td>\n",
       "      <td>-0.210775</td>\n",
       "      <td>0.038379</td>\n",
       "      <td>0.199510</td>\n",
       "      <td>-0.283324</td>\n",
       "      <td>-0.483493</td>\n",
       "      <td>-0.898885</td>\n",
       "      <td>0.334158</td>\n",
       "      <td>0.353439</td>\n",
       "      <td>-1.088240</td>\n",
       "      <td>-1.086190</td>\n",
       "      <td>0.170191</td>\n",
       "      <td>-0.323830</td>\n",
       "      <td>-0.223867</td>\n",
       "      <td>-0.157821</td>\n",
       "      <td>-0.040704</td>\n",
       "      <td>-0.135643</td>\n",
       "      <td>0.109425</td>\n",
       "      <td>-0.167469</td>\n",
       "      <td>-0.357555</td>\n",
       "      <td>-0.247309</td>\n",
       "      <td>-0.306711</td>\n",
       "      <td>0.350606</td>\n",
       "      <td>-0.533339</td>\n",
       "      <td>0.937067</td>\n",
       "      <td>0.810881</td>\n",
       "      <td>-0.742399</td>\n",
       "      <td>-0.461994</td>\n",
       "      <td>-0.695902</td>\n",
       "      <td>-0.148049</td>\n",
       "      <td>-0.560312</td>\n",
       "      <td>-0.004402</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.485970</td>\n",
       "      <td>-0.319889</td>\n",
       "      <td>0.264213</td>\n",
       "      <td>-0.837947</td>\n",
       "      <td>0.662875</td>\n",
       "      <td>0.054413</td>\n",
       "      <td>0.096752</td>\n",
       "      <td>-0.052930</td>\n",
       "      <td>-0.357925</td>\n",
       "      <td>0.054988</td>\n",
       "      <td>-0.023470</td>\n",
       "      <td>-0.015924</td>\n",
       "      <td>-0.816854</td>\n",
       "      <td>0.350448</td>\n",
       "      <td>0.005052</td>\n",
       "      <td>-0.324144</td>\n",
       "      <td>0.634460</td>\n",
       "      <td>-0.995354</td>\n",
       "      <td>-0.489603</td>\n",
       "      <td>0.468945</td>\n",
       "      <td>0.522310</td>\n",
       "      <td>1.203780</td>\n",
       "      <td>-0.918136</td>\n",
       "      <td>-0.362671</td>\n",
       "      <td>-1.22464</td>\n",
       "      <td>0.824677</td>\n",
       "      <td>0.418465</td>\n",
       "      <td>0.032358</td>\n",
       "      <td>-0.213618</td>\n",
       "      <td>-0.244904</td>\n",
       "      <td>0.089304</td>\n",
       "      <td>0.333229</td>\n",
       "      <td>0.727644</td>\n",
       "      <td>-0.946214</td>\n",
       "      <td>0.688935</td>\n",
       "      <td>-0.036943</td>\n",
       "      <td>-0.363213</td>\n",
       "      <td>0.905584</td>\n",
       "      <td>1.010070</td>\n",
       "      <td>0.210940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.254428</td>\n",
       "      <td>-0.011411</td>\n",
       "      <td>0.334044</td>\n",
       "      <td>-0.152447</td>\n",
       "      <td>0.132704</td>\n",
       "      <td>-0.040200</td>\n",
       "      <td>0.080763</td>\n",
       "      <td>0.169764</td>\n",
       "      <td>-0.161985</td>\n",
       "      <td>-0.002933</td>\n",
       "      <td>0.075230</td>\n",
       "      <td>0.053451</td>\n",
       "      <td>-0.158602</td>\n",
       "      <td>0.013316</td>\n",
       "      <td>-0.063966</td>\n",
       "      <td>0.150923</td>\n",
       "      <td>0.022072</td>\n",
       "      <td>0.168311</td>\n",
       "      <td>0.124978</td>\n",
       "      <td>-0.229316</td>\n",
       "      <td>0.006148</td>\n",
       "      <td>-0.104214</td>\n",
       "      <td>-0.089166</td>\n",
       "      <td>-0.112851</td>\n",
       "      <td>0.157767</td>\n",
       "      <td>-0.022986</td>\n",
       "      <td>0.275391</td>\n",
       "      <td>0.152245</td>\n",
       "      <td>-0.022349</td>\n",
       "      <td>0.043627</td>\n",
       "      <td>0.068497</td>\n",
       "      <td>-0.072003</td>\n",
       "      <td>0.164927</td>\n",
       "      <td>0.128153</td>\n",
       "      <td>-0.097877</td>\n",
       "      <td>0.112834</td>\n",
       "      <td>0.025989</td>\n",
       "      <td>-0.035945</td>\n",
       "      <td>0.029443</td>\n",
       "      <td>0.111205</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.048833</td>\n",
       "      <td>0.092955</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>-0.139243</td>\n",
       "      <td>-0.096447</td>\n",
       "      <td>0.371564</td>\n",
       "      <td>0.056716</td>\n",
       "      <td>-0.018409</td>\n",
       "      <td>0.003533</td>\n",
       "      <td>-0.065163</td>\n",
       "      <td>0.013406</td>\n",
       "      <td>0.051614</td>\n",
       "      <td>-0.313355</td>\n",
       "      <td>0.178504</td>\n",
       "      <td>-0.017465</td>\n",
       "      <td>0.077003</td>\n",
       "      <td>0.257123</td>\n",
       "      <td>0.087590</td>\n",
       "      <td>-0.035367</td>\n",
       "      <td>-0.082249</td>\n",
       "      <td>-0.172318</td>\n",
       "      <td>0.057436</td>\n",
       "      <td>-0.017893</td>\n",
       "      <td>-0.038150</td>\n",
       "      <td>-9.09304</td>\n",
       "      <td>0.102770</td>\n",
       "      <td>0.073625</td>\n",
       "      <td>0.083781</td>\n",
       "      <td>-0.100582</td>\n",
       "      <td>0.020675</td>\n",
       "      <td>-0.100587</td>\n",
       "      <td>-0.151935</td>\n",
       "      <td>-0.029423</td>\n",
       "      <td>-0.009458</td>\n",
       "      <td>0.317632</td>\n",
       "      <td>0.004331</td>\n",
       "      <td>0.124570</td>\n",
       "      <td>0.114443</td>\n",
       "      <td>-0.095432</td>\n",
       "      <td>-0.035912</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2    ...       765       766       767\n",
       "0  0.182144 -0.052698  0.296982  ...  0.050184 -0.023745 -0.108041\n",
       "1  0.090396 -0.063034  0.183588  ... -0.030987  0.065567  0.015198\n",
       "2  0.204430 -0.299815  0.278582  ...  0.201417 -0.064270  0.095350\n",
       "3  0.074055 -0.994510  0.083315  ...  0.905584  1.010070  0.210940\n",
       "4  0.254428 -0.011411  0.334044  ...  0.114443 -0.095432 -0.035912\n",
       "\n",
       "[5 rows x 768 columns]"
      ]
     },
     "execution_count": 344,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_vector = df['title_embeddings'].apply(pd.Series)\n",
    "title_vector.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g6XA-CoA_lVA"
   },
   "outputs": [],
   "source": [
    "text_vector = df['text_embeddings'].apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WqCNAwJP_snC"
   },
   "outputs": [],
   "source": [
    "expanded = pd.concat((reddit_df[['subreddit', 'up_votes']], title_vector, text_vector), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 242
    },
    "colab_type": "code",
    "id": "csmSlVBu_7yw",
    "outputId": "9094a1c9-95bc-4a2c-f040-9ae943df539c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        221854\n",
       "1        197524\n",
       "2        186368\n",
       "3        175339\n",
       "4        160311\n",
       "          ...  \n",
       "18572      7189\n",
       "18573      7168\n",
       "18574      7117\n",
       "18575      7108\n",
       "18576      7097\n",
       "Name: up_votes, Length: 18577, dtype: int64"
      ]
     },
     "execution_count": 349,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_df['up_votes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "CO4S0wuiAIkA",
    "outputId": "cfd59ba8-47ba-4a65-cc6e-9b43f9bdadd9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>up_votes</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>...</th>\n",
       "      <th>728</th>\n",
       "      <th>729</th>\n",
       "      <th>730</th>\n",
       "      <th>731</th>\n",
       "      <th>732</th>\n",
       "      <th>733</th>\n",
       "      <th>734</th>\n",
       "      <th>735</th>\n",
       "      <th>736</th>\n",
       "      <th>737</th>\n",
       "      <th>738</th>\n",
       "      <th>739</th>\n",
       "      <th>740</th>\n",
       "      <th>741</th>\n",
       "      <th>742</th>\n",
       "      <th>743</th>\n",
       "      <th>744</th>\n",
       "      <th>745</th>\n",
       "      <th>746</th>\n",
       "      <th>747</th>\n",
       "      <th>748</th>\n",
       "      <th>749</th>\n",
       "      <th>750</th>\n",
       "      <th>751</th>\n",
       "      <th>752</th>\n",
       "      <th>753</th>\n",
       "      <th>754</th>\n",
       "      <th>755</th>\n",
       "      <th>756</th>\n",
       "      <th>757</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>r/AskReddit</td>\n",
       "      <td>221854</td>\n",
       "      <td>0.182144</td>\n",
       "      <td>-0.052698</td>\n",
       "      <td>0.296982</td>\n",
       "      <td>-0.046854</td>\n",
       "      <td>0.066862</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.109145</td>\n",
       "      <td>0.115977</td>\n",
       "      <td>-0.300374</td>\n",
       "      <td>0.012484</td>\n",
       "      <td>0.142182</td>\n",
       "      <td>-0.068124</td>\n",
       "      <td>-0.208033</td>\n",
       "      <td>0.149216</td>\n",
       "      <td>0.046058</td>\n",
       "      <td>0.014590</td>\n",
       "      <td>0.069055</td>\n",
       "      <td>0.108952</td>\n",
       "      <td>0.146693</td>\n",
       "      <td>-0.115352</td>\n",
       "      <td>0.096257</td>\n",
       "      <td>-0.078912</td>\n",
       "      <td>-0.141155</td>\n",
       "      <td>-0.117024</td>\n",
       "      <td>0.098890</td>\n",
       "      <td>-0.202187</td>\n",
       "      <td>0.057147</td>\n",
       "      <td>0.059443</td>\n",
       "      <td>0.018391</td>\n",
       "      <td>0.045539</td>\n",
       "      <td>0.011138</td>\n",
       "      <td>-0.157200</td>\n",
       "      <td>-0.041015</td>\n",
       "      <td>0.011421</td>\n",
       "      <td>-0.031243</td>\n",
       "      <td>0.112957</td>\n",
       "      <td>-0.087513</td>\n",
       "      <td>-0.060702</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.204922</td>\n",
       "      <td>0.201669</td>\n",
       "      <td>0.095816</td>\n",
       "      <td>0.088479</td>\n",
       "      <td>-0.163301</td>\n",
       "      <td>0.178585</td>\n",
       "      <td>0.022706</td>\n",
       "      <td>0.072588</td>\n",
       "      <td>-0.085228</td>\n",
       "      <td>0.241856</td>\n",
       "      <td>-0.052063</td>\n",
       "      <td>0.206275</td>\n",
       "      <td>-0.226309</td>\n",
       "      <td>0.055761</td>\n",
       "      <td>-0.013927</td>\n",
       "      <td>-0.047932</td>\n",
       "      <td>0.055732</td>\n",
       "      <td>0.013788</td>\n",
       "      <td>0.058288</td>\n",
       "      <td>0.074495</td>\n",
       "      <td>-0.144048</td>\n",
       "      <td>0.24373</td>\n",
       "      <td>-0.124181</td>\n",
       "      <td>-0.023625</td>\n",
       "      <td>-8.46837</td>\n",
       "      <td>-0.155591</td>\n",
       "      <td>0.134095</td>\n",
       "      <td>0.142500</td>\n",
       "      <td>0.122580</td>\n",
       "      <td>0.353983</td>\n",
       "      <td>0.017859</td>\n",
       "      <td>-0.048209</td>\n",
       "      <td>-0.140820</td>\n",
       "      <td>-0.028225</td>\n",
       "      <td>0.431787</td>\n",
       "      <td>-0.342401</td>\n",
       "      <td>0.033433</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.178056</td>\n",
       "      <td>0.036207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>r/AskReddit</td>\n",
       "      <td>197524</td>\n",
       "      <td>0.090396</td>\n",
       "      <td>-0.063034</td>\n",
       "      <td>0.183588</td>\n",
       "      <td>-0.132425</td>\n",
       "      <td>0.218097</td>\n",
       "      <td>-0.061284</td>\n",
       "      <td>0.194140</td>\n",
       "      <td>0.099648</td>\n",
       "      <td>-0.110126</td>\n",
       "      <td>-0.059502</td>\n",
       "      <td>0.033862</td>\n",
       "      <td>0.062735</td>\n",
       "      <td>-0.222219</td>\n",
       "      <td>0.129082</td>\n",
       "      <td>-0.075026</td>\n",
       "      <td>0.102772</td>\n",
       "      <td>0.047288</td>\n",
       "      <td>0.282739</td>\n",
       "      <td>0.116910</td>\n",
       "      <td>-0.196864</td>\n",
       "      <td>-0.068466</td>\n",
       "      <td>-0.230079</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-0.075219</td>\n",
       "      <td>0.086746</td>\n",
       "      <td>-0.081459</td>\n",
       "      <td>0.198271</td>\n",
       "      <td>0.131429</td>\n",
       "      <td>0.164833</td>\n",
       "      <td>0.104517</td>\n",
       "      <td>0.061296</td>\n",
       "      <td>-0.225312</td>\n",
       "      <td>0.071521</td>\n",
       "      <td>-0.182611</td>\n",
       "      <td>-0.167435</td>\n",
       "      <td>0.170808</td>\n",
       "      <td>-0.015743</td>\n",
       "      <td>-0.016329</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.204922</td>\n",
       "      <td>0.201669</td>\n",
       "      <td>0.095816</td>\n",
       "      <td>0.088479</td>\n",
       "      <td>-0.163301</td>\n",
       "      <td>0.178585</td>\n",
       "      <td>0.022706</td>\n",
       "      <td>0.072588</td>\n",
       "      <td>-0.085228</td>\n",
       "      <td>0.241856</td>\n",
       "      <td>-0.052063</td>\n",
       "      <td>0.206275</td>\n",
       "      <td>-0.226309</td>\n",
       "      <td>0.055761</td>\n",
       "      <td>-0.013927</td>\n",
       "      <td>-0.047932</td>\n",
       "      <td>0.055732</td>\n",
       "      <td>0.013788</td>\n",
       "      <td>0.058288</td>\n",
       "      <td>0.074495</td>\n",
       "      <td>-0.144048</td>\n",
       "      <td>0.24373</td>\n",
       "      <td>-0.124181</td>\n",
       "      <td>-0.023625</td>\n",
       "      <td>-8.46837</td>\n",
       "      <td>-0.155591</td>\n",
       "      <td>0.134095</td>\n",
       "      <td>0.142500</td>\n",
       "      <td>0.122580</td>\n",
       "      <td>0.353983</td>\n",
       "      <td>0.017859</td>\n",
       "      <td>-0.048209</td>\n",
       "      <td>-0.140820</td>\n",
       "      <td>-0.028225</td>\n",
       "      <td>0.431787</td>\n",
       "      <td>-0.342401</td>\n",
       "      <td>0.033433</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.178056</td>\n",
       "      <td>0.036207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>r/AskReddit</td>\n",
       "      <td>186368</td>\n",
       "      <td>0.204430</td>\n",
       "      <td>-0.299815</td>\n",
       "      <td>0.278582</td>\n",
       "      <td>-0.330959</td>\n",
       "      <td>0.283490</td>\n",
       "      <td>0.167636</td>\n",
       "      <td>-0.000396</td>\n",
       "      <td>0.186616</td>\n",
       "      <td>-0.080640</td>\n",
       "      <td>-0.002074</td>\n",
       "      <td>0.083603</td>\n",
       "      <td>0.029153</td>\n",
       "      <td>-0.071468</td>\n",
       "      <td>-0.101955</td>\n",
       "      <td>-0.088183</td>\n",
       "      <td>0.027302</td>\n",
       "      <td>-0.042051</td>\n",
       "      <td>0.134516</td>\n",
       "      <td>0.101671</td>\n",
       "      <td>-0.028426</td>\n",
       "      <td>0.077124</td>\n",
       "      <td>-0.186997</td>\n",
       "      <td>-0.313275</td>\n",
       "      <td>-0.250894</td>\n",
       "      <td>0.080097</td>\n",
       "      <td>-0.078265</td>\n",
       "      <td>0.297395</td>\n",
       "      <td>0.140284</td>\n",
       "      <td>-0.008929</td>\n",
       "      <td>0.023978</td>\n",
       "      <td>0.057998</td>\n",
       "      <td>-0.050854</td>\n",
       "      <td>0.088528</td>\n",
       "      <td>0.067004</td>\n",
       "      <td>-0.133049</td>\n",
       "      <td>0.062508</td>\n",
       "      <td>-0.102993</td>\n",
       "      <td>-0.093009</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.204922</td>\n",
       "      <td>0.201669</td>\n",
       "      <td>0.095816</td>\n",
       "      <td>0.088479</td>\n",
       "      <td>-0.163301</td>\n",
       "      <td>0.178585</td>\n",
       "      <td>0.022706</td>\n",
       "      <td>0.072588</td>\n",
       "      <td>-0.085228</td>\n",
       "      <td>0.241856</td>\n",
       "      <td>-0.052063</td>\n",
       "      <td>0.206275</td>\n",
       "      <td>-0.226309</td>\n",
       "      <td>0.055761</td>\n",
       "      <td>-0.013927</td>\n",
       "      <td>-0.047932</td>\n",
       "      <td>0.055732</td>\n",
       "      <td>0.013788</td>\n",
       "      <td>0.058288</td>\n",
       "      <td>0.074495</td>\n",
       "      <td>-0.144048</td>\n",
       "      <td>0.24373</td>\n",
       "      <td>-0.124181</td>\n",
       "      <td>-0.023625</td>\n",
       "      <td>-8.46837</td>\n",
       "      <td>-0.155591</td>\n",
       "      <td>0.134095</td>\n",
       "      <td>0.142500</td>\n",
       "      <td>0.122580</td>\n",
       "      <td>0.353983</td>\n",
       "      <td>0.017859</td>\n",
       "      <td>-0.048209</td>\n",
       "      <td>-0.140820</td>\n",
       "      <td>-0.028225</td>\n",
       "      <td>0.431787</td>\n",
       "      <td>-0.342401</td>\n",
       "      <td>0.033433</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.178056</td>\n",
       "      <td>0.036207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>r/AskReddit</td>\n",
       "      <td>175339</td>\n",
       "      <td>0.074055</td>\n",
       "      <td>-0.994510</td>\n",
       "      <td>0.083315</td>\n",
       "      <td>0.173790</td>\n",
       "      <td>0.435514</td>\n",
       "      <td>0.489149</td>\n",
       "      <td>-0.442962</td>\n",
       "      <td>-0.515874</td>\n",
       "      <td>-0.247308</td>\n",
       "      <td>-0.210775</td>\n",
       "      <td>0.038379</td>\n",
       "      <td>0.199510</td>\n",
       "      <td>-0.283324</td>\n",
       "      <td>-0.483493</td>\n",
       "      <td>-0.898885</td>\n",
       "      <td>0.334158</td>\n",
       "      <td>0.353439</td>\n",
       "      <td>-1.088240</td>\n",
       "      <td>-1.086190</td>\n",
       "      <td>0.170191</td>\n",
       "      <td>-0.323830</td>\n",
       "      <td>-0.223867</td>\n",
       "      <td>-0.157821</td>\n",
       "      <td>-0.040704</td>\n",
       "      <td>-0.135643</td>\n",
       "      <td>0.109425</td>\n",
       "      <td>-0.167469</td>\n",
       "      <td>-0.357555</td>\n",
       "      <td>-0.247309</td>\n",
       "      <td>-0.306711</td>\n",
       "      <td>0.350606</td>\n",
       "      <td>-0.533339</td>\n",
       "      <td>0.937067</td>\n",
       "      <td>0.810881</td>\n",
       "      <td>-0.742399</td>\n",
       "      <td>-0.461994</td>\n",
       "      <td>-0.695902</td>\n",
       "      <td>-0.148049</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.123386</td>\n",
       "      <td>0.015488</td>\n",
       "      <td>0.403286</td>\n",
       "      <td>-0.721330</td>\n",
       "      <td>0.482571</td>\n",
       "      <td>-0.146677</td>\n",
       "      <td>0.164433</td>\n",
       "      <td>-0.812643</td>\n",
       "      <td>-0.319916</td>\n",
       "      <td>-0.050146</td>\n",
       "      <td>-0.242649</td>\n",
       "      <td>-0.401917</td>\n",
       "      <td>-1.172610</td>\n",
       "      <td>0.069800</td>\n",
       "      <td>0.322633</td>\n",
       "      <td>-0.226635</td>\n",
       "      <td>0.889395</td>\n",
       "      <td>-0.686808</td>\n",
       "      <td>-0.287803</td>\n",
       "      <td>-0.251987</td>\n",
       "      <td>0.591564</td>\n",
       "      <td>1.50253</td>\n",
       "      <td>-0.575015</td>\n",
       "      <td>-0.410068</td>\n",
       "      <td>-3.85364</td>\n",
       "      <td>0.476023</td>\n",
       "      <td>-0.121097</td>\n",
       "      <td>0.007005</td>\n",
       "      <td>-0.425543</td>\n",
       "      <td>-0.352296</td>\n",
       "      <td>-0.120428</td>\n",
       "      <td>-0.059130</td>\n",
       "      <td>0.952337</td>\n",
       "      <td>-1.308660</td>\n",
       "      <td>0.952924</td>\n",
       "      <td>0.261201</td>\n",
       "      <td>-0.729069</td>\n",
       "      <td>0.909033</td>\n",
       "      <td>0.791063</td>\n",
       "      <td>0.122177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>r/AskReddit</td>\n",
       "      <td>160311</td>\n",
       "      <td>0.254428</td>\n",
       "      <td>-0.011411</td>\n",
       "      <td>0.334044</td>\n",
       "      <td>-0.152447</td>\n",
       "      <td>0.132704</td>\n",
       "      <td>-0.040200</td>\n",
       "      <td>0.080763</td>\n",
       "      <td>0.169764</td>\n",
       "      <td>-0.161985</td>\n",
       "      <td>-0.002933</td>\n",
       "      <td>0.075230</td>\n",
       "      <td>0.053451</td>\n",
       "      <td>-0.158602</td>\n",
       "      <td>0.013316</td>\n",
       "      <td>-0.063966</td>\n",
       "      <td>0.150923</td>\n",
       "      <td>0.022072</td>\n",
       "      <td>0.168311</td>\n",
       "      <td>0.124978</td>\n",
       "      <td>-0.229316</td>\n",
       "      <td>0.006148</td>\n",
       "      <td>-0.104214</td>\n",
       "      <td>-0.089166</td>\n",
       "      <td>-0.112851</td>\n",
       "      <td>0.157767</td>\n",
       "      <td>-0.022986</td>\n",
       "      <td>0.275391</td>\n",
       "      <td>0.152245</td>\n",
       "      <td>-0.022349</td>\n",
       "      <td>0.043627</td>\n",
       "      <td>0.068497</td>\n",
       "      <td>-0.072003</td>\n",
       "      <td>0.164927</td>\n",
       "      <td>0.128153</td>\n",
       "      <td>-0.097877</td>\n",
       "      <td>0.112834</td>\n",
       "      <td>0.025989</td>\n",
       "      <td>-0.035945</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.204922</td>\n",
       "      <td>0.201669</td>\n",
       "      <td>0.095816</td>\n",
       "      <td>0.088479</td>\n",
       "      <td>-0.163301</td>\n",
       "      <td>0.178585</td>\n",
       "      <td>0.022706</td>\n",
       "      <td>0.072588</td>\n",
       "      <td>-0.085228</td>\n",
       "      <td>0.241856</td>\n",
       "      <td>-0.052063</td>\n",
       "      <td>0.206275</td>\n",
       "      <td>-0.226309</td>\n",
       "      <td>0.055761</td>\n",
       "      <td>-0.013927</td>\n",
       "      <td>-0.047932</td>\n",
       "      <td>0.055732</td>\n",
       "      <td>0.013788</td>\n",
       "      <td>0.058288</td>\n",
       "      <td>0.074495</td>\n",
       "      <td>-0.144048</td>\n",
       "      <td>0.24373</td>\n",
       "      <td>-0.124181</td>\n",
       "      <td>-0.023625</td>\n",
       "      <td>-8.46837</td>\n",
       "      <td>-0.155591</td>\n",
       "      <td>0.134095</td>\n",
       "      <td>0.142500</td>\n",
       "      <td>0.122580</td>\n",
       "      <td>0.353983</td>\n",
       "      <td>0.017859</td>\n",
       "      <td>-0.048209</td>\n",
       "      <td>-0.140820</td>\n",
       "      <td>-0.028225</td>\n",
       "      <td>0.431787</td>\n",
       "      <td>-0.342401</td>\n",
       "      <td>0.033433</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.178056</td>\n",
       "      <td>0.036207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1538 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     subreddit  up_votes         0  ...       765       766       767\n",
       "0  r/AskReddit    221854  0.182144  ...  0.000455  0.178056  0.036207\n",
       "1  r/AskReddit    197524  0.090396  ...  0.000455  0.178056  0.036207\n",
       "2  r/AskReddit    186368  0.204430  ...  0.000455  0.178056  0.036207\n",
       "3  r/AskReddit    175339  0.074055  ...  0.909033  0.791063  0.122177\n",
       "4  r/AskReddit    160311  0.254428  ...  0.000455  0.178056  0.036207\n",
       "\n",
       "[5 rows x 1538 columns]"
      ]
     },
     "execution_count": 350,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "r1NXK7FzARY1",
    "outputId": "e87a5d5e-3e32-4e0b-eff8-900f18ad63e7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>...</th>\n",
       "      <th>728</th>\n",
       "      <th>729</th>\n",
       "      <th>730</th>\n",
       "      <th>731</th>\n",
       "      <th>732</th>\n",
       "      <th>733</th>\n",
       "      <th>734</th>\n",
       "      <th>735</th>\n",
       "      <th>736</th>\n",
       "      <th>737</th>\n",
       "      <th>738</th>\n",
       "      <th>739</th>\n",
       "      <th>740</th>\n",
       "      <th>741</th>\n",
       "      <th>742</th>\n",
       "      <th>743</th>\n",
       "      <th>744</th>\n",
       "      <th>745</th>\n",
       "      <th>746</th>\n",
       "      <th>747</th>\n",
       "      <th>748</th>\n",
       "      <th>749</th>\n",
       "      <th>750</th>\n",
       "      <th>751</th>\n",
       "      <th>752</th>\n",
       "      <th>753</th>\n",
       "      <th>754</th>\n",
       "      <th>755</th>\n",
       "      <th>756</th>\n",
       "      <th>757</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.182144</td>\n",
       "      <td>-0.052698</td>\n",
       "      <td>0.296982</td>\n",
       "      <td>-0.046854</td>\n",
       "      <td>0.066862</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.109145</td>\n",
       "      <td>0.115977</td>\n",
       "      <td>-0.300374</td>\n",
       "      <td>0.012484</td>\n",
       "      <td>0.142182</td>\n",
       "      <td>-0.068124</td>\n",
       "      <td>-0.208033</td>\n",
       "      <td>0.149216</td>\n",
       "      <td>0.046058</td>\n",
       "      <td>0.014590</td>\n",
       "      <td>0.069055</td>\n",
       "      <td>0.108952</td>\n",
       "      <td>0.146693</td>\n",
       "      <td>-0.115352</td>\n",
       "      <td>0.096257</td>\n",
       "      <td>-0.078912</td>\n",
       "      <td>-0.141155</td>\n",
       "      <td>-0.117024</td>\n",
       "      <td>0.098890</td>\n",
       "      <td>-0.202187</td>\n",
       "      <td>0.057147</td>\n",
       "      <td>0.059443</td>\n",
       "      <td>0.018391</td>\n",
       "      <td>0.045539</td>\n",
       "      <td>0.011138</td>\n",
       "      <td>-0.157200</td>\n",
       "      <td>-0.041015</td>\n",
       "      <td>0.011421</td>\n",
       "      <td>-0.031243</td>\n",
       "      <td>0.112957</td>\n",
       "      <td>-0.087513</td>\n",
       "      <td>-0.060702</td>\n",
       "      <td>0.018499</td>\n",
       "      <td>-0.012571</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.079505</td>\n",
       "      <td>0.104720</td>\n",
       "      <td>-0.026521</td>\n",
       "      <td>-0.118880</td>\n",
       "      <td>-0.133549</td>\n",
       "      <td>0.286074</td>\n",
       "      <td>0.133212</td>\n",
       "      <td>0.056164</td>\n",
       "      <td>0.017515</td>\n",
       "      <td>0.037554</td>\n",
       "      <td>-0.028962</td>\n",
       "      <td>0.015284</td>\n",
       "      <td>-0.267593</td>\n",
       "      <td>0.120774</td>\n",
       "      <td>-0.010770</td>\n",
       "      <td>0.151480</td>\n",
       "      <td>0.093855</td>\n",
       "      <td>0.009536</td>\n",
       "      <td>-0.033602</td>\n",
       "      <td>-0.096230</td>\n",
       "      <td>0.050461</td>\n",
       "      <td>0.054975</td>\n",
       "      <td>-0.051313</td>\n",
       "      <td>0.039797</td>\n",
       "      <td>-9.00595</td>\n",
       "      <td>0.057208</td>\n",
       "      <td>-0.009514</td>\n",
       "      <td>0.064110</td>\n",
       "      <td>-0.130201</td>\n",
       "      <td>0.078111</td>\n",
       "      <td>-0.004505</td>\n",
       "      <td>-0.211739</td>\n",
       "      <td>-0.026336</td>\n",
       "      <td>0.000743</td>\n",
       "      <td>0.333825</td>\n",
       "      <td>-0.151763</td>\n",
       "      <td>-0.064953</td>\n",
       "      <td>0.050184</td>\n",
       "      <td>-0.023745</td>\n",
       "      <td>-0.108041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.090396</td>\n",
       "      <td>-0.063034</td>\n",
       "      <td>0.183588</td>\n",
       "      <td>-0.132425</td>\n",
       "      <td>0.218097</td>\n",
       "      <td>-0.061284</td>\n",
       "      <td>0.194140</td>\n",
       "      <td>0.099648</td>\n",
       "      <td>-0.110126</td>\n",
       "      <td>-0.059502</td>\n",
       "      <td>0.033862</td>\n",
       "      <td>0.062735</td>\n",
       "      <td>-0.222219</td>\n",
       "      <td>0.129082</td>\n",
       "      <td>-0.075026</td>\n",
       "      <td>0.102772</td>\n",
       "      <td>0.047288</td>\n",
       "      <td>0.282739</td>\n",
       "      <td>0.116910</td>\n",
       "      <td>-0.196864</td>\n",
       "      <td>-0.068466</td>\n",
       "      <td>-0.230079</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-0.075219</td>\n",
       "      <td>0.086746</td>\n",
       "      <td>-0.081459</td>\n",
       "      <td>0.198271</td>\n",
       "      <td>0.131429</td>\n",
       "      <td>0.164833</td>\n",
       "      <td>0.104517</td>\n",
       "      <td>0.061296</td>\n",
       "      <td>-0.225312</td>\n",
       "      <td>0.071521</td>\n",
       "      <td>-0.182611</td>\n",
       "      <td>-0.167435</td>\n",
       "      <td>0.170808</td>\n",
       "      <td>-0.015743</td>\n",
       "      <td>-0.016329</td>\n",
       "      <td>0.002535</td>\n",
       "      <td>0.039329</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027212</td>\n",
       "      <td>0.175481</td>\n",
       "      <td>-0.031569</td>\n",
       "      <td>-0.048201</td>\n",
       "      <td>-0.122848</td>\n",
       "      <td>0.264615</td>\n",
       "      <td>0.055259</td>\n",
       "      <td>-0.122896</td>\n",
       "      <td>-0.073748</td>\n",
       "      <td>0.025588</td>\n",
       "      <td>-0.026207</td>\n",
       "      <td>0.110434</td>\n",
       "      <td>-0.153051</td>\n",
       "      <td>0.239316</td>\n",
       "      <td>0.075326</td>\n",
       "      <td>0.244874</td>\n",
       "      <td>0.320699</td>\n",
       "      <td>0.046222</td>\n",
       "      <td>-0.022302</td>\n",
       "      <td>-0.087042</td>\n",
       "      <td>-0.110124</td>\n",
       "      <td>0.065037</td>\n",
       "      <td>-0.062415</td>\n",
       "      <td>0.060719</td>\n",
       "      <td>-8.97873</td>\n",
       "      <td>0.044415</td>\n",
       "      <td>0.048184</td>\n",
       "      <td>-0.005165</td>\n",
       "      <td>-0.039495</td>\n",
       "      <td>0.020332</td>\n",
       "      <td>-0.104020</td>\n",
       "      <td>-0.159283</td>\n",
       "      <td>0.081489</td>\n",
       "      <td>-0.094288</td>\n",
       "      <td>0.081652</td>\n",
       "      <td>-0.091267</td>\n",
       "      <td>0.003904</td>\n",
       "      <td>-0.030987</td>\n",
       "      <td>0.065567</td>\n",
       "      <td>0.015198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.204430</td>\n",
       "      <td>-0.299815</td>\n",
       "      <td>0.278582</td>\n",
       "      <td>-0.330959</td>\n",
       "      <td>0.283490</td>\n",
       "      <td>0.167636</td>\n",
       "      <td>-0.000396</td>\n",
       "      <td>0.186616</td>\n",
       "      <td>-0.080640</td>\n",
       "      <td>-0.002074</td>\n",
       "      <td>0.083603</td>\n",
       "      <td>0.029153</td>\n",
       "      <td>-0.071468</td>\n",
       "      <td>-0.101955</td>\n",
       "      <td>-0.088183</td>\n",
       "      <td>0.027302</td>\n",
       "      <td>-0.042051</td>\n",
       "      <td>0.134516</td>\n",
       "      <td>0.101671</td>\n",
       "      <td>-0.028426</td>\n",
       "      <td>0.077124</td>\n",
       "      <td>-0.186997</td>\n",
       "      <td>-0.313275</td>\n",
       "      <td>-0.250894</td>\n",
       "      <td>0.080097</td>\n",
       "      <td>-0.078265</td>\n",
       "      <td>0.297395</td>\n",
       "      <td>0.140284</td>\n",
       "      <td>-0.008929</td>\n",
       "      <td>0.023978</td>\n",
       "      <td>0.057998</td>\n",
       "      <td>-0.050854</td>\n",
       "      <td>0.088528</td>\n",
       "      <td>0.067004</td>\n",
       "      <td>-0.133049</td>\n",
       "      <td>0.062508</td>\n",
       "      <td>-0.102993</td>\n",
       "      <td>-0.093009</td>\n",
       "      <td>-0.085430</td>\n",
       "      <td>0.076165</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013532</td>\n",
       "      <td>0.145643</td>\n",
       "      <td>0.047052</td>\n",
       "      <td>-0.193214</td>\n",
       "      <td>-0.099032</td>\n",
       "      <td>0.441819</td>\n",
       "      <td>-0.026864</td>\n",
       "      <td>0.064393</td>\n",
       "      <td>-0.085460</td>\n",
       "      <td>-0.063976</td>\n",
       "      <td>0.001207</td>\n",
       "      <td>0.048360</td>\n",
       "      <td>-0.295799</td>\n",
       "      <td>0.250361</td>\n",
       "      <td>0.007193</td>\n",
       "      <td>0.002607</td>\n",
       "      <td>0.367675</td>\n",
       "      <td>0.064131</td>\n",
       "      <td>-0.044522</td>\n",
       "      <td>-0.257064</td>\n",
       "      <td>-0.175361</td>\n",
       "      <td>0.061791</td>\n",
       "      <td>-0.198281</td>\n",
       "      <td>-0.037277</td>\n",
       "      <td>-9.16263</td>\n",
       "      <td>0.137411</td>\n",
       "      <td>0.065680</td>\n",
       "      <td>0.157470</td>\n",
       "      <td>-0.289391</td>\n",
       "      <td>0.087019</td>\n",
       "      <td>-0.099552</td>\n",
       "      <td>-0.125894</td>\n",
       "      <td>0.119627</td>\n",
       "      <td>-0.012324</td>\n",
       "      <td>0.406567</td>\n",
       "      <td>-0.109556</td>\n",
       "      <td>-0.028680</td>\n",
       "      <td>0.201417</td>\n",
       "      <td>-0.064270</td>\n",
       "      <td>0.095350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.074055</td>\n",
       "      <td>-0.994510</td>\n",
       "      <td>0.083315</td>\n",
       "      <td>0.173790</td>\n",
       "      <td>0.435514</td>\n",
       "      <td>0.489149</td>\n",
       "      <td>-0.442962</td>\n",
       "      <td>-0.515874</td>\n",
       "      <td>-0.247308</td>\n",
       "      <td>-0.210775</td>\n",
       "      <td>0.038379</td>\n",
       "      <td>0.199510</td>\n",
       "      <td>-0.283324</td>\n",
       "      <td>-0.483493</td>\n",
       "      <td>-0.898885</td>\n",
       "      <td>0.334158</td>\n",
       "      <td>0.353439</td>\n",
       "      <td>-1.088240</td>\n",
       "      <td>-1.086190</td>\n",
       "      <td>0.170191</td>\n",
       "      <td>-0.323830</td>\n",
       "      <td>-0.223867</td>\n",
       "      <td>-0.157821</td>\n",
       "      <td>-0.040704</td>\n",
       "      <td>-0.135643</td>\n",
       "      <td>0.109425</td>\n",
       "      <td>-0.167469</td>\n",
       "      <td>-0.357555</td>\n",
       "      <td>-0.247309</td>\n",
       "      <td>-0.306711</td>\n",
       "      <td>0.350606</td>\n",
       "      <td>-0.533339</td>\n",
       "      <td>0.937067</td>\n",
       "      <td>0.810881</td>\n",
       "      <td>-0.742399</td>\n",
       "      <td>-0.461994</td>\n",
       "      <td>-0.695902</td>\n",
       "      <td>-0.148049</td>\n",
       "      <td>-0.560312</td>\n",
       "      <td>-0.004402</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.485970</td>\n",
       "      <td>-0.319889</td>\n",
       "      <td>0.264213</td>\n",
       "      <td>-0.837947</td>\n",
       "      <td>0.662875</td>\n",
       "      <td>0.054413</td>\n",
       "      <td>0.096752</td>\n",
       "      <td>-0.052930</td>\n",
       "      <td>-0.357925</td>\n",
       "      <td>0.054988</td>\n",
       "      <td>-0.023470</td>\n",
       "      <td>-0.015924</td>\n",
       "      <td>-0.816854</td>\n",
       "      <td>0.350448</td>\n",
       "      <td>0.005052</td>\n",
       "      <td>-0.324144</td>\n",
       "      <td>0.634460</td>\n",
       "      <td>-0.995354</td>\n",
       "      <td>-0.489603</td>\n",
       "      <td>0.468945</td>\n",
       "      <td>0.522310</td>\n",
       "      <td>1.203780</td>\n",
       "      <td>-0.918136</td>\n",
       "      <td>-0.362671</td>\n",
       "      <td>-1.22464</td>\n",
       "      <td>0.824677</td>\n",
       "      <td>0.418465</td>\n",
       "      <td>0.032358</td>\n",
       "      <td>-0.213618</td>\n",
       "      <td>-0.244904</td>\n",
       "      <td>0.089304</td>\n",
       "      <td>0.333229</td>\n",
       "      <td>0.727644</td>\n",
       "      <td>-0.946214</td>\n",
       "      <td>0.688935</td>\n",
       "      <td>-0.036943</td>\n",
       "      <td>-0.363213</td>\n",
       "      <td>0.905584</td>\n",
       "      <td>1.010070</td>\n",
       "      <td>0.210940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.254428</td>\n",
       "      <td>-0.011411</td>\n",
       "      <td>0.334044</td>\n",
       "      <td>-0.152447</td>\n",
       "      <td>0.132704</td>\n",
       "      <td>-0.040200</td>\n",
       "      <td>0.080763</td>\n",
       "      <td>0.169764</td>\n",
       "      <td>-0.161985</td>\n",
       "      <td>-0.002933</td>\n",
       "      <td>0.075230</td>\n",
       "      <td>0.053451</td>\n",
       "      <td>-0.158602</td>\n",
       "      <td>0.013316</td>\n",
       "      <td>-0.063966</td>\n",
       "      <td>0.150923</td>\n",
       "      <td>0.022072</td>\n",
       "      <td>0.168311</td>\n",
       "      <td>0.124978</td>\n",
       "      <td>-0.229316</td>\n",
       "      <td>0.006148</td>\n",
       "      <td>-0.104214</td>\n",
       "      <td>-0.089166</td>\n",
       "      <td>-0.112851</td>\n",
       "      <td>0.157767</td>\n",
       "      <td>-0.022986</td>\n",
       "      <td>0.275391</td>\n",
       "      <td>0.152245</td>\n",
       "      <td>-0.022349</td>\n",
       "      <td>0.043627</td>\n",
       "      <td>0.068497</td>\n",
       "      <td>-0.072003</td>\n",
       "      <td>0.164927</td>\n",
       "      <td>0.128153</td>\n",
       "      <td>-0.097877</td>\n",
       "      <td>0.112834</td>\n",
       "      <td>0.025989</td>\n",
       "      <td>-0.035945</td>\n",
       "      <td>0.029443</td>\n",
       "      <td>0.111205</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.048833</td>\n",
       "      <td>0.092955</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>-0.139243</td>\n",
       "      <td>-0.096447</td>\n",
       "      <td>0.371564</td>\n",
       "      <td>0.056716</td>\n",
       "      <td>-0.018409</td>\n",
       "      <td>0.003533</td>\n",
       "      <td>-0.065163</td>\n",
       "      <td>0.013406</td>\n",
       "      <td>0.051614</td>\n",
       "      <td>-0.313355</td>\n",
       "      <td>0.178504</td>\n",
       "      <td>-0.017465</td>\n",
       "      <td>0.077003</td>\n",
       "      <td>0.257123</td>\n",
       "      <td>0.087590</td>\n",
       "      <td>-0.035367</td>\n",
       "      <td>-0.082249</td>\n",
       "      <td>-0.172318</td>\n",
       "      <td>0.057436</td>\n",
       "      <td>-0.017893</td>\n",
       "      <td>-0.038150</td>\n",
       "      <td>-9.09304</td>\n",
       "      <td>0.102770</td>\n",
       "      <td>0.073625</td>\n",
       "      <td>0.083781</td>\n",
       "      <td>-0.100582</td>\n",
       "      <td>0.020675</td>\n",
       "      <td>-0.100587</td>\n",
       "      <td>-0.151935</td>\n",
       "      <td>-0.029423</td>\n",
       "      <td>-0.009458</td>\n",
       "      <td>0.317632</td>\n",
       "      <td>0.004331</td>\n",
       "      <td>0.124570</td>\n",
       "      <td>0.114443</td>\n",
       "      <td>-0.095432</td>\n",
       "      <td>-0.035912</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2    ...       765       766       767\n",
       "0  0.182144 -0.052698  0.296982  ...  0.050184 -0.023745 -0.108041\n",
       "1  0.090396 -0.063034  0.183588  ... -0.030987  0.065567  0.015198\n",
       "2  0.204430 -0.299815  0.278582  ...  0.201417 -0.064270  0.095350\n",
       "3  0.074055 -0.994510  0.083315  ...  0.905584  1.010070  0.210940\n",
       "4  0.254428 -0.011411  0.334044  ...  0.114443 -0.095432 -0.035912\n",
       "\n",
       "[5 rows x 768 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_vector.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "iZJVfpH1AYHp",
    "outputId": "9a748607-2aaa-4d74-bb41-c1b485a43ce4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>...</th>\n",
       "      <th>728</th>\n",
       "      <th>729</th>\n",
       "      <th>730</th>\n",
       "      <th>731</th>\n",
       "      <th>732</th>\n",
       "      <th>733</th>\n",
       "      <th>734</th>\n",
       "      <th>735</th>\n",
       "      <th>736</th>\n",
       "      <th>737</th>\n",
       "      <th>738</th>\n",
       "      <th>739</th>\n",
       "      <th>740</th>\n",
       "      <th>741</th>\n",
       "      <th>742</th>\n",
       "      <th>743</th>\n",
       "      <th>744</th>\n",
       "      <th>745</th>\n",
       "      <th>746</th>\n",
       "      <th>747</th>\n",
       "      <th>748</th>\n",
       "      <th>749</th>\n",
       "      <th>750</th>\n",
       "      <th>751</th>\n",
       "      <th>752</th>\n",
       "      <th>753</th>\n",
       "      <th>754</th>\n",
       "      <th>755</th>\n",
       "      <th>756</th>\n",
       "      <th>757</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.239577</td>\n",
       "      <td>0.182857</td>\n",
       "      <td>0.284122</td>\n",
       "      <td>-0.086142</td>\n",
       "      <td>0.207106</td>\n",
       "      <td>0.084421</td>\n",
       "      <td>0.253997</td>\n",
       "      <td>0.310796</td>\n",
       "      <td>-0.271986</td>\n",
       "      <td>-0.033349</td>\n",
       "      <td>-0.029973</td>\n",
       "      <td>0.060996</td>\n",
       "      <td>0.011152</td>\n",
       "      <td>0.016794</td>\n",
       "      <td>0.073788</td>\n",
       "      <td>0.049742</td>\n",
       "      <td>-0.120587</td>\n",
       "      <td>0.277185</td>\n",
       "      <td>-0.121792</td>\n",
       "      <td>-0.306350</td>\n",
       "      <td>0.089629</td>\n",
       "      <td>-0.016299</td>\n",
       "      <td>-0.364684</td>\n",
       "      <td>-0.291002</td>\n",
       "      <td>0.096310</td>\n",
       "      <td>-0.194393</td>\n",
       "      <td>0.088355</td>\n",
       "      <td>0.117249</td>\n",
       "      <td>0.018630</td>\n",
       "      <td>0.301229</td>\n",
       "      <td>0.073010</td>\n",
       "      <td>-0.302190</td>\n",
       "      <td>0.160194</td>\n",
       "      <td>0.217949</td>\n",
       "      <td>-0.125618</td>\n",
       "      <td>-0.342787</td>\n",
       "      <td>-0.145994</td>\n",
       "      <td>0.040412</td>\n",
       "      <td>0.097783</td>\n",
       "      <td>-0.077835</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.204922</td>\n",
       "      <td>0.201669</td>\n",
       "      <td>0.095816</td>\n",
       "      <td>0.088479</td>\n",
       "      <td>-0.163301</td>\n",
       "      <td>0.178585</td>\n",
       "      <td>0.022706</td>\n",
       "      <td>0.072588</td>\n",
       "      <td>-0.085228</td>\n",
       "      <td>0.241856</td>\n",
       "      <td>-0.052063</td>\n",
       "      <td>0.206275</td>\n",
       "      <td>-0.226309</td>\n",
       "      <td>0.055761</td>\n",
       "      <td>-0.013927</td>\n",
       "      <td>-0.047932</td>\n",
       "      <td>0.055732</td>\n",
       "      <td>0.013788</td>\n",
       "      <td>0.058288</td>\n",
       "      <td>0.074495</td>\n",
       "      <td>-0.144048</td>\n",
       "      <td>0.24373</td>\n",
       "      <td>-0.124181</td>\n",
       "      <td>-0.023625</td>\n",
       "      <td>-8.46837</td>\n",
       "      <td>-0.155591</td>\n",
       "      <td>0.134095</td>\n",
       "      <td>0.142500</td>\n",
       "      <td>0.122580</td>\n",
       "      <td>0.353983</td>\n",
       "      <td>0.017859</td>\n",
       "      <td>-0.048209</td>\n",
       "      <td>-0.140820</td>\n",
       "      <td>-0.028225</td>\n",
       "      <td>0.431787</td>\n",
       "      <td>-0.342401</td>\n",
       "      <td>0.033433</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.178056</td>\n",
       "      <td>0.036207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.239577</td>\n",
       "      <td>0.182857</td>\n",
       "      <td>0.284122</td>\n",
       "      <td>-0.086142</td>\n",
       "      <td>0.207106</td>\n",
       "      <td>0.084421</td>\n",
       "      <td>0.253997</td>\n",
       "      <td>0.310796</td>\n",
       "      <td>-0.271986</td>\n",
       "      <td>-0.033349</td>\n",
       "      <td>-0.029973</td>\n",
       "      <td>0.060996</td>\n",
       "      <td>0.011152</td>\n",
       "      <td>0.016794</td>\n",
       "      <td>0.073788</td>\n",
       "      <td>0.049742</td>\n",
       "      <td>-0.120587</td>\n",
       "      <td>0.277185</td>\n",
       "      <td>-0.121792</td>\n",
       "      <td>-0.306350</td>\n",
       "      <td>0.089629</td>\n",
       "      <td>-0.016299</td>\n",
       "      <td>-0.364684</td>\n",
       "      <td>-0.291002</td>\n",
       "      <td>0.096310</td>\n",
       "      <td>-0.194393</td>\n",
       "      <td>0.088355</td>\n",
       "      <td>0.117249</td>\n",
       "      <td>0.018630</td>\n",
       "      <td>0.301229</td>\n",
       "      <td>0.073010</td>\n",
       "      <td>-0.302190</td>\n",
       "      <td>0.160194</td>\n",
       "      <td>0.217949</td>\n",
       "      <td>-0.125618</td>\n",
       "      <td>-0.342787</td>\n",
       "      <td>-0.145994</td>\n",
       "      <td>0.040412</td>\n",
       "      <td>0.097783</td>\n",
       "      <td>-0.077835</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.204922</td>\n",
       "      <td>0.201669</td>\n",
       "      <td>0.095816</td>\n",
       "      <td>0.088479</td>\n",
       "      <td>-0.163301</td>\n",
       "      <td>0.178585</td>\n",
       "      <td>0.022706</td>\n",
       "      <td>0.072588</td>\n",
       "      <td>-0.085228</td>\n",
       "      <td>0.241856</td>\n",
       "      <td>-0.052063</td>\n",
       "      <td>0.206275</td>\n",
       "      <td>-0.226309</td>\n",
       "      <td>0.055761</td>\n",
       "      <td>-0.013927</td>\n",
       "      <td>-0.047932</td>\n",
       "      <td>0.055732</td>\n",
       "      <td>0.013788</td>\n",
       "      <td>0.058288</td>\n",
       "      <td>0.074495</td>\n",
       "      <td>-0.144048</td>\n",
       "      <td>0.24373</td>\n",
       "      <td>-0.124181</td>\n",
       "      <td>-0.023625</td>\n",
       "      <td>-8.46837</td>\n",
       "      <td>-0.155591</td>\n",
       "      <td>0.134095</td>\n",
       "      <td>0.142500</td>\n",
       "      <td>0.122580</td>\n",
       "      <td>0.353983</td>\n",
       "      <td>0.017859</td>\n",
       "      <td>-0.048209</td>\n",
       "      <td>-0.140820</td>\n",
       "      <td>-0.028225</td>\n",
       "      <td>0.431787</td>\n",
       "      <td>-0.342401</td>\n",
       "      <td>0.033433</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.178056</td>\n",
       "      <td>0.036207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.239577</td>\n",
       "      <td>0.182857</td>\n",
       "      <td>0.284122</td>\n",
       "      <td>-0.086142</td>\n",
       "      <td>0.207106</td>\n",
       "      <td>0.084421</td>\n",
       "      <td>0.253997</td>\n",
       "      <td>0.310796</td>\n",
       "      <td>-0.271986</td>\n",
       "      <td>-0.033349</td>\n",
       "      <td>-0.029973</td>\n",
       "      <td>0.060996</td>\n",
       "      <td>0.011152</td>\n",
       "      <td>0.016794</td>\n",
       "      <td>0.073788</td>\n",
       "      <td>0.049742</td>\n",
       "      <td>-0.120587</td>\n",
       "      <td>0.277185</td>\n",
       "      <td>-0.121792</td>\n",
       "      <td>-0.306350</td>\n",
       "      <td>0.089629</td>\n",
       "      <td>-0.016299</td>\n",
       "      <td>-0.364684</td>\n",
       "      <td>-0.291002</td>\n",
       "      <td>0.096310</td>\n",
       "      <td>-0.194393</td>\n",
       "      <td>0.088355</td>\n",
       "      <td>0.117249</td>\n",
       "      <td>0.018630</td>\n",
       "      <td>0.301229</td>\n",
       "      <td>0.073010</td>\n",
       "      <td>-0.302190</td>\n",
       "      <td>0.160194</td>\n",
       "      <td>0.217949</td>\n",
       "      <td>-0.125618</td>\n",
       "      <td>-0.342787</td>\n",
       "      <td>-0.145994</td>\n",
       "      <td>0.040412</td>\n",
       "      <td>0.097783</td>\n",
       "      <td>-0.077835</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.204922</td>\n",
       "      <td>0.201669</td>\n",
       "      <td>0.095816</td>\n",
       "      <td>0.088479</td>\n",
       "      <td>-0.163301</td>\n",
       "      <td>0.178585</td>\n",
       "      <td>0.022706</td>\n",
       "      <td>0.072588</td>\n",
       "      <td>-0.085228</td>\n",
       "      <td>0.241856</td>\n",
       "      <td>-0.052063</td>\n",
       "      <td>0.206275</td>\n",
       "      <td>-0.226309</td>\n",
       "      <td>0.055761</td>\n",
       "      <td>-0.013927</td>\n",
       "      <td>-0.047932</td>\n",
       "      <td>0.055732</td>\n",
       "      <td>0.013788</td>\n",
       "      <td>0.058288</td>\n",
       "      <td>0.074495</td>\n",
       "      <td>-0.144048</td>\n",
       "      <td>0.24373</td>\n",
       "      <td>-0.124181</td>\n",
       "      <td>-0.023625</td>\n",
       "      <td>-8.46837</td>\n",
       "      <td>-0.155591</td>\n",
       "      <td>0.134095</td>\n",
       "      <td>0.142500</td>\n",
       "      <td>0.122580</td>\n",
       "      <td>0.353983</td>\n",
       "      <td>0.017859</td>\n",
       "      <td>-0.048209</td>\n",
       "      <td>-0.140820</td>\n",
       "      <td>-0.028225</td>\n",
       "      <td>0.431787</td>\n",
       "      <td>-0.342401</td>\n",
       "      <td>0.033433</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.178056</td>\n",
       "      <td>0.036207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.018658</td>\n",
       "      <td>-1.244610</td>\n",
       "      <td>0.382696</td>\n",
       "      <td>0.328033</td>\n",
       "      <td>0.075736</td>\n",
       "      <td>0.147204</td>\n",
       "      <td>0.208668</td>\n",
       "      <td>-0.325944</td>\n",
       "      <td>0.059597</td>\n",
       "      <td>-0.496547</td>\n",
       "      <td>-0.492126</td>\n",
       "      <td>0.137977</td>\n",
       "      <td>-1.014930</td>\n",
       "      <td>0.142615</td>\n",
       "      <td>-1.325580</td>\n",
       "      <td>0.137420</td>\n",
       "      <td>-0.196991</td>\n",
       "      <td>-0.905221</td>\n",
       "      <td>-0.677996</td>\n",
       "      <td>-0.173905</td>\n",
       "      <td>-0.212492</td>\n",
       "      <td>-0.643445</td>\n",
       "      <td>-0.244356</td>\n",
       "      <td>0.041128</td>\n",
       "      <td>-0.385912</td>\n",
       "      <td>0.982765</td>\n",
       "      <td>-0.919323</td>\n",
       "      <td>-0.352857</td>\n",
       "      <td>-0.428382</td>\n",
       "      <td>-0.236438</td>\n",
       "      <td>0.109246</td>\n",
       "      <td>-0.151543</td>\n",
       "      <td>0.706613</td>\n",
       "      <td>0.750051</td>\n",
       "      <td>-1.196410</td>\n",
       "      <td>-0.400648</td>\n",
       "      <td>-0.963530</td>\n",
       "      <td>-0.347007</td>\n",
       "      <td>0.242928</td>\n",
       "      <td>0.705486</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.123386</td>\n",
       "      <td>0.015488</td>\n",
       "      <td>0.403286</td>\n",
       "      <td>-0.721330</td>\n",
       "      <td>0.482571</td>\n",
       "      <td>-0.146677</td>\n",
       "      <td>0.164433</td>\n",
       "      <td>-0.812643</td>\n",
       "      <td>-0.319916</td>\n",
       "      <td>-0.050146</td>\n",
       "      <td>-0.242649</td>\n",
       "      <td>-0.401917</td>\n",
       "      <td>-1.172610</td>\n",
       "      <td>0.069800</td>\n",
       "      <td>0.322633</td>\n",
       "      <td>-0.226635</td>\n",
       "      <td>0.889395</td>\n",
       "      <td>-0.686808</td>\n",
       "      <td>-0.287803</td>\n",
       "      <td>-0.251987</td>\n",
       "      <td>0.591564</td>\n",
       "      <td>1.50253</td>\n",
       "      <td>-0.575015</td>\n",
       "      <td>-0.410068</td>\n",
       "      <td>-3.85364</td>\n",
       "      <td>0.476023</td>\n",
       "      <td>-0.121097</td>\n",
       "      <td>0.007005</td>\n",
       "      <td>-0.425543</td>\n",
       "      <td>-0.352296</td>\n",
       "      <td>-0.120428</td>\n",
       "      <td>-0.059130</td>\n",
       "      <td>0.952337</td>\n",
       "      <td>-1.308660</td>\n",
       "      <td>0.952924</td>\n",
       "      <td>0.261201</td>\n",
       "      <td>-0.729069</td>\n",
       "      <td>0.909033</td>\n",
       "      <td>0.791063</td>\n",
       "      <td>0.122177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.239577</td>\n",
       "      <td>0.182857</td>\n",
       "      <td>0.284122</td>\n",
       "      <td>-0.086142</td>\n",
       "      <td>0.207106</td>\n",
       "      <td>0.084421</td>\n",
       "      <td>0.253997</td>\n",
       "      <td>0.310796</td>\n",
       "      <td>-0.271986</td>\n",
       "      <td>-0.033349</td>\n",
       "      <td>-0.029973</td>\n",
       "      <td>0.060996</td>\n",
       "      <td>0.011152</td>\n",
       "      <td>0.016794</td>\n",
       "      <td>0.073788</td>\n",
       "      <td>0.049742</td>\n",
       "      <td>-0.120587</td>\n",
       "      <td>0.277185</td>\n",
       "      <td>-0.121792</td>\n",
       "      <td>-0.306350</td>\n",
       "      <td>0.089629</td>\n",
       "      <td>-0.016299</td>\n",
       "      <td>-0.364684</td>\n",
       "      <td>-0.291002</td>\n",
       "      <td>0.096310</td>\n",
       "      <td>-0.194393</td>\n",
       "      <td>0.088355</td>\n",
       "      <td>0.117249</td>\n",
       "      <td>0.018630</td>\n",
       "      <td>0.301229</td>\n",
       "      <td>0.073010</td>\n",
       "      <td>-0.302190</td>\n",
       "      <td>0.160194</td>\n",
       "      <td>0.217949</td>\n",
       "      <td>-0.125618</td>\n",
       "      <td>-0.342787</td>\n",
       "      <td>-0.145994</td>\n",
       "      <td>0.040412</td>\n",
       "      <td>0.097783</td>\n",
       "      <td>-0.077835</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.204922</td>\n",
       "      <td>0.201669</td>\n",
       "      <td>0.095816</td>\n",
       "      <td>0.088479</td>\n",
       "      <td>-0.163301</td>\n",
       "      <td>0.178585</td>\n",
       "      <td>0.022706</td>\n",
       "      <td>0.072588</td>\n",
       "      <td>-0.085228</td>\n",
       "      <td>0.241856</td>\n",
       "      <td>-0.052063</td>\n",
       "      <td>0.206275</td>\n",
       "      <td>-0.226309</td>\n",
       "      <td>0.055761</td>\n",
       "      <td>-0.013927</td>\n",
       "      <td>-0.047932</td>\n",
       "      <td>0.055732</td>\n",
       "      <td>0.013788</td>\n",
       "      <td>0.058288</td>\n",
       "      <td>0.074495</td>\n",
       "      <td>-0.144048</td>\n",
       "      <td>0.24373</td>\n",
       "      <td>-0.124181</td>\n",
       "      <td>-0.023625</td>\n",
       "      <td>-8.46837</td>\n",
       "      <td>-0.155591</td>\n",
       "      <td>0.134095</td>\n",
       "      <td>0.142500</td>\n",
       "      <td>0.122580</td>\n",
       "      <td>0.353983</td>\n",
       "      <td>0.017859</td>\n",
       "      <td>-0.048209</td>\n",
       "      <td>-0.140820</td>\n",
       "      <td>-0.028225</td>\n",
       "      <td>0.431787</td>\n",
       "      <td>-0.342401</td>\n",
       "      <td>0.033433</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.178056</td>\n",
       "      <td>0.036207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2    ...       765       766       767\n",
       "0 -0.239577  0.182857  0.284122  ...  0.000455  0.178056  0.036207\n",
       "1 -0.239577  0.182857  0.284122  ...  0.000455  0.178056  0.036207\n",
       "2 -0.239577  0.182857  0.284122  ...  0.000455  0.178056  0.036207\n",
       "3  0.018658 -1.244610  0.382696  ...  0.909033  0.791063  0.122177\n",
       "4 -0.239577  0.182857  0.284122  ...  0.000455  0.178056  0.036207\n",
       "\n",
       "[5 rows x 768 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vector.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yJCWkeEEAZpg"
   },
   "outputs": [],
   "source": [
    "expanded = pd.concat([reddit_df[['subreddit', 'up_votes']], title_vector, text_vector], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2g9Y1giUAtel"
   },
   "outputs": [],
   "source": [
    "expanded.to_feather(\"expanded.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HkaJDqu2C36p"
   },
   "outputs": [],
   "source": [
    "expanded.to_parquet(\"expanded.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9vjwXVKzAfip"
   },
   "outputs": [],
   "source": [
    "expanded.columns = expanded.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "4FBAtjcPBHZ5",
    "outputId": "8b62b307-0839-454b-8930-55935690bc6e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>up_votes</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>...</th>\n",
       "      <th>728</th>\n",
       "      <th>729</th>\n",
       "      <th>730</th>\n",
       "      <th>731</th>\n",
       "      <th>732</th>\n",
       "      <th>733</th>\n",
       "      <th>734</th>\n",
       "      <th>735</th>\n",
       "      <th>736</th>\n",
       "      <th>737</th>\n",
       "      <th>738</th>\n",
       "      <th>739</th>\n",
       "      <th>740</th>\n",
       "      <th>741</th>\n",
       "      <th>742</th>\n",
       "      <th>743</th>\n",
       "      <th>744</th>\n",
       "      <th>745</th>\n",
       "      <th>746</th>\n",
       "      <th>747</th>\n",
       "      <th>748</th>\n",
       "      <th>749</th>\n",
       "      <th>750</th>\n",
       "      <th>751</th>\n",
       "      <th>752</th>\n",
       "      <th>753</th>\n",
       "      <th>754</th>\n",
       "      <th>755</th>\n",
       "      <th>756</th>\n",
       "      <th>757</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>r/AskReddit</td>\n",
       "      <td>221854</td>\n",
       "      <td>0.182144</td>\n",
       "      <td>-0.052698</td>\n",
       "      <td>0.296982</td>\n",
       "      <td>-0.046854</td>\n",
       "      <td>0.066862</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.109145</td>\n",
       "      <td>0.115977</td>\n",
       "      <td>-0.300374</td>\n",
       "      <td>0.012484</td>\n",
       "      <td>0.142182</td>\n",
       "      <td>-0.068124</td>\n",
       "      <td>-0.208033</td>\n",
       "      <td>0.149216</td>\n",
       "      <td>0.046058</td>\n",
       "      <td>0.014590</td>\n",
       "      <td>0.069055</td>\n",
       "      <td>0.108952</td>\n",
       "      <td>0.146693</td>\n",
       "      <td>-0.115352</td>\n",
       "      <td>0.096257</td>\n",
       "      <td>-0.078912</td>\n",
       "      <td>-0.141155</td>\n",
       "      <td>-0.117024</td>\n",
       "      <td>0.098890</td>\n",
       "      <td>-0.202187</td>\n",
       "      <td>0.057147</td>\n",
       "      <td>0.059443</td>\n",
       "      <td>0.018391</td>\n",
       "      <td>0.045539</td>\n",
       "      <td>0.011138</td>\n",
       "      <td>-0.157200</td>\n",
       "      <td>-0.041015</td>\n",
       "      <td>0.011421</td>\n",
       "      <td>-0.031243</td>\n",
       "      <td>0.112957</td>\n",
       "      <td>-0.087513</td>\n",
       "      <td>-0.060702</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.204922</td>\n",
       "      <td>0.201669</td>\n",
       "      <td>0.095816</td>\n",
       "      <td>0.088479</td>\n",
       "      <td>-0.163301</td>\n",
       "      <td>0.178585</td>\n",
       "      <td>0.022706</td>\n",
       "      <td>0.072588</td>\n",
       "      <td>-0.085228</td>\n",
       "      <td>0.241856</td>\n",
       "      <td>-0.052063</td>\n",
       "      <td>0.206275</td>\n",
       "      <td>-0.226309</td>\n",
       "      <td>0.055761</td>\n",
       "      <td>-0.013927</td>\n",
       "      <td>-0.047932</td>\n",
       "      <td>0.055732</td>\n",
       "      <td>0.013788</td>\n",
       "      <td>0.058288</td>\n",
       "      <td>0.074495</td>\n",
       "      <td>-0.144048</td>\n",
       "      <td>0.24373</td>\n",
       "      <td>-0.124181</td>\n",
       "      <td>-0.023625</td>\n",
       "      <td>-8.46837</td>\n",
       "      <td>-0.155591</td>\n",
       "      <td>0.134095</td>\n",
       "      <td>0.142500</td>\n",
       "      <td>0.122580</td>\n",
       "      <td>0.353983</td>\n",
       "      <td>0.017859</td>\n",
       "      <td>-0.048209</td>\n",
       "      <td>-0.140820</td>\n",
       "      <td>-0.028225</td>\n",
       "      <td>0.431787</td>\n",
       "      <td>-0.342401</td>\n",
       "      <td>0.033433</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.178056</td>\n",
       "      <td>0.036207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>r/AskReddit</td>\n",
       "      <td>197524</td>\n",
       "      <td>0.090396</td>\n",
       "      <td>-0.063034</td>\n",
       "      <td>0.183588</td>\n",
       "      <td>-0.132425</td>\n",
       "      <td>0.218097</td>\n",
       "      <td>-0.061284</td>\n",
       "      <td>0.194140</td>\n",
       "      <td>0.099648</td>\n",
       "      <td>-0.110126</td>\n",
       "      <td>-0.059502</td>\n",
       "      <td>0.033862</td>\n",
       "      <td>0.062735</td>\n",
       "      <td>-0.222219</td>\n",
       "      <td>0.129082</td>\n",
       "      <td>-0.075026</td>\n",
       "      <td>0.102772</td>\n",
       "      <td>0.047288</td>\n",
       "      <td>0.282739</td>\n",
       "      <td>0.116910</td>\n",
       "      <td>-0.196864</td>\n",
       "      <td>-0.068466</td>\n",
       "      <td>-0.230079</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-0.075219</td>\n",
       "      <td>0.086746</td>\n",
       "      <td>-0.081459</td>\n",
       "      <td>0.198271</td>\n",
       "      <td>0.131429</td>\n",
       "      <td>0.164833</td>\n",
       "      <td>0.104517</td>\n",
       "      <td>0.061296</td>\n",
       "      <td>-0.225312</td>\n",
       "      <td>0.071521</td>\n",
       "      <td>-0.182611</td>\n",
       "      <td>-0.167435</td>\n",
       "      <td>0.170808</td>\n",
       "      <td>-0.015743</td>\n",
       "      <td>-0.016329</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.204922</td>\n",
       "      <td>0.201669</td>\n",
       "      <td>0.095816</td>\n",
       "      <td>0.088479</td>\n",
       "      <td>-0.163301</td>\n",
       "      <td>0.178585</td>\n",
       "      <td>0.022706</td>\n",
       "      <td>0.072588</td>\n",
       "      <td>-0.085228</td>\n",
       "      <td>0.241856</td>\n",
       "      <td>-0.052063</td>\n",
       "      <td>0.206275</td>\n",
       "      <td>-0.226309</td>\n",
       "      <td>0.055761</td>\n",
       "      <td>-0.013927</td>\n",
       "      <td>-0.047932</td>\n",
       "      <td>0.055732</td>\n",
       "      <td>0.013788</td>\n",
       "      <td>0.058288</td>\n",
       "      <td>0.074495</td>\n",
       "      <td>-0.144048</td>\n",
       "      <td>0.24373</td>\n",
       "      <td>-0.124181</td>\n",
       "      <td>-0.023625</td>\n",
       "      <td>-8.46837</td>\n",
       "      <td>-0.155591</td>\n",
       "      <td>0.134095</td>\n",
       "      <td>0.142500</td>\n",
       "      <td>0.122580</td>\n",
       "      <td>0.353983</td>\n",
       "      <td>0.017859</td>\n",
       "      <td>-0.048209</td>\n",
       "      <td>-0.140820</td>\n",
       "      <td>-0.028225</td>\n",
       "      <td>0.431787</td>\n",
       "      <td>-0.342401</td>\n",
       "      <td>0.033433</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.178056</td>\n",
       "      <td>0.036207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>r/AskReddit</td>\n",
       "      <td>186368</td>\n",
       "      <td>0.204430</td>\n",
       "      <td>-0.299815</td>\n",
       "      <td>0.278582</td>\n",
       "      <td>-0.330959</td>\n",
       "      <td>0.283490</td>\n",
       "      <td>0.167636</td>\n",
       "      <td>-0.000396</td>\n",
       "      <td>0.186616</td>\n",
       "      <td>-0.080640</td>\n",
       "      <td>-0.002074</td>\n",
       "      <td>0.083603</td>\n",
       "      <td>0.029153</td>\n",
       "      <td>-0.071468</td>\n",
       "      <td>-0.101955</td>\n",
       "      <td>-0.088183</td>\n",
       "      <td>0.027302</td>\n",
       "      <td>-0.042051</td>\n",
       "      <td>0.134516</td>\n",
       "      <td>0.101671</td>\n",
       "      <td>-0.028426</td>\n",
       "      <td>0.077124</td>\n",
       "      <td>-0.186997</td>\n",
       "      <td>-0.313275</td>\n",
       "      <td>-0.250894</td>\n",
       "      <td>0.080097</td>\n",
       "      <td>-0.078265</td>\n",
       "      <td>0.297395</td>\n",
       "      <td>0.140284</td>\n",
       "      <td>-0.008929</td>\n",
       "      <td>0.023978</td>\n",
       "      <td>0.057998</td>\n",
       "      <td>-0.050854</td>\n",
       "      <td>0.088528</td>\n",
       "      <td>0.067004</td>\n",
       "      <td>-0.133049</td>\n",
       "      <td>0.062508</td>\n",
       "      <td>-0.102993</td>\n",
       "      <td>-0.093009</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.204922</td>\n",
       "      <td>0.201669</td>\n",
       "      <td>0.095816</td>\n",
       "      <td>0.088479</td>\n",
       "      <td>-0.163301</td>\n",
       "      <td>0.178585</td>\n",
       "      <td>0.022706</td>\n",
       "      <td>0.072588</td>\n",
       "      <td>-0.085228</td>\n",
       "      <td>0.241856</td>\n",
       "      <td>-0.052063</td>\n",
       "      <td>0.206275</td>\n",
       "      <td>-0.226309</td>\n",
       "      <td>0.055761</td>\n",
       "      <td>-0.013927</td>\n",
       "      <td>-0.047932</td>\n",
       "      <td>0.055732</td>\n",
       "      <td>0.013788</td>\n",
       "      <td>0.058288</td>\n",
       "      <td>0.074495</td>\n",
       "      <td>-0.144048</td>\n",
       "      <td>0.24373</td>\n",
       "      <td>-0.124181</td>\n",
       "      <td>-0.023625</td>\n",
       "      <td>-8.46837</td>\n",
       "      <td>-0.155591</td>\n",
       "      <td>0.134095</td>\n",
       "      <td>0.142500</td>\n",
       "      <td>0.122580</td>\n",
       "      <td>0.353983</td>\n",
       "      <td>0.017859</td>\n",
       "      <td>-0.048209</td>\n",
       "      <td>-0.140820</td>\n",
       "      <td>-0.028225</td>\n",
       "      <td>0.431787</td>\n",
       "      <td>-0.342401</td>\n",
       "      <td>0.033433</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.178056</td>\n",
       "      <td>0.036207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>r/AskReddit</td>\n",
       "      <td>175339</td>\n",
       "      <td>0.074055</td>\n",
       "      <td>-0.994510</td>\n",
       "      <td>0.083315</td>\n",
       "      <td>0.173790</td>\n",
       "      <td>0.435514</td>\n",
       "      <td>0.489149</td>\n",
       "      <td>-0.442962</td>\n",
       "      <td>-0.515874</td>\n",
       "      <td>-0.247308</td>\n",
       "      <td>-0.210775</td>\n",
       "      <td>0.038379</td>\n",
       "      <td>0.199510</td>\n",
       "      <td>-0.283324</td>\n",
       "      <td>-0.483493</td>\n",
       "      <td>-0.898885</td>\n",
       "      <td>0.334158</td>\n",
       "      <td>0.353439</td>\n",
       "      <td>-1.088240</td>\n",
       "      <td>-1.086190</td>\n",
       "      <td>0.170191</td>\n",
       "      <td>-0.323830</td>\n",
       "      <td>-0.223867</td>\n",
       "      <td>-0.157821</td>\n",
       "      <td>-0.040704</td>\n",
       "      <td>-0.135643</td>\n",
       "      <td>0.109425</td>\n",
       "      <td>-0.167469</td>\n",
       "      <td>-0.357555</td>\n",
       "      <td>-0.247309</td>\n",
       "      <td>-0.306711</td>\n",
       "      <td>0.350606</td>\n",
       "      <td>-0.533339</td>\n",
       "      <td>0.937067</td>\n",
       "      <td>0.810881</td>\n",
       "      <td>-0.742399</td>\n",
       "      <td>-0.461994</td>\n",
       "      <td>-0.695902</td>\n",
       "      <td>-0.148049</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.123386</td>\n",
       "      <td>0.015488</td>\n",
       "      <td>0.403286</td>\n",
       "      <td>-0.721330</td>\n",
       "      <td>0.482571</td>\n",
       "      <td>-0.146677</td>\n",
       "      <td>0.164433</td>\n",
       "      <td>-0.812643</td>\n",
       "      <td>-0.319916</td>\n",
       "      <td>-0.050146</td>\n",
       "      <td>-0.242649</td>\n",
       "      <td>-0.401917</td>\n",
       "      <td>-1.172610</td>\n",
       "      <td>0.069800</td>\n",
       "      <td>0.322633</td>\n",
       "      <td>-0.226635</td>\n",
       "      <td>0.889395</td>\n",
       "      <td>-0.686808</td>\n",
       "      <td>-0.287803</td>\n",
       "      <td>-0.251987</td>\n",
       "      <td>0.591564</td>\n",
       "      <td>1.50253</td>\n",
       "      <td>-0.575015</td>\n",
       "      <td>-0.410068</td>\n",
       "      <td>-3.85364</td>\n",
       "      <td>0.476023</td>\n",
       "      <td>-0.121097</td>\n",
       "      <td>0.007005</td>\n",
       "      <td>-0.425543</td>\n",
       "      <td>-0.352296</td>\n",
       "      <td>-0.120428</td>\n",
       "      <td>-0.059130</td>\n",
       "      <td>0.952337</td>\n",
       "      <td>-1.308660</td>\n",
       "      <td>0.952924</td>\n",
       "      <td>0.261201</td>\n",
       "      <td>-0.729069</td>\n",
       "      <td>0.909033</td>\n",
       "      <td>0.791063</td>\n",
       "      <td>0.122177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>r/AskReddit</td>\n",
       "      <td>160311</td>\n",
       "      <td>0.254428</td>\n",
       "      <td>-0.011411</td>\n",
       "      <td>0.334044</td>\n",
       "      <td>-0.152447</td>\n",
       "      <td>0.132704</td>\n",
       "      <td>-0.040200</td>\n",
       "      <td>0.080763</td>\n",
       "      <td>0.169764</td>\n",
       "      <td>-0.161985</td>\n",
       "      <td>-0.002933</td>\n",
       "      <td>0.075230</td>\n",
       "      <td>0.053451</td>\n",
       "      <td>-0.158602</td>\n",
       "      <td>0.013316</td>\n",
       "      <td>-0.063966</td>\n",
       "      <td>0.150923</td>\n",
       "      <td>0.022072</td>\n",
       "      <td>0.168311</td>\n",
       "      <td>0.124978</td>\n",
       "      <td>-0.229316</td>\n",
       "      <td>0.006148</td>\n",
       "      <td>-0.104214</td>\n",
       "      <td>-0.089166</td>\n",
       "      <td>-0.112851</td>\n",
       "      <td>0.157767</td>\n",
       "      <td>-0.022986</td>\n",
       "      <td>0.275391</td>\n",
       "      <td>0.152245</td>\n",
       "      <td>-0.022349</td>\n",
       "      <td>0.043627</td>\n",
       "      <td>0.068497</td>\n",
       "      <td>-0.072003</td>\n",
       "      <td>0.164927</td>\n",
       "      <td>0.128153</td>\n",
       "      <td>-0.097877</td>\n",
       "      <td>0.112834</td>\n",
       "      <td>0.025989</td>\n",
       "      <td>-0.035945</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.204922</td>\n",
       "      <td>0.201669</td>\n",
       "      <td>0.095816</td>\n",
       "      <td>0.088479</td>\n",
       "      <td>-0.163301</td>\n",
       "      <td>0.178585</td>\n",
       "      <td>0.022706</td>\n",
       "      <td>0.072588</td>\n",
       "      <td>-0.085228</td>\n",
       "      <td>0.241856</td>\n",
       "      <td>-0.052063</td>\n",
       "      <td>0.206275</td>\n",
       "      <td>-0.226309</td>\n",
       "      <td>0.055761</td>\n",
       "      <td>-0.013927</td>\n",
       "      <td>-0.047932</td>\n",
       "      <td>0.055732</td>\n",
       "      <td>0.013788</td>\n",
       "      <td>0.058288</td>\n",
       "      <td>0.074495</td>\n",
       "      <td>-0.144048</td>\n",
       "      <td>0.24373</td>\n",
       "      <td>-0.124181</td>\n",
       "      <td>-0.023625</td>\n",
       "      <td>-8.46837</td>\n",
       "      <td>-0.155591</td>\n",
       "      <td>0.134095</td>\n",
       "      <td>0.142500</td>\n",
       "      <td>0.122580</td>\n",
       "      <td>0.353983</td>\n",
       "      <td>0.017859</td>\n",
       "      <td>-0.048209</td>\n",
       "      <td>-0.140820</td>\n",
       "      <td>-0.028225</td>\n",
       "      <td>0.431787</td>\n",
       "      <td>-0.342401</td>\n",
       "      <td>0.033433</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.178056</td>\n",
       "      <td>0.036207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1538 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     subreddit  up_votes         0  ...       765       766       767\n",
       "0  r/AskReddit    221854  0.182144  ...  0.000455  0.178056  0.036207\n",
       "1  r/AskReddit    197524  0.090396  ...  0.000455  0.178056  0.036207\n",
       "2  r/AskReddit    186368  0.204430  ...  0.000455  0.178056  0.036207\n",
       "3  r/AskReddit    175339  0.074055  ...  0.909033  0.791063  0.122177\n",
       "4  r/AskReddit    160311  0.254428  ...  0.000455  0.178056  0.036207\n",
       "\n",
       "[5 rows x 1538 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sDX3o_T1BIWB"
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "def column_namer(df:pd.DataFrame):\n",
    "    yield \"subreddit\"\n",
    "    yield \"up_votes\"\n",
    "    for i in tqdm(range(df.shape[1]-2)):\n",
    "        yield str(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68,
     "referenced_widgets": [
      "b800925fb32b420f9b83f31e29b4d26d",
      "01f7eddf95324dcd860e6111b88716cc",
      "9bd68eac8d3746f8b7f6926d93681e6e",
      "168d8764bf474de1a3327514d275379f",
      "f38ad80944cd4c1ba40461c1a7878eff",
      "336a08e906d34a4da665d2d71691fbdc",
      "f64c4ed72d85446b945d00ce16e5548f",
      "dcc56ad2588243078c7923a820a9339a"
     ]
    },
    "colab_type": "code",
    "id": "p06DfevfCGkl",
    "outputId": "ff8c35a4-c82c-45b1-af30-afe8201afaba"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b800925fb32b420f9b83f31e29b4d26d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1536.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "expanded.columns = list(column_namer(expanded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "5Nj7voGcCNQv",
    "outputId": "c6b072de-0c13-4cae-d790-780a774aa329"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>up_votes</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>...</th>\n",
       "      <th>1496</th>\n",
       "      <th>1497</th>\n",
       "      <th>1498</th>\n",
       "      <th>1499</th>\n",
       "      <th>1500</th>\n",
       "      <th>1501</th>\n",
       "      <th>1502</th>\n",
       "      <th>1503</th>\n",
       "      <th>1504</th>\n",
       "      <th>1505</th>\n",
       "      <th>1506</th>\n",
       "      <th>1507</th>\n",
       "      <th>1508</th>\n",
       "      <th>1509</th>\n",
       "      <th>1510</th>\n",
       "      <th>1511</th>\n",
       "      <th>1512</th>\n",
       "      <th>1513</th>\n",
       "      <th>1514</th>\n",
       "      <th>1515</th>\n",
       "      <th>1516</th>\n",
       "      <th>1517</th>\n",
       "      <th>1518</th>\n",
       "      <th>1519</th>\n",
       "      <th>1520</th>\n",
       "      <th>1521</th>\n",
       "      <th>1522</th>\n",
       "      <th>1523</th>\n",
       "      <th>1524</th>\n",
       "      <th>1525</th>\n",
       "      <th>1526</th>\n",
       "      <th>1527</th>\n",
       "      <th>1528</th>\n",
       "      <th>1529</th>\n",
       "      <th>1530</th>\n",
       "      <th>1531</th>\n",
       "      <th>1532</th>\n",
       "      <th>1533</th>\n",
       "      <th>1534</th>\n",
       "      <th>1535</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>r/AskReddit</td>\n",
       "      <td>221854</td>\n",
       "      <td>0.182144</td>\n",
       "      <td>-0.052698</td>\n",
       "      <td>0.296982</td>\n",
       "      <td>-0.046854</td>\n",
       "      <td>0.066862</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.109145</td>\n",
       "      <td>0.115977</td>\n",
       "      <td>-0.300374</td>\n",
       "      <td>0.012484</td>\n",
       "      <td>0.142182</td>\n",
       "      <td>-0.068124</td>\n",
       "      <td>-0.208033</td>\n",
       "      <td>0.149216</td>\n",
       "      <td>0.046058</td>\n",
       "      <td>0.014590</td>\n",
       "      <td>0.069055</td>\n",
       "      <td>0.108952</td>\n",
       "      <td>0.146693</td>\n",
       "      <td>-0.115352</td>\n",
       "      <td>0.096257</td>\n",
       "      <td>-0.078912</td>\n",
       "      <td>-0.141155</td>\n",
       "      <td>-0.117024</td>\n",
       "      <td>0.098890</td>\n",
       "      <td>-0.202187</td>\n",
       "      <td>0.057147</td>\n",
       "      <td>0.059443</td>\n",
       "      <td>0.018391</td>\n",
       "      <td>0.045539</td>\n",
       "      <td>0.011138</td>\n",
       "      <td>-0.157200</td>\n",
       "      <td>-0.041015</td>\n",
       "      <td>0.011421</td>\n",
       "      <td>-0.031243</td>\n",
       "      <td>0.112957</td>\n",
       "      <td>-0.087513</td>\n",
       "      <td>-0.060702</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.204922</td>\n",
       "      <td>0.201669</td>\n",
       "      <td>0.095816</td>\n",
       "      <td>0.088479</td>\n",
       "      <td>-0.163301</td>\n",
       "      <td>0.178585</td>\n",
       "      <td>0.022706</td>\n",
       "      <td>0.072588</td>\n",
       "      <td>-0.085228</td>\n",
       "      <td>0.241856</td>\n",
       "      <td>-0.052063</td>\n",
       "      <td>0.206275</td>\n",
       "      <td>-0.226309</td>\n",
       "      <td>0.055761</td>\n",
       "      <td>-0.013927</td>\n",
       "      <td>-0.047932</td>\n",
       "      <td>0.055732</td>\n",
       "      <td>0.013788</td>\n",
       "      <td>0.058288</td>\n",
       "      <td>0.074495</td>\n",
       "      <td>-0.144048</td>\n",
       "      <td>0.24373</td>\n",
       "      <td>-0.124181</td>\n",
       "      <td>-0.023625</td>\n",
       "      <td>-8.46837</td>\n",
       "      <td>-0.155591</td>\n",
       "      <td>0.134095</td>\n",
       "      <td>0.142500</td>\n",
       "      <td>0.122580</td>\n",
       "      <td>0.353983</td>\n",
       "      <td>0.017859</td>\n",
       "      <td>-0.048209</td>\n",
       "      <td>-0.140820</td>\n",
       "      <td>-0.028225</td>\n",
       "      <td>0.431787</td>\n",
       "      <td>-0.342401</td>\n",
       "      <td>0.033433</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.178056</td>\n",
       "      <td>0.036207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>r/AskReddit</td>\n",
       "      <td>197524</td>\n",
       "      <td>0.090396</td>\n",
       "      <td>-0.063034</td>\n",
       "      <td>0.183588</td>\n",
       "      <td>-0.132425</td>\n",
       "      <td>0.218097</td>\n",
       "      <td>-0.061284</td>\n",
       "      <td>0.194140</td>\n",
       "      <td>0.099648</td>\n",
       "      <td>-0.110126</td>\n",
       "      <td>-0.059502</td>\n",
       "      <td>0.033862</td>\n",
       "      <td>0.062735</td>\n",
       "      <td>-0.222219</td>\n",
       "      <td>0.129082</td>\n",
       "      <td>-0.075026</td>\n",
       "      <td>0.102772</td>\n",
       "      <td>0.047288</td>\n",
       "      <td>0.282739</td>\n",
       "      <td>0.116910</td>\n",
       "      <td>-0.196864</td>\n",
       "      <td>-0.068466</td>\n",
       "      <td>-0.230079</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-0.075219</td>\n",
       "      <td>0.086746</td>\n",
       "      <td>-0.081459</td>\n",
       "      <td>0.198271</td>\n",
       "      <td>0.131429</td>\n",
       "      <td>0.164833</td>\n",
       "      <td>0.104517</td>\n",
       "      <td>0.061296</td>\n",
       "      <td>-0.225312</td>\n",
       "      <td>0.071521</td>\n",
       "      <td>-0.182611</td>\n",
       "      <td>-0.167435</td>\n",
       "      <td>0.170808</td>\n",
       "      <td>-0.015743</td>\n",
       "      <td>-0.016329</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.204922</td>\n",
       "      <td>0.201669</td>\n",
       "      <td>0.095816</td>\n",
       "      <td>0.088479</td>\n",
       "      <td>-0.163301</td>\n",
       "      <td>0.178585</td>\n",
       "      <td>0.022706</td>\n",
       "      <td>0.072588</td>\n",
       "      <td>-0.085228</td>\n",
       "      <td>0.241856</td>\n",
       "      <td>-0.052063</td>\n",
       "      <td>0.206275</td>\n",
       "      <td>-0.226309</td>\n",
       "      <td>0.055761</td>\n",
       "      <td>-0.013927</td>\n",
       "      <td>-0.047932</td>\n",
       "      <td>0.055732</td>\n",
       "      <td>0.013788</td>\n",
       "      <td>0.058288</td>\n",
       "      <td>0.074495</td>\n",
       "      <td>-0.144048</td>\n",
       "      <td>0.24373</td>\n",
       "      <td>-0.124181</td>\n",
       "      <td>-0.023625</td>\n",
       "      <td>-8.46837</td>\n",
       "      <td>-0.155591</td>\n",
       "      <td>0.134095</td>\n",
       "      <td>0.142500</td>\n",
       "      <td>0.122580</td>\n",
       "      <td>0.353983</td>\n",
       "      <td>0.017859</td>\n",
       "      <td>-0.048209</td>\n",
       "      <td>-0.140820</td>\n",
       "      <td>-0.028225</td>\n",
       "      <td>0.431787</td>\n",
       "      <td>-0.342401</td>\n",
       "      <td>0.033433</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.178056</td>\n",
       "      <td>0.036207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>r/AskReddit</td>\n",
       "      <td>186368</td>\n",
       "      <td>0.204430</td>\n",
       "      <td>-0.299815</td>\n",
       "      <td>0.278582</td>\n",
       "      <td>-0.330959</td>\n",
       "      <td>0.283490</td>\n",
       "      <td>0.167636</td>\n",
       "      <td>-0.000396</td>\n",
       "      <td>0.186616</td>\n",
       "      <td>-0.080640</td>\n",
       "      <td>-0.002074</td>\n",
       "      <td>0.083603</td>\n",
       "      <td>0.029153</td>\n",
       "      <td>-0.071468</td>\n",
       "      <td>-0.101955</td>\n",
       "      <td>-0.088183</td>\n",
       "      <td>0.027302</td>\n",
       "      <td>-0.042051</td>\n",
       "      <td>0.134516</td>\n",
       "      <td>0.101671</td>\n",
       "      <td>-0.028426</td>\n",
       "      <td>0.077124</td>\n",
       "      <td>-0.186997</td>\n",
       "      <td>-0.313275</td>\n",
       "      <td>-0.250894</td>\n",
       "      <td>0.080097</td>\n",
       "      <td>-0.078265</td>\n",
       "      <td>0.297395</td>\n",
       "      <td>0.140284</td>\n",
       "      <td>-0.008929</td>\n",
       "      <td>0.023978</td>\n",
       "      <td>0.057998</td>\n",
       "      <td>-0.050854</td>\n",
       "      <td>0.088528</td>\n",
       "      <td>0.067004</td>\n",
       "      <td>-0.133049</td>\n",
       "      <td>0.062508</td>\n",
       "      <td>-0.102993</td>\n",
       "      <td>-0.093009</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.204922</td>\n",
       "      <td>0.201669</td>\n",
       "      <td>0.095816</td>\n",
       "      <td>0.088479</td>\n",
       "      <td>-0.163301</td>\n",
       "      <td>0.178585</td>\n",
       "      <td>0.022706</td>\n",
       "      <td>0.072588</td>\n",
       "      <td>-0.085228</td>\n",
       "      <td>0.241856</td>\n",
       "      <td>-0.052063</td>\n",
       "      <td>0.206275</td>\n",
       "      <td>-0.226309</td>\n",
       "      <td>0.055761</td>\n",
       "      <td>-0.013927</td>\n",
       "      <td>-0.047932</td>\n",
       "      <td>0.055732</td>\n",
       "      <td>0.013788</td>\n",
       "      <td>0.058288</td>\n",
       "      <td>0.074495</td>\n",
       "      <td>-0.144048</td>\n",
       "      <td>0.24373</td>\n",
       "      <td>-0.124181</td>\n",
       "      <td>-0.023625</td>\n",
       "      <td>-8.46837</td>\n",
       "      <td>-0.155591</td>\n",
       "      <td>0.134095</td>\n",
       "      <td>0.142500</td>\n",
       "      <td>0.122580</td>\n",
       "      <td>0.353983</td>\n",
       "      <td>0.017859</td>\n",
       "      <td>-0.048209</td>\n",
       "      <td>-0.140820</td>\n",
       "      <td>-0.028225</td>\n",
       "      <td>0.431787</td>\n",
       "      <td>-0.342401</td>\n",
       "      <td>0.033433</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.178056</td>\n",
       "      <td>0.036207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>r/AskReddit</td>\n",
       "      <td>175339</td>\n",
       "      <td>0.074055</td>\n",
       "      <td>-0.994510</td>\n",
       "      <td>0.083315</td>\n",
       "      <td>0.173790</td>\n",
       "      <td>0.435514</td>\n",
       "      <td>0.489149</td>\n",
       "      <td>-0.442962</td>\n",
       "      <td>-0.515874</td>\n",
       "      <td>-0.247308</td>\n",
       "      <td>-0.210775</td>\n",
       "      <td>0.038379</td>\n",
       "      <td>0.199510</td>\n",
       "      <td>-0.283324</td>\n",
       "      <td>-0.483493</td>\n",
       "      <td>-0.898885</td>\n",
       "      <td>0.334158</td>\n",
       "      <td>0.353439</td>\n",
       "      <td>-1.088240</td>\n",
       "      <td>-1.086190</td>\n",
       "      <td>0.170191</td>\n",
       "      <td>-0.323830</td>\n",
       "      <td>-0.223867</td>\n",
       "      <td>-0.157821</td>\n",
       "      <td>-0.040704</td>\n",
       "      <td>-0.135643</td>\n",
       "      <td>0.109425</td>\n",
       "      <td>-0.167469</td>\n",
       "      <td>-0.357555</td>\n",
       "      <td>-0.247309</td>\n",
       "      <td>-0.306711</td>\n",
       "      <td>0.350606</td>\n",
       "      <td>-0.533339</td>\n",
       "      <td>0.937067</td>\n",
       "      <td>0.810881</td>\n",
       "      <td>-0.742399</td>\n",
       "      <td>-0.461994</td>\n",
       "      <td>-0.695902</td>\n",
       "      <td>-0.148049</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.123386</td>\n",
       "      <td>0.015488</td>\n",
       "      <td>0.403286</td>\n",
       "      <td>-0.721330</td>\n",
       "      <td>0.482571</td>\n",
       "      <td>-0.146677</td>\n",
       "      <td>0.164433</td>\n",
       "      <td>-0.812643</td>\n",
       "      <td>-0.319916</td>\n",
       "      <td>-0.050146</td>\n",
       "      <td>-0.242649</td>\n",
       "      <td>-0.401917</td>\n",
       "      <td>-1.172610</td>\n",
       "      <td>0.069800</td>\n",
       "      <td>0.322633</td>\n",
       "      <td>-0.226635</td>\n",
       "      <td>0.889395</td>\n",
       "      <td>-0.686808</td>\n",
       "      <td>-0.287803</td>\n",
       "      <td>-0.251987</td>\n",
       "      <td>0.591564</td>\n",
       "      <td>1.50253</td>\n",
       "      <td>-0.575015</td>\n",
       "      <td>-0.410068</td>\n",
       "      <td>-3.85364</td>\n",
       "      <td>0.476023</td>\n",
       "      <td>-0.121097</td>\n",
       "      <td>0.007005</td>\n",
       "      <td>-0.425543</td>\n",
       "      <td>-0.352296</td>\n",
       "      <td>-0.120428</td>\n",
       "      <td>-0.059130</td>\n",
       "      <td>0.952337</td>\n",
       "      <td>-1.308660</td>\n",
       "      <td>0.952924</td>\n",
       "      <td>0.261201</td>\n",
       "      <td>-0.729069</td>\n",
       "      <td>0.909033</td>\n",
       "      <td>0.791063</td>\n",
       "      <td>0.122177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>r/AskReddit</td>\n",
       "      <td>160311</td>\n",
       "      <td>0.254428</td>\n",
       "      <td>-0.011411</td>\n",
       "      <td>0.334044</td>\n",
       "      <td>-0.152447</td>\n",
       "      <td>0.132704</td>\n",
       "      <td>-0.040200</td>\n",
       "      <td>0.080763</td>\n",
       "      <td>0.169764</td>\n",
       "      <td>-0.161985</td>\n",
       "      <td>-0.002933</td>\n",
       "      <td>0.075230</td>\n",
       "      <td>0.053451</td>\n",
       "      <td>-0.158602</td>\n",
       "      <td>0.013316</td>\n",
       "      <td>-0.063966</td>\n",
       "      <td>0.150923</td>\n",
       "      <td>0.022072</td>\n",
       "      <td>0.168311</td>\n",
       "      <td>0.124978</td>\n",
       "      <td>-0.229316</td>\n",
       "      <td>0.006148</td>\n",
       "      <td>-0.104214</td>\n",
       "      <td>-0.089166</td>\n",
       "      <td>-0.112851</td>\n",
       "      <td>0.157767</td>\n",
       "      <td>-0.022986</td>\n",
       "      <td>0.275391</td>\n",
       "      <td>0.152245</td>\n",
       "      <td>-0.022349</td>\n",
       "      <td>0.043627</td>\n",
       "      <td>0.068497</td>\n",
       "      <td>-0.072003</td>\n",
       "      <td>0.164927</td>\n",
       "      <td>0.128153</td>\n",
       "      <td>-0.097877</td>\n",
       "      <td>0.112834</td>\n",
       "      <td>0.025989</td>\n",
       "      <td>-0.035945</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.204922</td>\n",
       "      <td>0.201669</td>\n",
       "      <td>0.095816</td>\n",
       "      <td>0.088479</td>\n",
       "      <td>-0.163301</td>\n",
       "      <td>0.178585</td>\n",
       "      <td>0.022706</td>\n",
       "      <td>0.072588</td>\n",
       "      <td>-0.085228</td>\n",
       "      <td>0.241856</td>\n",
       "      <td>-0.052063</td>\n",
       "      <td>0.206275</td>\n",
       "      <td>-0.226309</td>\n",
       "      <td>0.055761</td>\n",
       "      <td>-0.013927</td>\n",
       "      <td>-0.047932</td>\n",
       "      <td>0.055732</td>\n",
       "      <td>0.013788</td>\n",
       "      <td>0.058288</td>\n",
       "      <td>0.074495</td>\n",
       "      <td>-0.144048</td>\n",
       "      <td>0.24373</td>\n",
       "      <td>-0.124181</td>\n",
       "      <td>-0.023625</td>\n",
       "      <td>-8.46837</td>\n",
       "      <td>-0.155591</td>\n",
       "      <td>0.134095</td>\n",
       "      <td>0.142500</td>\n",
       "      <td>0.122580</td>\n",
       "      <td>0.353983</td>\n",
       "      <td>0.017859</td>\n",
       "      <td>-0.048209</td>\n",
       "      <td>-0.140820</td>\n",
       "      <td>-0.028225</td>\n",
       "      <td>0.431787</td>\n",
       "      <td>-0.342401</td>\n",
       "      <td>0.033433</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.178056</td>\n",
       "      <td>0.036207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1538 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     subreddit  up_votes         0  ...      1533      1534      1535\n",
       "0  r/AskReddit    221854  0.182144  ...  0.000455  0.178056  0.036207\n",
       "1  r/AskReddit    197524  0.090396  ...  0.000455  0.178056  0.036207\n",
       "2  r/AskReddit    186368  0.204430  ...  0.000455  0.178056  0.036207\n",
       "3  r/AskReddit    175339  0.074055  ...  0.909033  0.791063  0.122177\n",
       "4  r/AskReddit    160311  0.254428  ...  0.000455  0.178056  0.036207\n",
       "\n",
       "[5 rows x 1538 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Fh8pd6jCbSm"
   },
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D3FYg0j5837f"
   },
   "source": [
    "# Tensorflow with Basilia embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aztXGqLV88Vg"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e8flSUSq9Jfh"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3H8zqWwy9O-c"
   },
   "outputs": [],
   "source": [
    "reddit_df = pd.read_feather(\"expanded.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jCoqhcui9Ule"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    reddit_df.drop([\"up_votes\", 'subreddit'], axis=1), reddit_df[\"up_votes\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I-LIhJJvAe2H"
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "                    Dense(1024),\n",
    "                    LeakyReLU(),\n",
    "                    Dense(512),\n",
    "                    LeakyReLU(),\n",
    "                    Dense(512),\n",
    "                    LeakyReLU(),\n",
    "                    Dense(512),\n",
    "                    LeakyReLU(),\n",
    "                    Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MVmtGDdbBlYd"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(epsilon=0.1)\n",
    "# loss = tf.keras.losses.MeanSquaredError()\n",
    "# loss = tf.keras.losses.MeanSquaredLogarithmicError()\n",
    "loss = tf.keras.losses.Huber(1000)\n",
    "\n",
    "model.compile (\n",
    "    optimizer, loss, [\"mse, \"\"mae\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3MPxk2hLBA72"
   },
   "outputs": [],
   "source": [
    "stopping = tf.keras.callbacks.EarlyStopping('val_mae', 1, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "P25gnhqH9u7F",
    "outputId": "f7f5ac5c-6a2a-4d62-86d7-28acc2933af5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "436/436 [==============================] - 12s 29ms/step - loss: 4917489.0000 - mae: 5384.8618 - val_loss: 8637050.0000 - val_mae: 9108.1162\n",
      "Epoch 2/1000\n",
      "436/436 [==============================] - 12s 29ms/step - loss: 4745751.5000 - mae: 5210.4897 - val_loss: 8578394.0000 - val_mae: 9051.4062\n",
      "Epoch 3/1000\n",
      "436/436 [==============================] - 12s 28ms/step - loss: 4814850.5000 - mae: 5280.2622 - val_loss: 8976645.0000 - val_mae: 9452.4580\n",
      "Epoch 4/1000\n",
      "436/436 [==============================] - 12s 28ms/step - loss: 4831089.5000 - mae: 5295.3223 - val_loss: 8388234.0000 - val_mae: 8865.7646\n",
      "Epoch 5/1000\n",
      "436/436 [==============================] - 12s 28ms/step - loss: 4648857.5000 - mae: 5114.4092 - val_loss: 8277718.0000 - val_mae: 8744.5830\n",
      "Epoch 6/1000\n",
      "436/436 [==============================] - 12s 28ms/step - loss: 4535278.0000 - mae: 4997.2607 - val_loss: 8504876.0000 - val_mae: 8974.3438\n",
      "Epoch 7/1000\n",
      "436/436 [==============================] - 13s 30ms/step - loss: 4468897.5000 - mae: 4930.9229 - val_loss: 8529567.0000 - val_mae: 9002.9414\n",
      "Epoch 8/1000\n",
      "436/436 [==============================] - 13s 29ms/step - loss: 4388634.5000 - mae: 4848.2749 - val_loss: 8558064.0000 - val_mae: 9036.0283\n",
      "Epoch 9/1000\n",
      "436/436 [==============================] - 12s 28ms/step - loss: 4437706.0000 - mae: 4899.1890 - val_loss: 9258277.0000 - val_mae: 9735.7061\n",
      "Epoch 10/1000\n",
      "436/436 [==============================] - 12s 28ms/step - loss: 4423661.0000 - mae: 4884.9502 - val_loss: 8505110.0000 - val_mae: 8970.1631\n",
      "Epoch 11/1000\n",
      "436/436 [==============================] - 12s 28ms/step - loss: 4317321.5000 - mae: 4778.2720 - val_loss: 8911796.0000 - val_mae: 9383.8096\n",
      "Epoch 12/1000\n",
      "436/436 [==============================] - 12s 28ms/step - loss: 4238718.0000 - mae: 4697.8311 - val_loss: 8599287.0000 - val_mae: 9069.8906\n",
      "Epoch 13/1000\n",
      "436/436 [==============================] - 12s 28ms/step - loss: 4171681.0000 - mae: 4632.1133 - val_loss: 9039543.0000 - val_mae: 9509.0068\n",
      "Epoch 14/1000\n",
      "436/436 [==============================] - 12s 28ms/step - loss: 4046530.2500 - mae: 4503.6890 - val_loss: 8716217.0000 - val_mae: 9187.3984\n",
      "Epoch 15/1000\n",
      "436/436 [==============================] - 13s 29ms/step - loss: 4019753.2500 - mae: 4476.1514 - val_loss: 8715234.0000 - val_mae: 9185.7158\n",
      "Epoch 16/1000\n",
      "436/436 [==============================] - 12s 28ms/step - loss: 4016982.7500 - mae: 4475.1353 - val_loss: 8417706.0000 - val_mae: 8887.6436\n",
      "Epoch 17/1000\n",
      "436/436 [==============================] - 12s 28ms/step - loss: 3946113.5000 - mae: 4401.0312 - val_loss: 9001060.0000 - val_mae: 9466.4043\n",
      "Epoch 18/1000\n",
      "436/436 [==============================] - 12s 28ms/step - loss: 3940867.0000 - mae: 4395.7046 - val_loss: 8463242.0000 - val_mae: 8934.0264\n",
      "Epoch 19/1000\n",
      "436/436 [==============================] - 12s 28ms/step - loss: 3938117.2500 - mae: 4393.9248 - val_loss: 8314519.5000 - val_mae: 8782.1816\n",
      "Epoch 20/1000\n",
      "436/436 [==============================] - 12s 28ms/step - loss: 3960475.5000 - mae: 4417.6523 - val_loss: 8866872.0000 - val_mae: 9339.8994\n",
      "Epoch 21/1000\n",
      "436/436 [==============================] - 12s 28ms/step - loss: 3761574.2500 - mae: 4216.0107 - val_loss: 8762252.0000 - val_mae: 9234.5254\n",
      "Epoch 22/1000\n",
      "436/436 [==============================] - 12s 28ms/step - loss: 3821434.7500 - mae: 4276.4624 - val_loss: 8406518.0000 - val_mae: 8880.2441\n",
      "Epoch 23/1000\n",
      "436/436 [==============================] - 12s 28ms/step - loss: 3668814.7500 - mae: 4121.0527 - val_loss: 8992982.0000 - val_mae: 9468.8486\n",
      "Epoch 24/1000\n",
      "436/436 [==============================] - 12s 28ms/step - loss: 3698266.0000 - mae: 4152.2524 - val_loss: 8496772.0000 - val_mae: 8962.7451\n",
      "Epoch 25/1000\n",
      "436/436 [==============================] - 12s 28ms/step - loss: 3763270.5000 - mae: 4216.6562 - val_loss: 8494256.0000 - val_mae: 8963.5596\n",
      "Epoch 26/1000\n",
      "436/436 [==============================] - 13s 29ms/step - loss: 3648986.7500 - mae: 4101.8545 - val_loss: 8442884.0000 - val_mae: 8911.5361\n",
      "Epoch 27/1000\n",
      "436/436 [==============================] - 14s 32ms/step - loss: 3553937.7500 - mae: 4005.9929 - val_loss: 8284988.5000 - val_mae: 8754.6543\n",
      "Epoch 28/1000\n",
      "436/436 [==============================] - 14s 32ms/step - loss: 3580585.7500 - mae: 4032.0027 - val_loss: 8486523.0000 - val_mae: 8950.8584\n",
      "Epoch 29/1000\n",
      "436/436 [==============================] - 12s 28ms/step - loss: 3522595.0000 - mae: 3973.8518 - val_loss: 8454455.0000 - val_mae: 8921.1123\n",
      "Epoch 30/1000\n",
      "436/436 [==============================] - 12s 29ms/step - loss: 3514052.0000 - mae: 3963.1172 - val_loss: 8775789.0000 - val_mae: 9250.1016\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7062481b00>"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=1000,\n",
    "     validation_data=(X_test, y_test),\n",
    "    callbacks=[stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TZsqmc6fBdpN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qj3m70PqJgE6"
   },
   "source": [
    "# Autoencoder to reduce dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E3_Kdjm5Jj6T"
   },
   "outputs": [],
   "source": [
    "input = tf.keras.layers.Input(shape=(1536))\n",
    "encoder = Dense(1024, \"relu\")(input)\n",
    "encoder = Dense(512, \"relu\")(encoder)\n",
    "encoder = Dense(256, \"relu\")(encoder)\n",
    "\n",
    "decoder = Dense(512, \"relu\")(encoder)\n",
    "decoder = Dense(1024, \"relu\")(decoder)\n",
    "decoder = Dense(1536)(decoder)\n",
    "output = decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bHET3bIsJsp-"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Model(input, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KMzgCifPLr3X"
   },
   "outputs": [],
   "source": [
    "model.compile(\"adam\", \"mse\",[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "colab_type": "code",
    "id": "VledVZnxL4LL",
    "outputId": "4ad32e79-80bd-49d5-f426-d1ddc790c688"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "436/436 [==============================] - 23s 52ms/step - loss: 0.0235 - acc: 0.8150\n",
      "Epoch 2/10\n",
      "436/436 [==============================] - 22s 52ms/step - loss: 0.0142 - acc: 0.8599\n",
      "Epoch 3/10\n",
      "436/436 [==============================] - 22s 51ms/step - loss: 0.0125 - acc: 0.8673\n",
      "Epoch 4/10\n",
      "436/436 [==============================] - 23s 53ms/step - loss: 0.0115 - acc: 0.8732\n",
      "Epoch 5/10\n",
      "436/436 [==============================] - 22s 51ms/step - loss: 0.0108 - acc: 0.8799\n",
      "Epoch 6/10\n",
      "436/436 [==============================] - 22s 51ms/step - loss: 0.0103 - acc: 0.8806\n",
      "Epoch 7/10\n",
      "436/436 [==============================] - 22s 52ms/step - loss: 0.0101 - acc: 0.8791\n",
      "Epoch 8/10\n",
      "436/436 [==============================] - 22s 51ms/step - loss: 0.0097 - acc: 0.8840\n",
      "Epoch 9/10\n",
      "436/436 [==============================] - 22s 51ms/step - loss: 0.0095 - acc: 0.8844\n",
      "Epoch 10/10\n",
      "436/436 [==============================] - 22s 51ms/step - loss: 0.0091 - acc: 0.8868\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7062392d30>"
      ]
     },
     "execution_count": 161,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, X_train, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "id": "thyDZZtJL61z",
    "outputId": "3b011516-efb3-41cb-aa86-d54ced742ab8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:8 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f705e99f510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.00832764, 0.22634405, 0.35664377, 0.0007112 ], dtype=float32)"
      ]
     },
     "execution_count": 162,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([X_train.iloc[:1]])[0][0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "6SDWgPXuOLSO",
    "outputId": "83dd8b8d-317e-41e5-d5f2-efaea8e7d78b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.00601014, 0.227101, 0.431232, -0.0279421]"
      ]
     },
     "execution_count": 163,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[X_train.iloc[0].tolist(),][0][0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "m-PNiqFZOHYm",
    "outputId": "0e1e4585-0401-4324-d524-dc98404cac5c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.framework.ops.Tensor"
      ]
     },
     "execution_count": 166,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "qRpeMPYyXJ8V",
    "outputId": "4a5b7ed5-f682-47b5-9fec-39630f1874fd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x7f706247e4a8>"
      ]
     },
     "execution_count": 168,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 429
    },
    "colab_type": "code",
    "id": "U4h8a4_2XeTF",
    "outputId": "d58bd381-8523-4422-c4a0-84e423b54106"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_29\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_16 (InputLayer)        [(None, 1536)]            0         \n",
      "_________________________________________________________________\n",
      "dense_134 (Dense)            (None, 1024)              1573888   \n",
      "_________________________________________________________________\n",
      "dense_135 (Dense)            (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dense_136 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_137 (Dense)            (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_138 (Dense)            (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "dense_139 (Dense)            (None, 1536)              1574400   \n",
      "=================================================================\n",
      "Total params: 4,461,312\n",
      "Trainable params: 0\n",
      "Non-trainable params: 4,461,312\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder = model\n",
    "for layer in autoencoder.layers:\n",
    "    layer.trainable=False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7mqQXqJib6K"
   },
   "source": [
    "# Autoencoder + NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rq5i7os3iakr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y7PxfIORX9ic"
   },
   "outputs": [],
   "source": [
    "predictor = Dense(256, \"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.1), bias_regularizer=tf.keras.regularizers.l2(0.1))(encoder)\n",
    "Predictor = Dropout(0.5)(predictor)\n",
    "predictor = Dense(256, \"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.1), bias_regularizer=tf.keras.regularizers.l2(0.1))(predictor)\n",
    "Predictor = Dropout(0.5)(predictor)\n",
    "predictor = Dense(256, \"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.1), bias_regularizer=tf.keras.regularizers.l2(0.1))(predictor)\n",
    "Predictor = Dropout(0.5)(predictor)\n",
    "predictor = Dense(1)(predictor)\n",
    "upvote_predictor = predictor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "IXDYDy6oY7fk",
    "outputId": "d13fc98b-10d1-43e5-c428-7a60b9c00af6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dense_167/BiasAdd:0' shape=(None, 1) dtype=float32>"
      ]
     },
     "execution_count": 206,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q5_Z4SKWYaxh"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Model(input,upvote_predictor )\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(epsilon=0.01)\n",
    "# loss = tf.keras.losses.MeanSquaredError()\n",
    "# loss = tf.keras.losses.MeanSquaredLogarithmicError()\n",
    "loss = tf.keras.losses.Huber(10000)\n",
    "\n",
    "model.compile (\n",
    "    optimizer, loss, [\"mse\", \"mae\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qcge4BZzb6Ol"
   },
   "outputs": [],
   "source": [
    "stopping = tf.keras.callbacks.EarlyStopping('val_mae', 1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "dhS_rRLMZKwU",
    "outputId": "cc2c5ef4-5bf5-4b5f-ba7c-cd86164916a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "436/436 [==============================] - 5s 11ms/step - loss: 118118152.0000 - mse: 605566144.0000 - mae: 15918.3799 - val_loss: 103745328.0000 - val_mse: 508790336.0000 - val_mae: 14379.0391\n",
      "Epoch 2/1000\n",
      "436/436 [==============================] - 4s 10ms/step - loss: 93462520.0000 - mse: 404163168.0000 - mae: 13312.4053 - val_loss: 90515608.0000 - val_mse: 381173664.0000 - val_mae: 13079.1602\n",
      "Epoch 3/1000\n",
      "436/436 [==============================] - 4s 10ms/step - loss: 80906344.0000 - mse: 336550112.0000 - mae: 11890.4121 - val_loss: 80464592.0000 - val_mse: 367192544.0000 - val_mae: 11748.9248\n",
      "Epoch 4/1000\n",
      "436/436 [==============================] - 5s 10ms/step - loss: 74687184.0000 - mse: 315335552.0000 - mae: 11152.9551 - val_loss: 80096128.0000 - val_mse: 342045440.0000 - val_mae: 11899.6855\n",
      "Epoch 5/1000\n",
      "436/436 [==============================] - 5s 10ms/step - loss: 71830968.0000 - mse: 301632000.0000 - mae: 10810.8975 - val_loss: 76391208.0000 - val_mse: 354808768.0000 - val_mae: 11181.7373\n",
      "Epoch 6/1000\n",
      "436/436 [==============================] - 5s 10ms/step - loss: 70313376.0000 - mse: 294455488.0000 - mae: 10640.8203 - val_loss: 73887464.0000 - val_mse: 333324032.0000 - val_mae: 11000.4844\n",
      "Epoch 7/1000\n",
      "436/436 [==============================] - 5s 11ms/step - loss: 68570448.0000 - mse: 285328928.0000 - mae: 10445.3838 - val_loss: 72736776.0000 - val_mse: 329766176.0000 - val_mae: 10802.0938\n",
      "Epoch 8/1000\n",
      "436/436 [==============================] - 5s 11ms/step - loss: 67277448.0000 - mse: 279609632.0000 - mae: 10288.7529 - val_loss: 71624240.0000 - val_mse: 320243584.0000 - val_mae: 10757.9814\n",
      "Epoch 9/1000\n",
      "436/436 [==============================] - 5s 11ms/step - loss: 66126564.0000 - mse: 274383328.0000 - mae: 10150.9893 - val_loss: 71649432.0000 - val_mse: 308060224.0000 - val_mae: 10894.7266\n",
      "Epoch 10/1000\n",
      "436/436 [==============================] - 5s 10ms/step - loss: 65160660.0000 - mse: 269310560.0000 - mae: 10043.2168 - val_loss: 69142952.0000 - val_mse: 311207264.0000 - val_mae: 10431.2139\n",
      "Epoch 11/1000\n",
      "436/436 [==============================] - 4s 10ms/step - loss: 63870240.0000 - mse: 263766320.0000 - mae: 9890.3115 - val_loss: 68536184.0000 - val_mse: 303487616.0000 - val_mae: 10398.1299\n",
      "Epoch 12/1000\n",
      "436/436 [==============================] - 5s 10ms/step - loss: 63105616.0000 - mse: 259271392.0000 - mae: 9809.3252 - val_loss: 67762496.0000 - val_mse: 299231104.0000 - val_mae: 10301.6836\n",
      "Epoch 13/1000\n",
      "436/436 [==============================] - 5s 11ms/step - loss: 62017548.0000 - mse: 253497024.0000 - mae: 9672.2744 - val_loss: 67151200.0000 - val_mse: 305102080.0000 - val_mae: 10153.5557\n",
      "Epoch 14/1000\n",
      "436/436 [==============================] - 5s 11ms/step - loss: 61243848.0000 - mse: 249715328.0000 - mae: 9584.5039 - val_loss: 66483584.0000 - val_mse: 300129440.0000 - val_mae: 10107.5488\n",
      "Epoch 15/1000\n",
      "436/436 [==============================] - 5s 11ms/step - loss: 60161432.0000 - mse: 245112864.0000 - mae: 9464.6377 - val_loss: 65035704.0000 - val_mse: 291584704.0000 - val_mae: 9957.9551\n",
      "Epoch 16/1000\n",
      "436/436 [==============================] - 5s 11ms/step - loss: 59584180.0000 - mse: 242353968.0000 - mae: 9405.4062 - val_loss: 65084288.0000 - val_mse: 294236608.0000 - val_mae: 9879.4990\n",
      "Epoch 17/1000\n",
      "436/436 [==============================] - 5s 11ms/step - loss: 58832168.0000 - mse: 237959184.0000 - mae: 9313.4951 - val_loss: 64347824.0000 - val_mse: 277169728.0000 - val_mae: 9922.9961\n",
      "Epoch 18/1000\n",
      "436/436 [==============================] - 5s 11ms/step - loss: 58214340.0000 - mse: 234830544.0000 - mae: 9244.0781 - val_loss: 63053456.0000 - val_mse: 279159552.0000 - val_mae: 9732.3750\n",
      "Epoch 19/1000\n",
      "436/436 [==============================] - 5s 11ms/step - loss: 57524536.0000 - mse: 232134160.0000 - mae: 9165.1914 - val_loss: 63571220.0000 - val_mse: 283585888.0000 - val_mae: 9692.2510\n",
      "Epoch 20/1000\n",
      "436/436 [==============================] - 5s 11ms/step - loss: 56933832.0000 - mse: 228711248.0000 - mae: 9090.7578 - val_loss: 62961980.0000 - val_mse: 269401440.0000 - val_mae: 9846.4980\n",
      "Epoch 21/1000\n",
      "436/436 [==============================] - 5s 12ms/step - loss: 56366188.0000 - mse: 226128064.0000 - mae: 9030.7549 - val_loss: 62223228.0000 - val_mse: 268982784.0000 - val_mae: 9645.4082\n",
      "Epoch 22/1000\n",
      "436/436 [==============================] - 5s 11ms/step - loss: 55964024.0000 - mse: 225021344.0000 - mae: 8976.1514 - val_loss: 62161596.0000 - val_mse: 276078016.0000 - val_mae: 9572.5586\n",
      "Epoch 23/1000\n",
      "436/436 [==============================] - 5s 11ms/step - loss: 55729104.0000 - mse: 223189632.0000 - mae: 8941.8447 - val_loss: 61928516.0000 - val_mse: 268906624.0000 - val_mae: 9594.4307\n",
      "Epoch 24/1000\n",
      "436/436 [==============================] - 5s 10ms/step - loss: 55220768.0000 - mse: 222371200.0000 - mae: 8875.2080 - val_loss: 61720416.0000 - val_mse: 273456896.0000 - val_mae: 9563.4678\n",
      "Epoch 25/1000\n",
      "436/436 [==============================] - 5s 11ms/step - loss: 54868584.0000 - mse: 220108880.0000 - mae: 8847.0908 - val_loss: 61416332.0000 - val_mse: 267180896.0000 - val_mae: 9503.9160\n",
      "Epoch 26/1000\n",
      "436/436 [==============================] - 5s 10ms/step - loss: 54165576.0000 - mse: 216822480.0000 - mae: 8759.2842 - val_loss: 61719144.0000 - val_mse: 261303216.0000 - val_mae: 9659.8604\n",
      "Epoch 27/1000\n",
      "436/436 [==============================] - 5s 10ms/step - loss: 54004852.0000 - mse: 216111504.0000 - mae: 8744.9375 - val_loss: 61191928.0000 - val_mse: 267095728.0000 - val_mae: 9500.6641\n",
      "Epoch 28/1000\n",
      "436/436 [==============================] - 5s 10ms/step - loss: 53455624.0000 - mse: 213387216.0000 - mae: 8671.8271 - val_loss: 61457776.0000 - val_mse: 267731056.0000 - val_mae: 9612.1934\n",
      "Epoch 29/1000\n",
      "436/436 [==============================] - 5s 11ms/step - loss: 53030524.0000 - mse: 211707456.0000 - mae: 8627.4170 - val_loss: 62383860.0000 - val_mse: 258362192.0000 - val_mae: 9818.4551\n",
      "Epoch 30/1000\n",
      "436/436 [==============================] - 5s 11ms/step - loss: 52672360.0000 - mse: 210020464.0000 - mae: 8581.6387 - val_loss: 61715152.0000 - val_mse: 261617568.0000 - val_mae: 9591.1377\n",
      "Epoch 31/1000\n",
      "436/436 [==============================] - 5s 11ms/step - loss: 52294940.0000 - mse: 208890368.0000 - mae: 8520.8086 - val_loss: 60868644.0000 - val_mse: 266165872.0000 - val_mae: 9443.4795\n",
      "Epoch 32/1000\n",
      "436/436 [==============================] - 5s 11ms/step - loss: 51910976.0000 - mse: 207060064.0000 - mae: 8483.6953 - val_loss: 61005824.0000 - val_mse: 257305856.0000 - val_mae: 9606.6631\n",
      "Epoch 33/1000\n",
      "436/436 [==============================] - 5s 12ms/step - loss: 51395496.0000 - mse: 205522144.0000 - mae: 8414.7227 - val_loss: 60327544.0000 - val_mse: 260186752.0000 - val_mae: 9394.2842\n",
      "Epoch 34/1000\n",
      "436/436 [==============================] - 5s 11ms/step - loss: 51388280.0000 - mse: 204041856.0000 - mae: 8410.4248 - val_loss: 60788308.0000 - val_mse: 256670912.0000 - val_mae: 9544.3604\n",
      "Epoch 35/1000\n",
      "436/436 [==============================] - 5s 11ms/step - loss: 50916092.0000 - mse: 203071088.0000 - mae: 8340.0352 - val_loss: 60907888.0000 - val_mse: 259614896.0000 - val_mae: 9465.3770\n",
      "Epoch 36/1000\n",
      "436/436 [==============================] - 5s 11ms/step - loss: 50417764.0000 - mse: 200532000.0000 - mae: 8289.3320 - val_loss: 60144596.0000 - val_mse: 254941312.0000 - val_mae: 9493.5771\n",
      "Epoch 37/1000\n",
      "436/436 [==============================] - 5s 11ms/step - loss: 50060904.0000 - mse: 199035008.0000 - mae: 8257.4541 - val_loss: 61189172.0000 - val_mse: 267210096.0000 - val_mae: 9401.6777\n",
      "Epoch 38/1000\n",
      "436/436 [==============================] - 5s 11ms/step - loss: 49502288.0000 - mse: 196146112.0000 - mae: 8167.9780 - val_loss: 60720344.0000 - val_mse: 262529280.0000 - val_mae: 9456.7363\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f705e8338d0>"
      ]
     },
     "execution_count": 209,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=1000, validation_data=(X_test, y_test),\n",
    "    callbacks=[stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pYxKOFYAZXz1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ayUH6Htpiqmn"
   },
   "source": [
    "# Autoendoer + sklearn models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X7x8eydUjzBy"
   },
   "outputs": [],
   "source": [
    "encoded = tf.keras.Model(input, encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "gvav3JMhk_4u",
    "outputId": "4b75bee6-1b03-43bb-b52b-5e0cdce139e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /usr/local/lib/python3.6/dist-packages (0.90)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from xgboost) (1.18.5)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from xgboost) (1.4.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vfeQJRkZithx"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNetCV as sk_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mIyQyDFplIgS"
   },
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor as sk_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NE_0T1dJlXsR"
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor as sk_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t-QpQiIOk-vB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YOvrt38ri5Bx"
   },
   "outputs": [],
   "source": [
    "model = sk_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 894
    },
    "colab_type": "code",
    "id": "bIH80I2Mi737",
    "outputId": "e27451da-a72c-41e1-c8d7-ec30ca37d73e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 1536) for input Tensor(\"input_16:0\", shape=(None, 1536), dtype=float32), but it was called on an input with incompatible shape (None, 768).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-284-6d8c819c5464>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m    129\u001b[0m           method.__name__))\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m   return tf_decorator.make_decorator(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1597\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1598\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1599\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1600\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1601\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    812\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2826\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2828\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2829\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3208\u001b[0m           \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_signature\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3209\u001b[0m           and call_context_key in self._function_cache.missed):\n\u001b[0;32m-> 3210\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_define_function_with_shape_relaxation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_define_function_with_shape_relaxation\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3141\u001b[0m     graph_function = self._create_graph_function(\n\u001b[0;32m-> 3142\u001b[0;31m         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\n\u001b[0m\u001b[1;32m   3143\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marg_relaxed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrank_only_cache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3073\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3074\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3075\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3076\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3077\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    971\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1462 predict_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1452 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1445 run_step  **\n        outputs = model.predict_step(data)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1418 predict_step\n        return self(x, training=False)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:985 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py:386 call\n        inputs, training=training, mask=mask)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py:508 _run_internal_graph\n        outputs = node.layer(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:976 __call__\n        self.name)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/input_spec.py:216 assert_input_compatibility\n        ' but received input with shape ' + str(shape))\n\n    ValueError: Input 0 of layer dense_134 is incompatible with the layer: expected axis -1 of input shape to have value 1536 but received input with shape [None, 768]\n"
     ]
    }
   ],
   "source": [
    "model.fit(encoded.predict(X_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "baad10mKi-h4"
   },
   "outputs": [],
   "source": [
    "model.score(encoded.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ip-9OJ7AkBmm"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 894
    },
    "colab_type": "code",
    "id": "oH0jzpslkksv",
    "outputId": "0585872c-0290-43d3-a19f-ad52997dc50c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 1536) for input Tensor(\"input_16:0\", shape=(None, 1536), dtype=float32), but it was called on an input with incompatible shape (None, 768).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-286-c296a2ef57d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m    129\u001b[0m           method.__name__))\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m   return tf_decorator.make_decorator(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1597\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1598\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1599\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1600\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1601\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    812\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2826\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2828\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2829\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3208\u001b[0m           \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_signature\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3209\u001b[0m           and call_context_key in self._function_cache.missed):\n\u001b[0;32m-> 3210\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_define_function_with_shape_relaxation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_define_function_with_shape_relaxation\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3141\u001b[0m     graph_function = self._create_graph_function(\n\u001b[0;32m-> 3142\u001b[0;31m         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\n\u001b[0m\u001b[1;32m   3143\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marg_relaxed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrank_only_cache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3073\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3074\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3075\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3076\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3077\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    971\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1462 predict_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1452 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1445 run_step  **\n        outputs = model.predict_step(data)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1418 predict_step\n        return self(x, training=False)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:985 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py:386 call\n        inputs, training=training, mask=mask)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py:508 _run_internal_graph\n        outputs = node.layer(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:976 __call__\n        self.name)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/input_spec.py:216 assert_input_compatibility\n        ' but received input with shape ' + str(shape))\n\n    ValueError: Input 0 of layer dense_134 is incompatible with the layer: expected axis -1 of input shape to have value 1536 but received input with shape [None, 768]\n"
     ]
    }
   ],
   "source": [
    "mean_absolute_error(y_test, model.predict(encoded.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jXvtBmKtkscU"
   },
   "outputs": [],
   "source": [
    "!unxz \"embeddings.pq(1).xz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Aj9KPJlxltTs"
   },
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"embeddings.pq(1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "colab_type": "code",
    "id": "r7F5FoYwmALh",
    "outputId": "8620147d-f828-4c63-83cb-2cad8ad51efb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>up_votes</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title_embeddings</th>\n",
       "      <th>text_embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>221854</td>\n",
       "      <td>r/AskReddit</td>\n",
       "      <td>[0.182144, -0.0526975, 0.296982, -0.0468543, 0...</td>\n",
       "      <td>[-0.239577, 0.182857, 0.284122, -0.0861422, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   up_votes  ...                                    text_embeddings\n",
       "0    221854  ...  [-0.239577, 0.182857, 0.284122, -0.0861422, 0....\n",
       "\n",
       "[1 rows x 4 columns]"
      ]
     },
     "execution_count": 277,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dJA55K_2mBj2"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.drop([\"up_votes\", 'subreddit'], axis=1), df[\"up_votes\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "id": "d-96vCGToljC",
    "outputId": "f4455b1d-b380-46cc-c37e-e78c09371e92"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_embeddings</th>\n",
       "      <th>text_embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14398</th>\n",
       "      <td>[0.0267503, -0.098055, 0.199862, -0.0685595, 0...</td>\n",
       "      <td>[-0.239577, 0.182857, 0.284122, -0.0861422, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7164</th>\n",
       "      <td>[0.0193013, -0.0542681, 0.306836, -0.0460507, ...</td>\n",
       "      <td>[-0.239577, 0.182857, 0.284122, -0.0861422, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16879</th>\n",
       "      <td>[-0.108175, -0.948139, 0.287051, -0.0746186, 0...</td>\n",
       "      <td>[-0.279692, -0.564432, 0.462589, 0.0895764, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15784</th>\n",
       "      <td>[0.0553842, 0.109612, 0.324887, -0.265758, 0.2...</td>\n",
       "      <td>[0.109471, -0.191406, 0.534527, -0.285127, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3050</th>\n",
       "      <td>[0.0601381, 0.102246, 0.318399, -0.00483384, 0...</td>\n",
       "      <td>[0.0452943, -0.234401, 0.0393467, -0.00753656,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        title_embeddings                                    text_embeddings\n",
       "14398  [0.0267503, -0.098055, 0.199862, -0.0685595, 0...  [-0.239577, 0.182857, 0.284122, -0.0861422, 0....\n",
       "7164   [0.0193013, -0.0542681, 0.306836, -0.0460507, ...  [-0.239577, 0.182857, 0.284122, -0.0861422, 0....\n",
       "16879  [-0.108175, -0.948139, 0.287051, -0.0746186, 0...  [-0.279692, -0.564432, 0.462589, 0.0895764, 0....\n",
       "15784  [0.0553842, 0.109612, 0.324887, -0.265758, 0.2...  [0.109471, -0.191406, 0.534527, -0.285127, 0.1...\n",
       "3050   [0.0601381, 0.102246, 0.318399, -0.00483384, 0...  [0.0452943, -0.234401, 0.0393467, -0.00753656,..."
      ]
     },
     "execution_count": 301,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "colab_type": "code",
    "id": "EzwoDpmOmn_C",
    "outputId": "b15388b8-692d-4580-c622-b4ccdb32b517"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-299-d768f88d541e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    358\u001b[0m                                    missing=self.missing, nthread=self.n_jobs)\n\u001b[1;32m    359\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m             \u001b[0mtrainDmatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnthread\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0mevals_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, label, missing, weight, silent, feature_names, feature_types, nthread)\u001b[0m\n\u001b[1;32m    378\u001b[0m         data, feature_names, feature_types = _maybe_pandas_data(data,\n\u001b[1;32m    379\u001b[0m                                                                 \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m                                                                 feature_types)\n\u001b[0m\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         data, feature_names, feature_types = _maybe_dt_data(data,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_maybe_pandas_data\u001b[0;34m(data, feature_names, feature_types)\u001b[0m\n\u001b[1;32m    237\u001b[0m         msg = \"\"\"DataFrame.dtypes for data must be int, float or bool.\n\u001b[1;32m    238\u001b[0m                 Did not expect the data types in fields \"\"\"\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbad_fields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfeature_names\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: DataFrame.dtypes for data must be int, float or bool.\n                Did not expect the data types in fields title_embeddings, text_embeddings"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7WDu3-0smx-w"
   },
   "outputs": [],
   "source": [
    "model.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xkjTfnsjm5MW"
   },
   "outputs": [],
   "source": [
    "reddit_df = pd.read_parquet(\"embeddings.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W6PP_XTdO4K9"
   },
   "outputs": [],
   "source": [
    "title_embeddings = reddit_df['title_embeddings'].apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "colab_type": "code",
    "id": "8kHqhBGpPonL",
    "outputId": "a4d06a37-1d25-4440-f18f-e77ed2ce922d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>...</th>\n",
       "      <th>728</th>\n",
       "      <th>729</th>\n",
       "      <th>730</th>\n",
       "      <th>731</th>\n",
       "      <th>732</th>\n",
       "      <th>733</th>\n",
       "      <th>734</th>\n",
       "      <th>735</th>\n",
       "      <th>736</th>\n",
       "      <th>737</th>\n",
       "      <th>738</th>\n",
       "      <th>739</th>\n",
       "      <th>740</th>\n",
       "      <th>741</th>\n",
       "      <th>742</th>\n",
       "      <th>743</th>\n",
       "      <th>744</th>\n",
       "      <th>745</th>\n",
       "      <th>746</th>\n",
       "      <th>747</th>\n",
       "      <th>748</th>\n",
       "      <th>749</th>\n",
       "      <th>750</th>\n",
       "      <th>751</th>\n",
       "      <th>752</th>\n",
       "      <th>753</th>\n",
       "      <th>754</th>\n",
       "      <th>755</th>\n",
       "      <th>756</th>\n",
       "      <th>757</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.182144</td>\n",
       "      <td>-0.052698</td>\n",
       "      <td>0.296982</td>\n",
       "      <td>-0.046854</td>\n",
       "      <td>0.066862</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.109145</td>\n",
       "      <td>0.115977</td>\n",
       "      <td>-0.300374</td>\n",
       "      <td>0.012484</td>\n",
       "      <td>0.142182</td>\n",
       "      <td>-0.068124</td>\n",
       "      <td>-0.208033</td>\n",
       "      <td>0.149216</td>\n",
       "      <td>0.046058</td>\n",
       "      <td>0.014590</td>\n",
       "      <td>0.069055</td>\n",
       "      <td>0.108952</td>\n",
       "      <td>0.146693</td>\n",
       "      <td>-0.115352</td>\n",
       "      <td>0.096257</td>\n",
       "      <td>-0.078912</td>\n",
       "      <td>-0.141155</td>\n",
       "      <td>-0.117024</td>\n",
       "      <td>0.098890</td>\n",
       "      <td>-0.202187</td>\n",
       "      <td>0.057147</td>\n",
       "      <td>0.059443</td>\n",
       "      <td>0.018391</td>\n",
       "      <td>0.045539</td>\n",
       "      <td>0.011138</td>\n",
       "      <td>-0.157200</td>\n",
       "      <td>-0.041015</td>\n",
       "      <td>0.011421</td>\n",
       "      <td>-0.031243</td>\n",
       "      <td>0.112957</td>\n",
       "      <td>-0.087513</td>\n",
       "      <td>-0.060702</td>\n",
       "      <td>0.018499</td>\n",
       "      <td>-0.012571</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.079505</td>\n",
       "      <td>0.104720</td>\n",
       "      <td>-0.026521</td>\n",
       "      <td>-0.118880</td>\n",
       "      <td>-0.133549</td>\n",
       "      <td>0.286074</td>\n",
       "      <td>0.133212</td>\n",
       "      <td>0.056164</td>\n",
       "      <td>0.017515</td>\n",
       "      <td>0.037554</td>\n",
       "      <td>-0.028962</td>\n",
       "      <td>0.015284</td>\n",
       "      <td>-0.267593</td>\n",
       "      <td>0.120774</td>\n",
       "      <td>-0.010770</td>\n",
       "      <td>0.151480</td>\n",
       "      <td>0.093855</td>\n",
       "      <td>0.009536</td>\n",
       "      <td>-0.033602</td>\n",
       "      <td>-0.096230</td>\n",
       "      <td>0.050461</td>\n",
       "      <td>0.054975</td>\n",
       "      <td>-0.051313</td>\n",
       "      <td>0.039797</td>\n",
       "      <td>-9.00595</td>\n",
       "      <td>0.057208</td>\n",
       "      <td>-0.009514</td>\n",
       "      <td>0.064110</td>\n",
       "      <td>-0.130201</td>\n",
       "      <td>0.078111</td>\n",
       "      <td>-0.004505</td>\n",
       "      <td>-0.211739</td>\n",
       "      <td>-0.026336</td>\n",
       "      <td>0.000743</td>\n",
       "      <td>0.333825</td>\n",
       "      <td>-0.151763</td>\n",
       "      <td>-0.064953</td>\n",
       "      <td>0.050184</td>\n",
       "      <td>-0.023745</td>\n",
       "      <td>-0.108041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.090396</td>\n",
       "      <td>-0.063034</td>\n",
       "      <td>0.183588</td>\n",
       "      <td>-0.132425</td>\n",
       "      <td>0.218097</td>\n",
       "      <td>-0.061284</td>\n",
       "      <td>0.194140</td>\n",
       "      <td>0.099648</td>\n",
       "      <td>-0.110126</td>\n",
       "      <td>-0.059502</td>\n",
       "      <td>0.033862</td>\n",
       "      <td>0.062735</td>\n",
       "      <td>-0.222219</td>\n",
       "      <td>0.129082</td>\n",
       "      <td>-0.075026</td>\n",
       "      <td>0.102772</td>\n",
       "      <td>0.047288</td>\n",
       "      <td>0.282739</td>\n",
       "      <td>0.116910</td>\n",
       "      <td>-0.196864</td>\n",
       "      <td>-0.068466</td>\n",
       "      <td>-0.230079</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>-0.075219</td>\n",
       "      <td>0.086746</td>\n",
       "      <td>-0.081459</td>\n",
       "      <td>0.198271</td>\n",
       "      <td>0.131429</td>\n",
       "      <td>0.164833</td>\n",
       "      <td>0.104517</td>\n",
       "      <td>0.061296</td>\n",
       "      <td>-0.225312</td>\n",
       "      <td>0.071521</td>\n",
       "      <td>-0.182611</td>\n",
       "      <td>-0.167435</td>\n",
       "      <td>0.170808</td>\n",
       "      <td>-0.015743</td>\n",
       "      <td>-0.016329</td>\n",
       "      <td>0.002535</td>\n",
       "      <td>0.039329</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027212</td>\n",
       "      <td>0.175481</td>\n",
       "      <td>-0.031569</td>\n",
       "      <td>-0.048201</td>\n",
       "      <td>-0.122848</td>\n",
       "      <td>0.264615</td>\n",
       "      <td>0.055259</td>\n",
       "      <td>-0.122896</td>\n",
       "      <td>-0.073748</td>\n",
       "      <td>0.025588</td>\n",
       "      <td>-0.026207</td>\n",
       "      <td>0.110434</td>\n",
       "      <td>-0.153051</td>\n",
       "      <td>0.239316</td>\n",
       "      <td>0.075326</td>\n",
       "      <td>0.244874</td>\n",
       "      <td>0.320699</td>\n",
       "      <td>0.046222</td>\n",
       "      <td>-0.022302</td>\n",
       "      <td>-0.087042</td>\n",
       "      <td>-0.110124</td>\n",
       "      <td>0.065037</td>\n",
       "      <td>-0.062415</td>\n",
       "      <td>0.060719</td>\n",
       "      <td>-8.97873</td>\n",
       "      <td>0.044415</td>\n",
       "      <td>0.048184</td>\n",
       "      <td>-0.005165</td>\n",
       "      <td>-0.039495</td>\n",
       "      <td>0.020332</td>\n",
       "      <td>-0.104020</td>\n",
       "      <td>-0.159283</td>\n",
       "      <td>0.081489</td>\n",
       "      <td>-0.094288</td>\n",
       "      <td>0.081652</td>\n",
       "      <td>-0.091267</td>\n",
       "      <td>0.003904</td>\n",
       "      <td>-0.030987</td>\n",
       "      <td>0.065567</td>\n",
       "      <td>0.015198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.204430</td>\n",
       "      <td>-0.299815</td>\n",
       "      <td>0.278582</td>\n",
       "      <td>-0.330959</td>\n",
       "      <td>0.283490</td>\n",
       "      <td>0.167636</td>\n",
       "      <td>-0.000396</td>\n",
       "      <td>0.186616</td>\n",
       "      <td>-0.080640</td>\n",
       "      <td>-0.002074</td>\n",
       "      <td>0.083603</td>\n",
       "      <td>0.029153</td>\n",
       "      <td>-0.071468</td>\n",
       "      <td>-0.101955</td>\n",
       "      <td>-0.088183</td>\n",
       "      <td>0.027302</td>\n",
       "      <td>-0.042051</td>\n",
       "      <td>0.134516</td>\n",
       "      <td>0.101671</td>\n",
       "      <td>-0.028426</td>\n",
       "      <td>0.077124</td>\n",
       "      <td>-0.186997</td>\n",
       "      <td>-0.313275</td>\n",
       "      <td>-0.250894</td>\n",
       "      <td>0.080097</td>\n",
       "      <td>-0.078265</td>\n",
       "      <td>0.297395</td>\n",
       "      <td>0.140284</td>\n",
       "      <td>-0.008929</td>\n",
       "      <td>0.023978</td>\n",
       "      <td>0.057998</td>\n",
       "      <td>-0.050854</td>\n",
       "      <td>0.088528</td>\n",
       "      <td>0.067004</td>\n",
       "      <td>-0.133049</td>\n",
       "      <td>0.062508</td>\n",
       "      <td>-0.102993</td>\n",
       "      <td>-0.093009</td>\n",
       "      <td>-0.085430</td>\n",
       "      <td>0.076165</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013532</td>\n",
       "      <td>0.145643</td>\n",
       "      <td>0.047052</td>\n",
       "      <td>-0.193214</td>\n",
       "      <td>-0.099032</td>\n",
       "      <td>0.441819</td>\n",
       "      <td>-0.026864</td>\n",
       "      <td>0.064393</td>\n",
       "      <td>-0.085460</td>\n",
       "      <td>-0.063976</td>\n",
       "      <td>0.001207</td>\n",
       "      <td>0.048360</td>\n",
       "      <td>-0.295799</td>\n",
       "      <td>0.250361</td>\n",
       "      <td>0.007193</td>\n",
       "      <td>0.002607</td>\n",
       "      <td>0.367675</td>\n",
       "      <td>0.064131</td>\n",
       "      <td>-0.044522</td>\n",
       "      <td>-0.257064</td>\n",
       "      <td>-0.175361</td>\n",
       "      <td>0.061791</td>\n",
       "      <td>-0.198281</td>\n",
       "      <td>-0.037277</td>\n",
       "      <td>-9.16263</td>\n",
       "      <td>0.137411</td>\n",
       "      <td>0.065680</td>\n",
       "      <td>0.157470</td>\n",
       "      <td>-0.289391</td>\n",
       "      <td>0.087019</td>\n",
       "      <td>-0.099552</td>\n",
       "      <td>-0.125894</td>\n",
       "      <td>0.119627</td>\n",
       "      <td>-0.012324</td>\n",
       "      <td>0.406567</td>\n",
       "      <td>-0.109556</td>\n",
       "      <td>-0.028680</td>\n",
       "      <td>0.201417</td>\n",
       "      <td>-0.064270</td>\n",
       "      <td>0.095350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.074055</td>\n",
       "      <td>-0.994510</td>\n",
       "      <td>0.083315</td>\n",
       "      <td>0.173790</td>\n",
       "      <td>0.435514</td>\n",
       "      <td>0.489149</td>\n",
       "      <td>-0.442962</td>\n",
       "      <td>-0.515874</td>\n",
       "      <td>-0.247308</td>\n",
       "      <td>-0.210775</td>\n",
       "      <td>0.038379</td>\n",
       "      <td>0.199510</td>\n",
       "      <td>-0.283324</td>\n",
       "      <td>-0.483493</td>\n",
       "      <td>-0.898885</td>\n",
       "      <td>0.334158</td>\n",
       "      <td>0.353439</td>\n",
       "      <td>-1.088240</td>\n",
       "      <td>-1.086190</td>\n",
       "      <td>0.170191</td>\n",
       "      <td>-0.323830</td>\n",
       "      <td>-0.223867</td>\n",
       "      <td>-0.157821</td>\n",
       "      <td>-0.040704</td>\n",
       "      <td>-0.135643</td>\n",
       "      <td>0.109425</td>\n",
       "      <td>-0.167469</td>\n",
       "      <td>-0.357555</td>\n",
       "      <td>-0.247309</td>\n",
       "      <td>-0.306711</td>\n",
       "      <td>0.350606</td>\n",
       "      <td>-0.533339</td>\n",
       "      <td>0.937067</td>\n",
       "      <td>0.810881</td>\n",
       "      <td>-0.742399</td>\n",
       "      <td>-0.461994</td>\n",
       "      <td>-0.695902</td>\n",
       "      <td>-0.148049</td>\n",
       "      <td>-0.560312</td>\n",
       "      <td>-0.004402</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.485970</td>\n",
       "      <td>-0.319889</td>\n",
       "      <td>0.264213</td>\n",
       "      <td>-0.837947</td>\n",
       "      <td>0.662875</td>\n",
       "      <td>0.054413</td>\n",
       "      <td>0.096752</td>\n",
       "      <td>-0.052930</td>\n",
       "      <td>-0.357925</td>\n",
       "      <td>0.054988</td>\n",
       "      <td>-0.023470</td>\n",
       "      <td>-0.015924</td>\n",
       "      <td>-0.816854</td>\n",
       "      <td>0.350448</td>\n",
       "      <td>0.005052</td>\n",
       "      <td>-0.324144</td>\n",
       "      <td>0.634460</td>\n",
       "      <td>-0.995354</td>\n",
       "      <td>-0.489603</td>\n",
       "      <td>0.468945</td>\n",
       "      <td>0.522310</td>\n",
       "      <td>1.203780</td>\n",
       "      <td>-0.918136</td>\n",
       "      <td>-0.362671</td>\n",
       "      <td>-1.22464</td>\n",
       "      <td>0.824677</td>\n",
       "      <td>0.418465</td>\n",
       "      <td>0.032358</td>\n",
       "      <td>-0.213618</td>\n",
       "      <td>-0.244904</td>\n",
       "      <td>0.089304</td>\n",
       "      <td>0.333229</td>\n",
       "      <td>0.727644</td>\n",
       "      <td>-0.946214</td>\n",
       "      <td>0.688935</td>\n",
       "      <td>-0.036943</td>\n",
       "      <td>-0.363213</td>\n",
       "      <td>0.905584</td>\n",
       "      <td>1.010070</td>\n",
       "      <td>0.210940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.254428</td>\n",
       "      <td>-0.011411</td>\n",
       "      <td>0.334044</td>\n",
       "      <td>-0.152447</td>\n",
       "      <td>0.132704</td>\n",
       "      <td>-0.040200</td>\n",
       "      <td>0.080763</td>\n",
       "      <td>0.169764</td>\n",
       "      <td>-0.161985</td>\n",
       "      <td>-0.002933</td>\n",
       "      <td>0.075230</td>\n",
       "      <td>0.053451</td>\n",
       "      <td>-0.158602</td>\n",
       "      <td>0.013316</td>\n",
       "      <td>-0.063966</td>\n",
       "      <td>0.150923</td>\n",
       "      <td>0.022072</td>\n",
       "      <td>0.168311</td>\n",
       "      <td>0.124978</td>\n",
       "      <td>-0.229316</td>\n",
       "      <td>0.006148</td>\n",
       "      <td>-0.104214</td>\n",
       "      <td>-0.089166</td>\n",
       "      <td>-0.112851</td>\n",
       "      <td>0.157767</td>\n",
       "      <td>-0.022986</td>\n",
       "      <td>0.275391</td>\n",
       "      <td>0.152245</td>\n",
       "      <td>-0.022349</td>\n",
       "      <td>0.043627</td>\n",
       "      <td>0.068497</td>\n",
       "      <td>-0.072003</td>\n",
       "      <td>0.164927</td>\n",
       "      <td>0.128153</td>\n",
       "      <td>-0.097877</td>\n",
       "      <td>0.112834</td>\n",
       "      <td>0.025989</td>\n",
       "      <td>-0.035945</td>\n",
       "      <td>0.029443</td>\n",
       "      <td>0.111205</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.048833</td>\n",
       "      <td>0.092955</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>-0.139243</td>\n",
       "      <td>-0.096447</td>\n",
       "      <td>0.371564</td>\n",
       "      <td>0.056716</td>\n",
       "      <td>-0.018409</td>\n",
       "      <td>0.003533</td>\n",
       "      <td>-0.065163</td>\n",
       "      <td>0.013406</td>\n",
       "      <td>0.051614</td>\n",
       "      <td>-0.313355</td>\n",
       "      <td>0.178504</td>\n",
       "      <td>-0.017465</td>\n",
       "      <td>0.077003</td>\n",
       "      <td>0.257123</td>\n",
       "      <td>0.087590</td>\n",
       "      <td>-0.035367</td>\n",
       "      <td>-0.082249</td>\n",
       "      <td>-0.172318</td>\n",
       "      <td>0.057436</td>\n",
       "      <td>-0.017893</td>\n",
       "      <td>-0.038150</td>\n",
       "      <td>-9.09304</td>\n",
       "      <td>0.102770</td>\n",
       "      <td>0.073625</td>\n",
       "      <td>0.083781</td>\n",
       "      <td>-0.100582</td>\n",
       "      <td>0.020675</td>\n",
       "      <td>-0.100587</td>\n",
       "      <td>-0.151935</td>\n",
       "      <td>-0.029423</td>\n",
       "      <td>-0.009458</td>\n",
       "      <td>0.317632</td>\n",
       "      <td>0.004331</td>\n",
       "      <td>0.124570</td>\n",
       "      <td>0.114443</td>\n",
       "      <td>-0.095432</td>\n",
       "      <td>-0.035912</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2    ...       765       766       767\n",
       "0  0.182144 -0.052698  0.296982  ...  0.050184 -0.023745 -0.108041\n",
       "1  0.090396 -0.063034  0.183588  ... -0.030987  0.065567  0.015198\n",
       "2  0.204430 -0.299815  0.278582  ...  0.201417 -0.064270  0.095350\n",
       "3  0.074055 -0.994510  0.083315  ...  0.905584  1.010070  0.210940\n",
       "4  0.254428 -0.011411  0.334044  ...  0.114443 -0.095432 -0.035912\n",
       "\n",
       "[5 rows x 768 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_embeddings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DuvbA4E1PrkG"
   },
   "outputs": [],
   "source": [
    "text_embeddings = reddit_df['text_embeddings'].apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jzPievAXSJ1k"
   },
   "outputs": [],
   "source": [
    "embeddings = pd.concat([title_embeddings, text_embeddings], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8m8x5DY0SUJR"
   },
   "outputs": [],
   "source": [
    "embeddings.columns = [i for i in range(1536)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FAjuDcE0RLAC"
   },
   "outputs": [],
   "source": [
    "subreddit_ce = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7TMAROOiRSg6"
   },
   "outputs": [],
   "source": [
    "subreddits = subreddit_ce.fit_transform(reddit_df['subreddit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fjtn4upFPzTh"
   },
   "outputs": [],
   "source": [
    "df = pd.concat([reddit_df['up_votes'],reddit_df['subreddit'], embeddings], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "colab_type": "code",
    "id": "TzdWmsu8QGpc",
    "outputId": "00d2d661-990e-4165-8695-1abee442ec6d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>up_votes</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>1526</th>\n",
       "      <th>1527</th>\n",
       "      <th>1528</th>\n",
       "      <th>1529</th>\n",
       "      <th>1530</th>\n",
       "      <th>1531</th>\n",
       "      <th>1532</th>\n",
       "      <th>1533</th>\n",
       "      <th>1534</th>\n",
       "      <th>1535</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>221854</td>\n",
       "      <td>r/AskReddit</td>\n",
       "      <td>0.182144</td>\n",
       "      <td>-0.052698</td>\n",
       "      <td>0.296982</td>\n",
       "      <td>-0.046854</td>\n",
       "      <td>0.066862</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.109145</td>\n",
       "      <td>0.115977</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017859</td>\n",
       "      <td>-0.048209</td>\n",
       "      <td>-0.140820</td>\n",
       "      <td>-0.028225</td>\n",
       "      <td>0.431787</td>\n",
       "      <td>-0.342401</td>\n",
       "      <td>0.033433</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.178056</td>\n",
       "      <td>0.036207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>197524</td>\n",
       "      <td>r/AskReddit</td>\n",
       "      <td>0.090396</td>\n",
       "      <td>-0.063034</td>\n",
       "      <td>0.183588</td>\n",
       "      <td>-0.132425</td>\n",
       "      <td>0.218097</td>\n",
       "      <td>-0.061284</td>\n",
       "      <td>0.194140</td>\n",
       "      <td>0.099648</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017859</td>\n",
       "      <td>-0.048209</td>\n",
       "      <td>-0.140820</td>\n",
       "      <td>-0.028225</td>\n",
       "      <td>0.431787</td>\n",
       "      <td>-0.342401</td>\n",
       "      <td>0.033433</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.178056</td>\n",
       "      <td>0.036207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>186368</td>\n",
       "      <td>r/AskReddit</td>\n",
       "      <td>0.204430</td>\n",
       "      <td>-0.299815</td>\n",
       "      <td>0.278582</td>\n",
       "      <td>-0.330959</td>\n",
       "      <td>0.283490</td>\n",
       "      <td>0.167636</td>\n",
       "      <td>-0.000396</td>\n",
       "      <td>0.186616</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017859</td>\n",
       "      <td>-0.048209</td>\n",
       "      <td>-0.140820</td>\n",
       "      <td>-0.028225</td>\n",
       "      <td>0.431787</td>\n",
       "      <td>-0.342401</td>\n",
       "      <td>0.033433</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.178056</td>\n",
       "      <td>0.036207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>175339</td>\n",
       "      <td>r/AskReddit</td>\n",
       "      <td>0.074055</td>\n",
       "      <td>-0.994510</td>\n",
       "      <td>0.083315</td>\n",
       "      <td>0.173790</td>\n",
       "      <td>0.435514</td>\n",
       "      <td>0.489149</td>\n",
       "      <td>-0.442962</td>\n",
       "      <td>-0.515874</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.120428</td>\n",
       "      <td>-0.059130</td>\n",
       "      <td>0.952337</td>\n",
       "      <td>-1.308660</td>\n",
       "      <td>0.952924</td>\n",
       "      <td>0.261201</td>\n",
       "      <td>-0.729069</td>\n",
       "      <td>0.909033</td>\n",
       "      <td>0.791063</td>\n",
       "      <td>0.122177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>160311</td>\n",
       "      <td>r/AskReddit</td>\n",
       "      <td>0.254428</td>\n",
       "      <td>-0.011411</td>\n",
       "      <td>0.334044</td>\n",
       "      <td>-0.152447</td>\n",
       "      <td>0.132704</td>\n",
       "      <td>-0.040200</td>\n",
       "      <td>0.080763</td>\n",
       "      <td>0.169764</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017859</td>\n",
       "      <td>-0.048209</td>\n",
       "      <td>-0.140820</td>\n",
       "      <td>-0.028225</td>\n",
       "      <td>0.431787</td>\n",
       "      <td>-0.342401</td>\n",
       "      <td>0.033433</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.178056</td>\n",
       "      <td>0.036207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1538 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   up_votes    subreddit         0         1         2         3         4  \\\n",
       "0    221854  r/AskReddit  0.182144 -0.052698  0.296982 -0.046854  0.066862   \n",
       "1    197524  r/AskReddit  0.090396 -0.063034  0.183588 -0.132425  0.218097   \n",
       "2    186368  r/AskReddit  0.204430 -0.299815  0.278582 -0.330959  0.283490   \n",
       "3    175339  r/AskReddit  0.074055 -0.994510  0.083315  0.173790  0.435514   \n",
       "4    160311  r/AskReddit  0.254428 -0.011411  0.334044 -0.152447  0.132704   \n",
       "\n",
       "          5         6         7  ...      1526      1527      1528      1529  \\\n",
       "0 -0.110474  0.109145  0.115977  ...  0.017859 -0.048209 -0.140820 -0.028225   \n",
       "1 -0.061284  0.194140  0.099648  ...  0.017859 -0.048209 -0.140820 -0.028225   \n",
       "2  0.167636 -0.000396  0.186616  ...  0.017859 -0.048209 -0.140820 -0.028225   \n",
       "3  0.489149 -0.442962 -0.515874  ... -0.120428 -0.059130  0.952337 -1.308660   \n",
       "4 -0.040200  0.080763  0.169764  ...  0.017859 -0.048209 -0.140820 -0.028225   \n",
       "\n",
       "       1530      1531      1532      1533      1534      1535  \n",
       "0  0.431787 -0.342401  0.033433  0.000455  0.178056  0.036207  \n",
       "1  0.431787 -0.342401  0.033433  0.000455  0.178056  0.036207  \n",
       "2  0.431787 -0.342401  0.033433  0.000455  0.178056  0.036207  \n",
       "3  0.952924  0.261201 -0.729069  0.909033  0.791063  0.122177  \n",
       "4  0.431787 -0.342401  0.033433  0.000455  0.178056  0.036207  \n",
       "\n",
       "[5 rows x 1538 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-osydgXSQLgQ"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('up_votes', axis=1), df['up_votes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-SaQ9DaTQkvR"
   },
   "outputs": [],
   "source": [
    "train_set = (X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o-wtqvCjQu88"
   },
   "outputs": [],
   "source": [
    "model = sk_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "colab_type": "code",
    "id": "vfgM0ilTQycA",
    "outputId": "5e4d4443-afea-4a45-e87c-41f0123140ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:44:59] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
       "             max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "             silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 52,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(*(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "zdycmcteQ2wT",
    "outputId": "23d054f2-895a-4527-d699-750956f6b759"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-9c2dfab6169e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_regression\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_check_reg_targets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m         \u001b[0;31m# XXX: Remove the check in 0.23\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_reg_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, output_margin, ntree_limit, validate_features)\u001b[0m\n\u001b[1;32m    454\u001b[0m                                           \u001b[0moutput_margin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_margin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                                           \u001b[0mntree_limit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mntree_limit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m                                           validate_features=validate_features)\n\u001b[0m\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntree_limit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, output_margin, ntree_limit, pred_leaf, pred_contribs, approx_contribs, pred_interactions, validate_features)\u001b[0m\n\u001b[1;32m   1282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidate_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1284\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1286\u001b[0m         \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_bst_ulong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_validate_features\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1689\u001b[0m                 raise ValueError(msg.format(self.feature_names,\n\u001b[0;32m-> 1690\u001b[0;31m                                             data.feature_names))\n\u001b[0m\u001b[1;32m   1691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1692\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_split_value_histogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_pandas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: feature_names mismatch: ['subreddit_1', 'subreddit_2', 'subreddit_3', 'subreddit_4', 'subreddit_5', 'subreddit_6', 'subreddit_7', 'subreddit_8', 'subreddit_9', 'subreddit_10', 'subreddit_11', 'subreddit_12', 'subreddit_13', 'subreddit_14', 'subreddit_15', 'subreddit_16', 'subreddit_17', 'subreddit_18', 'subreddit_19', 'subreddit_20', 'subreddit_21', 'subreddit_22', 'subreddit_23', 'subreddit_24', 'subreddit_25', 'subreddit_26', 'subreddit_27', 'subreddit_28', 'subreddit_29', 'subreddit_30', 'subreddit_31', 'subreddit_32', 'subreddit_33', 'subreddit_34', 'subreddit_35', 'subreddit_36', 'subreddit_37', 'subreddit_38', 'subreddit_39', 'subreddit_40', 'subreddit_41', 'subreddit_42', 'subreddit_43', 'subreddit_44', 'subreddit_45', 'subreddit_46', 'subreddit_47', 'subreddit_48', 'subreddit_49', 'subreddit_50', 'subreddit_51', 'subreddit_52', 'subreddit_53', 'subreddit_54', 'subreddit_55', 'subreddit_56', 'subreddit_57', 'subreddit_58', 'subreddit_59', 'subreddit_60', 'subreddit_61', 'subreddit_62', 'subreddit_63', 'subreddit_64', 'subreddit_65', 'subreddit_66', 'subreddit_67', 'subreddit_68', 'subreddit_69', 'subreddit_70', 'subreddit_71', 'subreddit_72', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52'...\nexpected 1508, 612, 1504, subreddit_69, 510, 1368, 1189, 1356, 874, 662, 914, 450, 922, 903, 1194, 623, 83, 476, 641, 1095, 1511, 927, 188, 628, 825, 462, 9, 1263, 272, 784, 33, 1277, 464, subreddit_5, 468, 948, 290, 285, 1224, 43, 1186, 1475, 58, 46, 507, 540, 1184, 535, 1007, 1454, 613, 424, 1002, 1269, 1100, 1492, 951, 249, 625, 1256, 200, 1474, 894, 1245, 892, 1340, 573, 361, 322, 968, subreddit_54, 849, 1019, 324, 136, 154, 225, 1465, 1375, 1424, 588, 1045, 1179, 684, 307, 416, subreddit_64, 1526, 524, 942, 987, 1170, 1533, 991, 534, 965, 705, 405, 564, 1082, 530, 152, 936, subreddit_61, 1459, 302, 266, 1413, subreddit_7, 1009, 409, 244, 757, 1079, 80, 358, 71, 1049, 1294, 1151, 747, 639, 659, 443, 1225, 433, 651, 430, 352, 119, 1347, 616, 944, 345, 1239, 962, 1233, 1501, 515, 792, 1365, 716, 547, 21, 341, 1188, 1498, 1146, 649, 646, 87, 1419, 1027, 180, 391, 1210, 264, 66, 699, 901, 946, 859, 721, 1039, 1335, 164, 1129, 576, 816, 1105, 29, 1275, 598, 1116, 529, 1494, 683, 1081, 1080, 890, 989, 400, 1168, subreddit_19, subreddit_55, 1387, 229, 1422, 110, 239, 906, 941, subreddit_21, 1487, 1403, 1373, 960, 449, 897, 187, 18, 1361, 197, 949, 403, 69, 337, 153, 336, 369, 741, 980, subreddit_27, 349, 389, 622, 503, 971, 1029, 1158, 401, 1190, 800, 521, 1303, 428, 1020, 694, 208, 298, 357, 159, 781, 64, 1026, 563, 557, 696, 1041, 931, 350, 343, 1060, subreddit_49, 512, 1099, 332, 1071, 438, 6, 1291, 1372, 511, 1473, 1490, 1317, 210, 425, 633, 740, 494, 90, 902, 1023, 258, ...\ntraining data did not have the following fields: f1205, f4153, f1840, f2790, f4351, f4398, f16, f2433, f2704, f1614, f2161, f276, f4230, f3580, f3280, f4115, f2800, f1148, f4176, f1131, f1353, f1528, f556, f4533, f166, f577, f3003, f2119, f3170, f418, f195, f831, f1980, f4641, f789, f3168, f1002, f3961, f1517, f2208, f4075, f2661, f2725, f1642, f1583, f3592, f4365, f4279, f1482, f4239, f4327, f4381, f1013, f1174, f3050, f1762, f2957, f36, f3198, f381, f2633, f1477, f1161, f1600, f1117, f1861, f2203, f3329, f4255, f4538, f57, f240, f202, f1867, f1686, f2169, f764, f3219, f396, f3676, f770, f2880, f4241, f457, f2215, f2467, f1554, f3916, f3896, f1288, f1743, f2604, f3678, f4547, f1393, f1481, f2218, f2936, f4152, f390, f67, f1753, f1934, f413, f4314, f4383, f2651, f1773, f1877, f2595, f4144, f1531, f555, f838, f4520, f2101, f3011, f4474, f4497, f1098, f2744, f1586, f2075, f2310, f2810, f543, f2638, f4434, f4498, f4282, f4581, f2589, f2993, f1143, f4071, f78, f1366, f1810, f2881, f3107, f4034, f1845, f2321, f4461, f1316, f2752, f4268, f4212, f4524, f55, f4606, f527, f2507, f2533, f4254, f389, f473, f1569, f2713, f4256, f4232, f4225, f2207, f2358, f3373, f3586, f470, f1841, f4432, f1176, f322, f3311, f711, f3900, f2843, f189, f3513, f1485, f900, f3308, f151, f4610, f4611, f125, f2769, f3682, f3501, f1273, f2378, f516, f793, f1945, f2728, f574, f2699, f4319, f2068, f975, f2539, f807, f3919, f2513, f2614, f2856, f2684, f4231, f2044, f909, f398, f4193, f4359, f15, f408, f4442, f4..."
     ]
    }
   ],
   "source": [
    "model.score(y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oHbzV-R1TZkS"
   },
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "colab_type": "code",
    "id": "yTLgNk0bQ_Z8",
    "outputId": "b31cf72a-4b6d-456e-894a-bf720687c346"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit_1</th>\n",
       "      <th>subreddit_2</th>\n",
       "      <th>subreddit_3</th>\n",
       "      <th>subreddit_4</th>\n",
       "      <th>subreddit_5</th>\n",
       "      <th>subreddit_6</th>\n",
       "      <th>subreddit_7</th>\n",
       "      <th>subreddit_8</th>\n",
       "      <th>subreddit_9</th>\n",
       "      <th>subreddit_10</th>\n",
       "      <th>subreddit_11</th>\n",
       "      <th>subreddit_12</th>\n",
       "      <th>subreddit_13</th>\n",
       "      <th>subreddit_14</th>\n",
       "      <th>subreddit_15</th>\n",
       "      <th>subreddit_16</th>\n",
       "      <th>subreddit_17</th>\n",
       "      <th>subreddit_18</th>\n",
       "      <th>subreddit_19</th>\n",
       "      <th>subreddit_20</th>\n",
       "      <th>subreddit_21</th>\n",
       "      <th>subreddit_22</th>\n",
       "      <th>subreddit_23</th>\n",
       "      <th>subreddit_24</th>\n",
       "      <th>subreddit_25</th>\n",
       "      <th>subreddit_26</th>\n",
       "      <th>subreddit_27</th>\n",
       "      <th>subreddit_28</th>\n",
       "      <th>subreddit_29</th>\n",
       "      <th>subreddit_30</th>\n",
       "      <th>subreddit_31</th>\n",
       "      <th>subreddit_32</th>\n",
       "      <th>subreddit_33</th>\n",
       "      <th>subreddit_34</th>\n",
       "      <th>subreddit_35</th>\n",
       "      <th>subreddit_36</th>\n",
       "      <th>subreddit_37</th>\n",
       "      <th>subreddit_38</th>\n",
       "      <th>subreddit_39</th>\n",
       "      <th>subreddit_40</th>\n",
       "      <th>...</th>\n",
       "      <th>1496</th>\n",
       "      <th>1497</th>\n",
       "      <th>1498</th>\n",
       "      <th>1499</th>\n",
       "      <th>1500</th>\n",
       "      <th>1501</th>\n",
       "      <th>1502</th>\n",
       "      <th>1503</th>\n",
       "      <th>1504</th>\n",
       "      <th>1505</th>\n",
       "      <th>1506</th>\n",
       "      <th>1507</th>\n",
       "      <th>1508</th>\n",
       "      <th>1509</th>\n",
       "      <th>1510</th>\n",
       "      <th>1511</th>\n",
       "      <th>1512</th>\n",
       "      <th>1513</th>\n",
       "      <th>1514</th>\n",
       "      <th>1515</th>\n",
       "      <th>1516</th>\n",
       "      <th>1517</th>\n",
       "      <th>1518</th>\n",
       "      <th>1519</th>\n",
       "      <th>1520</th>\n",
       "      <th>1521</th>\n",
       "      <th>1522</th>\n",
       "      <th>1523</th>\n",
       "      <th>1524</th>\n",
       "      <th>1525</th>\n",
       "      <th>1526</th>\n",
       "      <th>1527</th>\n",
       "      <th>1528</th>\n",
       "      <th>1529</th>\n",
       "      <th>1530</th>\n",
       "      <th>1531</th>\n",
       "      <th>1532</th>\n",
       "      <th>1533</th>\n",
       "      <th>1534</th>\n",
       "      <th>1535</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11429</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.225477</td>\n",
       "      <td>0.091579</td>\n",
       "      <td>0.373201</td>\n",
       "      <td>0.024850</td>\n",
       "      <td>-0.161485</td>\n",
       "      <td>0.085262</td>\n",
       "      <td>0.001845</td>\n",
       "      <td>-0.118379</td>\n",
       "      <td>-0.106382</td>\n",
       "      <td>0.102978</td>\n",
       "      <td>-0.118154</td>\n",
       "      <td>-0.042724</td>\n",
       "      <td>-0.300285</td>\n",
       "      <td>0.531707</td>\n",
       "      <td>-0.036965</td>\n",
       "      <td>0.182377</td>\n",
       "      <td>0.195290</td>\n",
       "      <td>-0.127690</td>\n",
       "      <td>0.076081</td>\n",
       "      <td>-0.044050</td>\n",
       "      <td>-0.069825</td>\n",
       "      <td>0.074869</td>\n",
       "      <td>-0.269732</td>\n",
       "      <td>-0.130279</td>\n",
       "      <td>-8.02170</td>\n",
       "      <td>0.006108</td>\n",
       "      <td>0.107337</td>\n",
       "      <td>0.002257</td>\n",
       "      <td>-0.077550</td>\n",
       "      <td>-0.213090</td>\n",
       "      <td>-0.237284</td>\n",
       "      <td>-0.127032</td>\n",
       "      <td>0.137565</td>\n",
       "      <td>-0.142593</td>\n",
       "      <td>0.308050</td>\n",
       "      <td>-0.009991</td>\n",
       "      <td>-0.002525</td>\n",
       "      <td>0.121720</td>\n",
       "      <td>0.097867</td>\n",
       "      <td>-0.041106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2799</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.273151</td>\n",
       "      <td>-0.059469</td>\n",
       "      <td>0.205945</td>\n",
       "      <td>-0.340266</td>\n",
       "      <td>0.320234</td>\n",
       "      <td>-0.104631</td>\n",
       "      <td>-0.317225</td>\n",
       "      <td>-0.647029</td>\n",
       "      <td>-0.521021</td>\n",
       "      <td>-0.077025</td>\n",
       "      <td>-0.237873</td>\n",
       "      <td>0.028217</td>\n",
       "      <td>-0.691589</td>\n",
       "      <td>0.438793</td>\n",
       "      <td>-0.188345</td>\n",
       "      <td>-0.068932</td>\n",
       "      <td>1.090510</td>\n",
       "      <td>-0.445843</td>\n",
       "      <td>-0.439159</td>\n",
       "      <td>0.140661</td>\n",
       "      <td>0.223976</td>\n",
       "      <td>1.168210</td>\n",
       "      <td>-0.543404</td>\n",
       "      <td>-0.289159</td>\n",
       "      <td>-3.82132</td>\n",
       "      <td>0.443102</td>\n",
       "      <td>0.297242</td>\n",
       "      <td>0.115645</td>\n",
       "      <td>-0.576921</td>\n",
       "      <td>-0.736589</td>\n",
       "      <td>0.111816</td>\n",
       "      <td>-0.010130</td>\n",
       "      <td>0.362283</td>\n",
       "      <td>-0.764631</td>\n",
       "      <td>0.505366</td>\n",
       "      <td>0.325662</td>\n",
       "      <td>-0.763775</td>\n",
       "      <td>0.710965</td>\n",
       "      <td>0.409210</td>\n",
       "      <td>0.220402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1642</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.204922</td>\n",
       "      <td>0.201669</td>\n",
       "      <td>0.095816</td>\n",
       "      <td>0.088479</td>\n",
       "      <td>-0.163301</td>\n",
       "      <td>0.178585</td>\n",
       "      <td>0.022706</td>\n",
       "      <td>0.072588</td>\n",
       "      <td>-0.085228</td>\n",
       "      <td>0.241856</td>\n",
       "      <td>-0.052063</td>\n",
       "      <td>0.206275</td>\n",
       "      <td>-0.226309</td>\n",
       "      <td>0.055761</td>\n",
       "      <td>-0.013927</td>\n",
       "      <td>-0.047932</td>\n",
       "      <td>0.055732</td>\n",
       "      <td>0.013788</td>\n",
       "      <td>0.058288</td>\n",
       "      <td>0.074495</td>\n",
       "      <td>-0.144048</td>\n",
       "      <td>0.243730</td>\n",
       "      <td>-0.124181</td>\n",
       "      <td>-0.023625</td>\n",
       "      <td>-8.46837</td>\n",
       "      <td>-0.155591</td>\n",
       "      <td>0.134095</td>\n",
       "      <td>0.142500</td>\n",
       "      <td>0.122580</td>\n",
       "      <td>0.353983</td>\n",
       "      <td>0.017859</td>\n",
       "      <td>-0.048209</td>\n",
       "      <td>-0.140820</td>\n",
       "      <td>-0.028225</td>\n",
       "      <td>0.431787</td>\n",
       "      <td>-0.342401</td>\n",
       "      <td>0.033433</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.178056</td>\n",
       "      <td>0.036207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3093</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.179900</td>\n",
       "      <td>-0.073909</td>\n",
       "      <td>0.070310</td>\n",
       "      <td>0.066482</td>\n",
       "      <td>-0.262494</td>\n",
       "      <td>-0.145907</td>\n",
       "      <td>-0.530300</td>\n",
       "      <td>0.088729</td>\n",
       "      <td>-0.280738</td>\n",
       "      <td>0.243182</td>\n",
       "      <td>-0.100594</td>\n",
       "      <td>0.201325</td>\n",
       "      <td>-0.359257</td>\n",
       "      <td>-0.072506</td>\n",
       "      <td>-0.187338</td>\n",
       "      <td>-0.001095</td>\n",
       "      <td>0.710825</td>\n",
       "      <td>0.153688</td>\n",
       "      <td>0.064037</td>\n",
       "      <td>-0.128739</td>\n",
       "      <td>-0.222729</td>\n",
       "      <td>-0.375751</td>\n",
       "      <td>0.067073</td>\n",
       "      <td>-0.116550</td>\n",
       "      <td>-3.25659</td>\n",
       "      <td>-0.118616</td>\n",
       "      <td>0.087087</td>\n",
       "      <td>-0.372002</td>\n",
       "      <td>-0.641340</td>\n",
       "      <td>-0.293612</td>\n",
       "      <td>0.303397</td>\n",
       "      <td>-0.294435</td>\n",
       "      <td>-0.540565</td>\n",
       "      <td>-0.239870</td>\n",
       "      <td>-0.162302</td>\n",
       "      <td>-0.099532</td>\n",
       "      <td>-0.323647</td>\n",
       "      <td>0.183855</td>\n",
       "      <td>0.203667</td>\n",
       "      <td>-0.355926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15278</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.182354</td>\n",
       "      <td>-0.352118</td>\n",
       "      <td>0.436717</td>\n",
       "      <td>-0.153194</td>\n",
       "      <td>0.241390</td>\n",
       "      <td>-0.388667</td>\n",
       "      <td>-0.542244</td>\n",
       "      <td>0.001208</td>\n",
       "      <td>0.255917</td>\n",
       "      <td>-0.177232</td>\n",
       "      <td>0.223216</td>\n",
       "      <td>0.131535</td>\n",
       "      <td>-0.424447</td>\n",
       "      <td>0.177258</td>\n",
       "      <td>-0.072651</td>\n",
       "      <td>0.265313</td>\n",
       "      <td>0.435464</td>\n",
       "      <td>-0.003332</td>\n",
       "      <td>0.015523</td>\n",
       "      <td>0.138728</td>\n",
       "      <td>0.390841</td>\n",
       "      <td>-0.177971</td>\n",
       "      <td>-0.496885</td>\n",
       "      <td>-0.177800</td>\n",
       "      <td>-1.24036</td>\n",
       "      <td>0.287297</td>\n",
       "      <td>-0.014849</td>\n",
       "      <td>-0.680968</td>\n",
       "      <td>-0.742825</td>\n",
       "      <td>-1.095090</td>\n",
       "      <td>0.260193</td>\n",
       "      <td>-0.075919</td>\n",
       "      <td>-0.452468</td>\n",
       "      <td>-0.761588</td>\n",
       "      <td>0.283345</td>\n",
       "      <td>0.335066</td>\n",
       "      <td>-0.463583</td>\n",
       "      <td>0.238207</td>\n",
       "      <td>0.085481</td>\n",
       "      <td>-0.230454</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1608 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       subreddit_1  subreddit_2  subreddit_3  ...      1533      1534      1535\n",
       "11429            0            0            0  ...  0.121720  0.097867 -0.041106\n",
       "2799             0            0            0  ...  0.710965  0.409210  0.220402\n",
       "1642             0            0            0  ...  0.000455  0.178056  0.036207\n",
       "3093             0            0            0  ...  0.183855  0.203667 -0.355926\n",
       "15278            0            0            0  ...  0.238207  0.085481 -0.230454\n",
       "\n",
       "[5 rows x 1608 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "colab_type": "code",
    "id": "CkPRLCFhTrQe",
    "outputId": "f58e7df5-b927-47aa-da8a-f91766c39c59"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit_1</th>\n",
       "      <th>subreddit_2</th>\n",
       "      <th>subreddit_3</th>\n",
       "      <th>subreddit_4</th>\n",
       "      <th>subreddit_5</th>\n",
       "      <th>subreddit_6</th>\n",
       "      <th>subreddit_7</th>\n",
       "      <th>subreddit_8</th>\n",
       "      <th>subreddit_9</th>\n",
       "      <th>subreddit_10</th>\n",
       "      <th>subreddit_11</th>\n",
       "      <th>subreddit_12</th>\n",
       "      <th>subreddit_13</th>\n",
       "      <th>subreddit_14</th>\n",
       "      <th>subreddit_15</th>\n",
       "      <th>subreddit_16</th>\n",
       "      <th>subreddit_17</th>\n",
       "      <th>subreddit_18</th>\n",
       "      <th>subreddit_19</th>\n",
       "      <th>subreddit_20</th>\n",
       "      <th>subreddit_21</th>\n",
       "      <th>subreddit_22</th>\n",
       "      <th>subreddit_23</th>\n",
       "      <th>subreddit_24</th>\n",
       "      <th>subreddit_25</th>\n",
       "      <th>subreddit_26</th>\n",
       "      <th>subreddit_27</th>\n",
       "      <th>subreddit_28</th>\n",
       "      <th>subreddit_29</th>\n",
       "      <th>subreddit_30</th>\n",
       "      <th>subreddit_31</th>\n",
       "      <th>subreddit_32</th>\n",
       "      <th>subreddit_33</th>\n",
       "      <th>subreddit_34</th>\n",
       "      <th>subreddit_35</th>\n",
       "      <th>subreddit_36</th>\n",
       "      <th>subreddit_37</th>\n",
       "      <th>subreddit_38</th>\n",
       "      <th>subreddit_39</th>\n",
       "      <th>subreddit_40</th>\n",
       "      <th>...</th>\n",
       "      <th>1496</th>\n",
       "      <th>1497</th>\n",
       "      <th>1498</th>\n",
       "      <th>1499</th>\n",
       "      <th>1500</th>\n",
       "      <th>1501</th>\n",
       "      <th>1502</th>\n",
       "      <th>1503</th>\n",
       "      <th>1504</th>\n",
       "      <th>1505</th>\n",
       "      <th>1506</th>\n",
       "      <th>1507</th>\n",
       "      <th>1508</th>\n",
       "      <th>1509</th>\n",
       "      <th>1510</th>\n",
       "      <th>1511</th>\n",
       "      <th>1512</th>\n",
       "      <th>1513</th>\n",
       "      <th>1514</th>\n",
       "      <th>1515</th>\n",
       "      <th>1516</th>\n",
       "      <th>1517</th>\n",
       "      <th>1518</th>\n",
       "      <th>1519</th>\n",
       "      <th>1520</th>\n",
       "      <th>1521</th>\n",
       "      <th>1522</th>\n",
       "      <th>1523</th>\n",
       "      <th>1524</th>\n",
       "      <th>1525</th>\n",
       "      <th>1526</th>\n",
       "      <th>1527</th>\n",
       "      <th>1528</th>\n",
       "      <th>1529</th>\n",
       "      <th>1530</th>\n",
       "      <th>1531</th>\n",
       "      <th>1532</th>\n",
       "      <th>1533</th>\n",
       "      <th>1534</th>\n",
       "      <th>1535</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1451</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.204922</td>\n",
       "      <td>0.201669</td>\n",
       "      <td>0.095816</td>\n",
       "      <td>0.088479</td>\n",
       "      <td>-0.163301</td>\n",
       "      <td>0.178585</td>\n",
       "      <td>0.022706</td>\n",
       "      <td>0.072588</td>\n",
       "      <td>-0.085228</td>\n",
       "      <td>0.241856</td>\n",
       "      <td>-0.052063</td>\n",
       "      <td>0.206275</td>\n",
       "      <td>-0.226309</td>\n",
       "      <td>0.055761</td>\n",
       "      <td>-0.013927</td>\n",
       "      <td>-0.047932</td>\n",
       "      <td>0.055732</td>\n",
       "      <td>0.013788</td>\n",
       "      <td>0.058288</td>\n",
       "      <td>0.074495</td>\n",
       "      <td>-0.144048</td>\n",
       "      <td>0.243730</td>\n",
       "      <td>-0.124181</td>\n",
       "      <td>-0.023625</td>\n",
       "      <td>-8.46837</td>\n",
       "      <td>-0.155591</td>\n",
       "      <td>0.134095</td>\n",
       "      <td>0.142500</td>\n",
       "      <td>0.122580</td>\n",
       "      <td>0.353983</td>\n",
       "      <td>0.017859</td>\n",
       "      <td>-0.048209</td>\n",
       "      <td>-0.140820</td>\n",
       "      <td>-0.028225</td>\n",
       "      <td>0.431787</td>\n",
       "      <td>-0.342401</td>\n",
       "      <td>0.033433</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.178056</td>\n",
       "      <td>0.036207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8793</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104173</td>\n",
       "      <td>0.205295</td>\n",
       "      <td>0.322830</td>\n",
       "      <td>-0.145345</td>\n",
       "      <td>-0.323853</td>\n",
       "      <td>0.138185</td>\n",
       "      <td>0.110294</td>\n",
       "      <td>-0.094105</td>\n",
       "      <td>-0.135802</td>\n",
       "      <td>0.018956</td>\n",
       "      <td>-0.094599</td>\n",
       "      <td>-0.091893</td>\n",
       "      <td>-0.425965</td>\n",
       "      <td>0.005144</td>\n",
       "      <td>0.016788</td>\n",
       "      <td>0.341178</td>\n",
       "      <td>0.452654</td>\n",
       "      <td>-0.209478</td>\n",
       "      <td>0.003217</td>\n",
       "      <td>-0.067886</td>\n",
       "      <td>-0.135384</td>\n",
       "      <td>0.363025</td>\n",
       "      <td>-0.292006</td>\n",
       "      <td>0.191587</td>\n",
       "      <td>-7.48267</td>\n",
       "      <td>0.198522</td>\n",
       "      <td>-0.322114</td>\n",
       "      <td>0.032515</td>\n",
       "      <td>-0.277872</td>\n",
       "      <td>-0.156811</td>\n",
       "      <td>-0.091525</td>\n",
       "      <td>0.012621</td>\n",
       "      <td>0.149762</td>\n",
       "      <td>-0.261669</td>\n",
       "      <td>0.454063</td>\n",
       "      <td>0.080677</td>\n",
       "      <td>-0.370884</td>\n",
       "      <td>0.320492</td>\n",
       "      <td>0.259246</td>\n",
       "      <td>-0.321208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12814</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.204922</td>\n",
       "      <td>0.201669</td>\n",
       "      <td>0.095816</td>\n",
       "      <td>0.088479</td>\n",
       "      <td>-0.163301</td>\n",
       "      <td>0.178585</td>\n",
       "      <td>0.022706</td>\n",
       "      <td>0.072588</td>\n",
       "      <td>-0.085228</td>\n",
       "      <td>0.241856</td>\n",
       "      <td>-0.052063</td>\n",
       "      <td>0.206275</td>\n",
       "      <td>-0.226309</td>\n",
       "      <td>0.055761</td>\n",
       "      <td>-0.013927</td>\n",
       "      <td>-0.047932</td>\n",
       "      <td>0.055732</td>\n",
       "      <td>0.013788</td>\n",
       "      <td>0.058288</td>\n",
       "      <td>0.074495</td>\n",
       "      <td>-0.144048</td>\n",
       "      <td>0.243730</td>\n",
       "      <td>-0.124181</td>\n",
       "      <td>-0.023625</td>\n",
       "      <td>-8.46837</td>\n",
       "      <td>-0.155591</td>\n",
       "      <td>0.134095</td>\n",
       "      <td>0.142500</td>\n",
       "      <td>0.122580</td>\n",
       "      <td>0.353983</td>\n",
       "      <td>0.017859</td>\n",
       "      <td>-0.048209</td>\n",
       "      <td>-0.140820</td>\n",
       "      <td>-0.028225</td>\n",
       "      <td>0.431787</td>\n",
       "      <td>-0.342401</td>\n",
       "      <td>0.033433</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.178056</td>\n",
       "      <td>0.036207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18106</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008551</td>\n",
       "      <td>-0.053723</td>\n",
       "      <td>0.268799</td>\n",
       "      <td>-0.177113</td>\n",
       "      <td>-0.126211</td>\n",
       "      <td>0.411242</td>\n",
       "      <td>-0.155015</td>\n",
       "      <td>-0.250619</td>\n",
       "      <td>-0.122082</td>\n",
       "      <td>-0.137647</td>\n",
       "      <td>0.078025</td>\n",
       "      <td>0.210796</td>\n",
       "      <td>-0.349288</td>\n",
       "      <td>0.260548</td>\n",
       "      <td>-0.167950</td>\n",
       "      <td>0.214914</td>\n",
       "      <td>0.524902</td>\n",
       "      <td>-0.239258</td>\n",
       "      <td>0.073209</td>\n",
       "      <td>-0.099148</td>\n",
       "      <td>-0.344925</td>\n",
       "      <td>0.245079</td>\n",
       "      <td>-0.206464</td>\n",
       "      <td>-0.106118</td>\n",
       "      <td>-8.02177</td>\n",
       "      <td>-0.063310</td>\n",
       "      <td>0.144854</td>\n",
       "      <td>0.157686</td>\n",
       "      <td>-0.373674</td>\n",
       "      <td>-0.336626</td>\n",
       "      <td>-0.136701</td>\n",
       "      <td>-0.087974</td>\n",
       "      <td>0.153318</td>\n",
       "      <td>-0.347229</td>\n",
       "      <td>0.228383</td>\n",
       "      <td>-0.016742</td>\n",
       "      <td>-0.071017</td>\n",
       "      <td>0.166031</td>\n",
       "      <td>0.178876</td>\n",
       "      <td>-0.002682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12720</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.204922</td>\n",
       "      <td>0.201669</td>\n",
       "      <td>0.095816</td>\n",
       "      <td>0.088479</td>\n",
       "      <td>-0.163301</td>\n",
       "      <td>0.178585</td>\n",
       "      <td>0.022706</td>\n",
       "      <td>0.072588</td>\n",
       "      <td>-0.085228</td>\n",
       "      <td>0.241856</td>\n",
       "      <td>-0.052063</td>\n",
       "      <td>0.206275</td>\n",
       "      <td>-0.226309</td>\n",
       "      <td>0.055761</td>\n",
       "      <td>-0.013927</td>\n",
       "      <td>-0.047932</td>\n",
       "      <td>0.055732</td>\n",
       "      <td>0.013788</td>\n",
       "      <td>0.058288</td>\n",
       "      <td>0.074495</td>\n",
       "      <td>-0.144048</td>\n",
       "      <td>0.243730</td>\n",
       "      <td>-0.124181</td>\n",
       "      <td>-0.023625</td>\n",
       "      <td>-8.46837</td>\n",
       "      <td>-0.155591</td>\n",
       "      <td>0.134095</td>\n",
       "      <td>0.142500</td>\n",
       "      <td>0.122580</td>\n",
       "      <td>0.353983</td>\n",
       "      <td>0.017859</td>\n",
       "      <td>-0.048209</td>\n",
       "      <td>-0.140820</td>\n",
       "      <td>-0.028225</td>\n",
       "      <td>0.431787</td>\n",
       "      <td>-0.342401</td>\n",
       "      <td>0.033433</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.178056</td>\n",
       "      <td>0.036207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1608 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       subreddit_1  subreddit_2  subreddit_3  ...      1533      1534      1535\n",
       "1451             0            0            0  ...  0.000455  0.178056  0.036207\n",
       "8793             0            0            0  ...  0.320492  0.259246 -0.321208\n",
       "12814            0            0            0  ...  0.000455  0.178056  0.036207\n",
       "18106            0            0            0  ...  0.166031  0.178876 -0.002682\n",
       "12720            0            0            0  ...  0.000455  0.178056  0.036207\n",
       "\n",
       "[5 rows x 1608 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "G3zoVqvRTsW8",
    "outputId": "148e52ea-e2a3-4561-9026-15564d27310a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7230084956060623"
      ]
     },
     "execution_count": 61,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9r7J0drCTzB9"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "lbWkw8JMT2fI",
    "outputId": "cc51f6fe-7a02-441f-c4ab-07ed96766bc6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6877.239513831657"
      ]
     },
     "execution_count": 63,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V_eWrZ1VT5ZD"
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JZ2x5s7ET_kj"
   },
   "outputs": [],
   "source": [
    "with open(\"up_vote_predicter.pickle\", \"wb\") as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2mQ5tQNhUIMD"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit_1</th>\n",
       "      <th>subreddit_2</th>\n",
       "      <th>subreddit_3</th>\n",
       "      <th>subreddit_4</th>\n",
       "      <th>subreddit_5</th>\n",
       "      <th>subreddit_6</th>\n",
       "      <th>subreddit_7</th>\n",
       "      <th>subreddit_8</th>\n",
       "      <th>subreddit_9</th>\n",
       "      <th>subreddit_10</th>\n",
       "      <th>...</th>\n",
       "      <th>1526</th>\n",
       "      <th>1527</th>\n",
       "      <th>1528</th>\n",
       "      <th>1529</th>\n",
       "      <th>1530</th>\n",
       "      <th>1531</th>\n",
       "      <th>1532</th>\n",
       "      <th>1533</th>\n",
       "      <th>1534</th>\n",
       "      <th>1535</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5444</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017859</td>\n",
       "      <td>-0.048209</td>\n",
       "      <td>-0.140820</td>\n",
       "      <td>-0.028225</td>\n",
       "      <td>0.431787</td>\n",
       "      <td>-0.342401</td>\n",
       "      <td>0.033433</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.178056</td>\n",
       "      <td>0.036207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9643</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017859</td>\n",
       "      <td>-0.048209</td>\n",
       "      <td>-0.140820</td>\n",
       "      <td>-0.028225</td>\n",
       "      <td>0.431787</td>\n",
       "      <td>-0.342401</td>\n",
       "      <td>0.033433</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.178056</td>\n",
       "      <td>0.036207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12720</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017859</td>\n",
       "      <td>-0.048209</td>\n",
       "      <td>-0.140820</td>\n",
       "      <td>-0.028225</td>\n",
       "      <td>0.431787</td>\n",
       "      <td>-0.342401</td>\n",
       "      <td>0.033433</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.178056</td>\n",
       "      <td>0.036207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2059</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017859</td>\n",
       "      <td>-0.048209</td>\n",
       "      <td>-0.140820</td>\n",
       "      <td>-0.028225</td>\n",
       "      <td>0.431787</td>\n",
       "      <td>-0.342401</td>\n",
       "      <td>0.033433</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.178056</td>\n",
       "      <td>0.036207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13125</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.250635</td>\n",
       "      <td>-0.079167</td>\n",
       "      <td>0.579189</td>\n",
       "      <td>-1.117600</td>\n",
       "      <td>0.529818</td>\n",
       "      <td>-0.080692</td>\n",
       "      <td>-0.199609</td>\n",
       "      <td>0.943206</td>\n",
       "      <td>0.533414</td>\n",
       "      <td>-0.018670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10213</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017859</td>\n",
       "      <td>-0.048209</td>\n",
       "      <td>-0.140820</td>\n",
       "      <td>-0.028225</td>\n",
       "      <td>0.431787</td>\n",
       "      <td>-0.342401</td>\n",
       "      <td>0.033433</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.178056</td>\n",
       "      <td>0.036207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017859</td>\n",
       "      <td>-0.048209</td>\n",
       "      <td>-0.140820</td>\n",
       "      <td>-0.028225</td>\n",
       "      <td>0.431787</td>\n",
       "      <td>-0.342401</td>\n",
       "      <td>0.033433</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.178056</td>\n",
       "      <td>0.036207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15953</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.123563</td>\n",
       "      <td>-0.029524</td>\n",
       "      <td>0.030757</td>\n",
       "      <td>-0.314385</td>\n",
       "      <td>0.380583</td>\n",
       "      <td>0.336724</td>\n",
       "      <td>0.068898</td>\n",
       "      <td>0.377340</td>\n",
       "      <td>0.013820</td>\n",
       "      <td>0.234301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16011</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.101914</td>\n",
       "      <td>0.082471</td>\n",
       "      <td>-0.075933</td>\n",
       "      <td>0.077133</td>\n",
       "      <td>0.193770</td>\n",
       "      <td>-0.203548</td>\n",
       "      <td>0.052253</td>\n",
       "      <td>0.018732</td>\n",
       "      <td>0.116457</td>\n",
       "      <td>-0.027252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14217</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103896</td>\n",
       "      <td>0.249866</td>\n",
       "      <td>-0.095697</td>\n",
       "      <td>-0.602027</td>\n",
       "      <td>0.437848</td>\n",
       "      <td>0.161073</td>\n",
       "      <td>-0.388599</td>\n",
       "      <td>0.374252</td>\n",
       "      <td>0.112776</td>\n",
       "      <td>-0.207442</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13932 rows × 1604 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       subreddit_1  subreddit_2  subreddit_3  subreddit_4  subreddit_5  \\\n",
       "5444             1            0            0            0            0   \n",
       "9643             0            1            0            0            0   \n",
       "12720            0            0            1            0            0   \n",
       "2059             0            0            0            1            0   \n",
       "13125            0            0            0            0            1   \n",
       "...            ...          ...          ...          ...          ...   \n",
       "10213            0            1            0            0            0   \n",
       "211              0            0            0            0            0   \n",
       "15953            0            0            0            0            0   \n",
       "16011            0            0            0            0            0   \n",
       "14217            0            0            0            0            0   \n",
       "\n",
       "       subreddit_6  subreddit_7  subreddit_8  subreddit_9  subreddit_10  ...  \\\n",
       "5444             0            0            0            0             0  ...   \n",
       "9643             0            0            0            0             0  ...   \n",
       "12720            0            0            0            0             0  ...   \n",
       "2059             0            0            0            0             0  ...   \n",
       "13125            0            0            0            0             0  ...   \n",
       "...            ...          ...          ...          ...           ...  ...   \n",
       "10213            0            0            0            0             0  ...   \n",
       "211              0            0            0            0             0  ...   \n",
       "15953            0            0            0            0             0  ...   \n",
       "16011            0            0            0            0             1  ...   \n",
       "14217            0            0            0            0             0  ...   \n",
       "\n",
       "           1526      1527      1528      1529      1530      1531      1532  \\\n",
       "5444   0.017859 -0.048209 -0.140820 -0.028225  0.431787 -0.342401  0.033433   \n",
       "9643   0.017859 -0.048209 -0.140820 -0.028225  0.431787 -0.342401  0.033433   \n",
       "12720  0.017859 -0.048209 -0.140820 -0.028225  0.431787 -0.342401  0.033433   \n",
       "2059   0.017859 -0.048209 -0.140820 -0.028225  0.431787 -0.342401  0.033433   \n",
       "13125 -0.250635 -0.079167  0.579189 -1.117600  0.529818 -0.080692 -0.199609   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "10213  0.017859 -0.048209 -0.140820 -0.028225  0.431787 -0.342401  0.033433   \n",
       "211    0.017859 -0.048209 -0.140820 -0.028225  0.431787 -0.342401  0.033433   \n",
       "15953 -0.123563 -0.029524  0.030757 -0.314385  0.380583  0.336724  0.068898   \n",
       "16011 -0.101914  0.082471 -0.075933  0.077133  0.193770 -0.203548  0.052253   \n",
       "14217  0.103896  0.249866 -0.095697 -0.602027  0.437848  0.161073 -0.388599   \n",
       "\n",
       "           1533      1534      1535  \n",
       "5444   0.000455  0.178056  0.036207  \n",
       "9643   0.000455  0.178056  0.036207  \n",
       "12720  0.000455  0.178056  0.036207  \n",
       "2059   0.000455  0.178056  0.036207  \n",
       "13125  0.943206  0.533414 -0.018670  \n",
       "...         ...       ...       ...  \n",
       "10213  0.000455  0.178056  0.036207  \n",
       "211    0.000455  0.178056  0.036207  \n",
       "15953  0.377340  0.013820  0.234301  \n",
       "16011  0.018732  0.116457 -0.027252  \n",
       "14217  0.374252  0.112776 -0.207442  \n",
       "\n",
       "[13932 rows x 1604 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = OneHotEncoder()\n",
    "enc.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(OneHotEncoder(), XGBRegressor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('onehotencoder',\n",
       "                 OneHotEncoder(cols=['subreddit'], drop_invariant=False,\n",
       "                               handle_missing='value', handle_unknown='value',\n",
       "                               return_df=True, use_cat_names=False,\n",
       "                               verbose=0)),\n",
       "                ('xgbregressor',\n",
       "                 XGBRegressor(base_score=0.5, booster='gbtree',\n",
       "                              colsample_bylevel=1, colsample_bynode=1,\n",
       "                              colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "                              importance_type='gain',\n",
       "                              interaction_constraints='',\n",
       "                              learning_rate=0.300000012, max_delta_step=0,\n",
       "                              max_depth=6, min_child_weight=1, missing=nan,\n",
       "                              monotone_constraints='()', n_estimators=100,\n",
       "                              n_jobs=0, num_parallel_tree=1,\n",
       "                              objective='reg:squarederror', random_state=0,\n",
       "                              reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "                              subsample=1, tree_method='exact',\n",
       "                              validate_parameters=1, verbosity=None))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(*train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('onehotencoder',\n",
       "                 OneHotEncoder(cols=['subreddit'], drop_invariant=False,\n",
       "                               handle_missing='value', handle_unknown='value',\n",
       "                               return_df=True, use_cat_names=False,\n",
       "                               verbose=0)),\n",
       "                ('type', <class 'xgboost.sklearn.XGBRegressor'>)],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7087282762411543"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6939.68660248509"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_test, pipeline.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model.pickle\", \"wb\") as f:\n",
    "    pickle.dump(pipeline, f, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(OneHotEncoder(), LassoCV(tol=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('onehotencoder',\n",
       "                 OneHotEncoder(cols=['subreddit'], drop_invariant=False,\n",
       "                               handle_missing='value', handle_unknown='value',\n",
       "                               return_df=True, use_cat_names=False,\n",
       "                               verbose=0)),\n",
       "                ('lassocv',\n",
       "                 LassoCV(alphas=None, copy_X=True, cv=None, eps=0.001,\n",
       "                         fit_intercept=True, max_iter=1000, n_alphas=100,\n",
       "                         n_jobs=None, normalize=False, positive=False,\n",
       "                         precompute='auto', random_state=None,\n",
       "                         selection='cyclic', tol=0.001, verbose=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(*train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7376746473730273"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6491.900799453385"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_test, pipeline.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BW Post here",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "01f7eddf95324dcd860e6111b88716cc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "168d8764bf474de1a3327514d275379f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dcc56ad2588243078c7923a820a9339a",
      "placeholder": "​",
      "style": "IPY_MODEL_f64c4ed72d85446b945d00ce16e5548f",
      "value": " 1536/1536 [00:07&lt;00:00, 219.40it/s]"
     }
    },
    "336a08e906d34a4da665d2d71691fbdc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9bd68eac8d3746f8b7f6926d93681e6e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_336a08e906d34a4da665d2d71691fbdc",
      "max": 1536,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f38ad80944cd4c1ba40461c1a7878eff",
      "value": 1536
     }
    },
    "b800925fb32b420f9b83f31e29b4d26d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9bd68eac8d3746f8b7f6926d93681e6e",
       "IPY_MODEL_168d8764bf474de1a3327514d275379f"
      ],
      "layout": "IPY_MODEL_01f7eddf95324dcd860e6111b88716cc"
     }
    },
    "dcc56ad2588243078c7923a820a9339a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f38ad80944cd4c1ba40461c1a7878eff": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "f64c4ed72d85446b945d00ce16e5548f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
